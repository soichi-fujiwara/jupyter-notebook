{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web_scraping_Qiita.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soichi-fujiwara/jupyter-notebook/blob/master/Web_scraping_Qiita.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmCv19wpeqxa",
        "colab_type": "code",
        "outputId": "c4d59cb7-edc4-4b2a-e420-8f5814730074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install mecab-python"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNMKawvAeYAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "#import MeCab\n",
        "from google.colab import files\n",
        "\n",
        "# 親ページURL(検索ワード指定：python 回帰 ソート:ストック順)\n",
        "url_pr = 'https://qiita.com/search?page={0}&q=python%E3%80%80%E5%9B%9E%E5%B8%B0&sort=stock'\n",
        "# 子ページURL(ベース)\n",
        "url_c_base = 'https://qiita.com/' \n",
        "\n",
        "# データフレームを定義\n",
        "columns = [\"タイトル名\",\"URL\",\"単語\"]\n",
        "df = pd.DataFrame(columns=columns)\n",
        "\n",
        "#親ページ20ページ分\n",
        "#※1からindex21まで\n",
        "for i in range(1,21):\n",
        "\n",
        "  html_doc = requests.get(url_pr.format(i)).text\n",
        "\n",
        "  # BeautifulSoupでスクレイピング\n",
        "  soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "  # ラッピング単位のクラスを取得\n",
        "  contents  = soup.find_all(\"div\", {\"class\": \"searchResult_main\"})\n",
        "\n",
        "  for content in contents:\n",
        "    content_data = content.find(\"h1\", {\"class\": \"searchResult_itemTitle\"})\n",
        "\n",
        "    #子ページタイトル\n",
        "    content_name = content_data.text\n",
        "    #子ページURL\n",
        "    chilld_link = url_c_base + str(content_data.a.get(\"href\")) \n",
        "\n",
        "    html_c_doc = requests.get(chilld_link).text\n",
        "    soup_c = BeautifulSoup(html_c_doc, 'html.parser')\n",
        "\n",
        "    # 子ページ内の単語を取得\n",
        "    code_word = ''\n",
        "    contents_c  = soup_c.find_all(\"div\", {\"class\": \"code-frame\"})\n",
        "    for code_word_list in contents_c:\n",
        "      code_word = code_word + code_word_list.text\n",
        "\n",
        "    # 各データをデータフレームに格納\n",
        "    se = pd.Series([content_name, chilld_link,code_word], columns)\n",
        "    df = df.append(se, columns)\n",
        "\n",
        "# 収集したデータをCSV形式で保存\n",
        "filename = \"Qiita_python_list.csv\"\n",
        "df.to_csv(filename, encoding = 'utf-8')\n",
        "files.download(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbF1CzqH9YP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0aab65d1-ee5a-4c28-cfa8-829fe1e08d2e"
      },
      "source": [
        "#コーパス作成\n",
        "df2 = df[[\"単語\"]]\n",
        "str_wk = ''\n",
        "\n",
        "for index,item in df2.iterrows():\n",
        "  str_wk = str_wk + str(item['単語'])\n",
        "\n",
        "print(str_wk)\n",
        "  \n",
        "# tagger = MeCab.Tagger(\"-Owakati\")\n",
        "# list_output = (tagger.parse(str_wk)).split(' ')\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "input_Tensor達\n",
            "x = tf.placeholder(tf.float32, [None, 784]) #images\n",
            "y_ = tf.placeholder(tf.float32, [None, 10]) #labels\n",
            "#None部分にはバッチの数が入る\n",
            "\n",
            "\n",
            "最後の方の学習実行開始コード\n",
            "for i in range(1000):\n",
            " batch_xs, batch_ys = mnist.train.next_batch(100)\n",
            " sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
            "\n",
            "\n",
            "行列演算matmul\n",
            "tf.matmul(A,B) # A is [4,2] and B is [2,3]. output would be [4,3]\n",
            "重みW\n",
            "W = tf.Variable(tf.zeros([784, 10]))\n",
            "\n",
            "\n",
            "行列演算された後\n",
            "matmul = tf.matmul(x,W)\n",
            "print \"matmul:\", matmul[0] #最初の画像(答えは7)\n",
            "matmul: [ 1.43326855 -10.14613152 2.10967159 6.07900429 -3.25419664\n",
            "-1.93730605 -8.57098293 10.21759605 1.16319525 2.90590048]\n",
            "\n",
            "\n",
            "バイアス\n",
            "b = tf.Variable(tf.zeros([10]))\n",
            "print \"b:\",b #学習後のバイアス\n",
            "b: [-0.98651898 0.82111627 0.23709664 -0.55601585 0.00611385 2.46202803\n",
            "-0.34819031 1.39600098 -2.53770232 -0.49392569]\n",
            "\n",
            "\n",
            "softmax\n",
            "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
            "print \"y\", y[0] #最初の画像(答えは7)\n",
            "y [ 2.04339485e-05 6.08732953e-10 5.19737077e-05 2.63350527e-03\n",
            "2.94665284e-07 2.85405549e-05 2.29651920e-09 9.96997833e-01\n",
            "1.14465665e-05 2.55984633e-04]\n",
            "\n",
            "\n",
            "答えちょうだいな\n",
            "x_answer = tf.argmax(y,1)\n",
            "y_answer = tf.argmax(y_,1)\n",
            "print \"x\",x_answer[0:10] #Tensorflowが思う最初の10画像の答え\n",
            "print \"y\",y_answer[0:10] #10画像の本当の答え\n",
            "x [7 2 1 0 4 1 4 9 6 9]\n",
            "y [7 2 1 0 4 1 4 9 5 9]\n",
            "\n",
            "\n",
            "精度が知りたいな\n",
            "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
            "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
            "print \"accuracy:\", accuracy\n",
            "accuracy: 0.9128\n",
            "\n",
            "\n",
            "最後の方の学習実行開始コード\n",
            "for i in range(1000):\n",
            " batch_xs, batch_ys = mnist.train.next_batch(100)\n",
            " sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
            "\n",
            "\n",
            "学習方法\n",
            "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
            "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
            "'''\n",
            " y: [batch_num, 10] y is a list of processed numbers of x(images)\n",
            "y_: [batch_num, 10] y_ is labels\n",
            "0.01 is a learning rate\n",
            "'''\n",
            "\n",
            "log-y = tf.log(y)\n",
            "print log-y[0]\n",
            "[ -1.06416254e+01 -2.04846172e+01 -8.92418385e+00 -5.71210337e+00\n",
            " -1.47629070e+01 -1.18935766e+01 -1.92577553e+01 -3.63449310e-03\n",
            " -1.08472376e+01 -8.88469982e+00]\n",
            "y_times_log-y = y_*tf.log(y)\n",
            "print y_times_log-y[0] #7の値のみが残る。\n",
            "[-0. -0. -0. -0. -0. -0.\n",
            "-0. -0.00181153 -0. -0. ]\n",
            "\n",
            "例tf.reduce_sum()\n",
            "# 'x' is [[1, 1, 1]\n",
            "# [1, 1, 1]]\n",
            "tf.reduce_sum(x) ==> 6\n",
            "tf.reduce_sum(x, 0) ==> [2, 2, 2]\n",
            "tf.reduce_sum(x, 1) ==> [3, 3]\n",
            "tf.reduce_sum(x, 1, keep_dims=True) ==> [[3], [3]]\n",
            "tf.reduce_sum(x, [0, 1]) ==> 6\n",
            "------\n",
            "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
            "print \"cross_entropy:\", cross_entropy #y_*tf.log(y)の中身を全部足した数\n",
            "cross_entropy 23026.0 #最初の学習後の数値\n",
            ".\n",
            ".\n",
            ".\n",
            "cross_entropy: 3089.6 #最後の学習後の数値\n",
            "\n",
            "\n",
            "Gradient_values\n",
            "#学習初期\n",
            "cross_entropy 23026.0\n",
            "grad W[0] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "grad W[380] [ 511.78765869 59.3368187 -34.74549103 -163.8828125 -103.32589722\n",
            " 181.61528015 17.56824303 -60.38471603 -175.52197266 -232.44744873]\n",
            "grad b [ 19.99900627 -135.00904846 -32.00152588 -9.99949074 18.00206184\n",
            " 107.99274445 41.992836 -27.99754715 26.00336075 -8.99738121]\n",
            "#学習最後\n",
            "cross_entropy 2870.42\n",
            "grad W[0] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "grad W[380] [ 6.80800724 1.27235568 -6.85943699 -22.70822525 -17.48428154\n",
            " 13.11752224 19.7425499 -32.00106812 -41.48160553 79.59416199]\n",
            "grad b [ 19.52701187 3.17797041 -20.07606125 -48.88145447 -28.05920601\n",
            " 37.52313232 40.22808456 -34.04494858 -74.16973114 104.77211761]\n",
            "\n",
            "1. データサイエンティスト講座概要とPythonの基礎\n",
            "2. Numpy、Scipy、Pandas、Matplotlibの基礎\n",
            "3. 記述統計学と単回帰分析\n",
            "4. 確率と統計の基礎\n",
            "5. Pythonによる科学計算の基礎（NumpyとScipy）\n",
            "6. Pandasを使ったデータ加工処理\n",
            "7. Matplotlibを使ったデータ可視化\n",
            "8. データベースとSQLの基礎\n",
            "9. データベースの応用（高度なSQL処理と高速化）\n",
            "10. ドキュメント型DB（MongoDB）\n",
            "11. 機械学習の基礎（教師あり学習）\n",
            "12. 機械学習の基礎（教師なし学習）\n",
            "13. モデルの検証方法とチューニング方法\n",
            "14. データサイエンスティスト中級者への道\n",
            "15. 総合演習問題\n",
            "2. numpy, scipy, scikit-learn の使い方を理解する\n",
            "3. k-Nearest Neighbors (k-NN) を使った手書き文字認識\n",
            "4. ロジスティック回帰の実装と学習、活性化関数とその微分の実装、多層パーセプトロンの実装と学習\n",
            "5. Tensorflowの基礎を学ぶ\n",
            "6. Denoising Autoencoderの実装. また, MNISTを用いて次のことを確認、Stacked Denoising Autoencoder (SdA) の実装\n",
            "7. 畳み込みニューラルネットワーク(Convolutional Neural Networks)の実装と学習\n",
            "8. CIFAR10データセットを使ったAugmentation、前処理、Batch Normalization、CNN実装、Activation可視化\n",
            "9. Recurrent Neural Network (RNN) によるIMDbのsentiment analysis\n",
            "10. Recurrent Neural Network (RNN) Encoder-Decoderモデルで英日翻訳\n",
            "11-1. Attentionを用いた機械翻訳モデルの実装\n",
            "11-2. Attentionを用いたキャプション生成モデルの実装\n",
            "$ apm list --installed --bare\n",
            "angularjs@0.3.4\n",
            "atom-beautify@0.29.7\n",
            "autocomplete-modules@1.4.1\n",
            "autocomplete-paths@1.0.2\n",
            "autocomplete-sass@0.1.0\n",
            "color-picker@2.1.1\n",
            "docblockr@0.7.3\n",
            "file-icons@1.7.10\n",
            "git-time-machine@1.5.2\n",
            "highlight-selected@0.11.2\n",
            "javascript-snippets@1.2.1\n",
            "linter@1.11.4\n",
            "linter-eslint@7.2.2\n",
            "linter-htmlhint@1.1.4\n",
            "linter-sass-lint@1.4.3\n",
            "local-history@3.2.3\n",
            "pigments@0.26.2\n",
            "todo-show@1.6.0\n",
            "\n",
            "gulpfile.js\n",
            "'use strict';\n",
            "package.json\n",
            "{\n",
            "  \"name\": \"planetter\",\n",
            "  \"description\": \"Travel the planet\",\n",
            "  \"version\": \"1.1.0\",\n",
            "  \"private\": true,\n",
            "  \"scripts\": {\n",
            "    \"start\": \"node ./bin/www\",\n",
            "    \"start:watch\": \"nodemon ./bin/www -L -e js,json,dot --watch ./app/server/\",\n",
            "    \"start:debug\": \"node-debug ./bin/www\",\n",
            "    \"start:sync\": \"browser-sync start -s --directory --no-notify -f='./test/assets/**/*.html' -f='./public/app.css'\",\n",
            "    \"test\": \"npm run test:server && npm run test:client\",\n",
            "    \"test:server\": \"NODE_ENV=test istanbul cover _mocha -- ./test/server/\",\n",
            "    \"test:server:watch\": \"NODE_ENV=test mocha -w ./test/server/\",\n",
            "    \"test:client\": \"karma start\",\n",
            "    \"test:client:watch\": \"karma start --no-single-run --reporters=mocha\",\n",
            "    \"test:client:linux\": \"karma start --browsers PhantomJS,Chrome,Firefox\",\n",
            "    \"test:client:windows\": \"karma start --browsers PhantomJS,Chrome,Firefox,IE\",\n",
            "    \"build\": \"npm run build:js && npm run build:css\",\n",
            "    \"build:js\": \"rm ./public/app.js.map -f && browserify ./app/client/app.js | uglifyjs -m -o ./public/app.js\",\n",
            "    \"build:js:watch\": \"watchify ./app/client/app.js -d -v -o 'exorcist ./public/app.js.map > ./public/app.js'\",\n",
            "    \"build:css\": \"gulp build:css\",\n",
            "    \"build:css:watch\": \"gulp build:css:watch\",\n",
            "    \"lint\": \"npm run lint:js && npm run lint:css && npm run lint:html\",\n",
            "    \"lint:js\": \"eslint ./app/ ./test/\",\n",
            "    \"lint:css\": \"sass-lint './app/assets/stylesheets/**/*.scss' -v -q\",\n",
            "    \"lint:html\": \"htmlhint ./app/server/views/*.dot\"\n",
            "  },\n",
            "  \"dependencies\": {\n",
            "    \"body-parser\": \"^1.15.1\",\n",
            "    \"config\": \"^1.19.0\",\n",
            "    \"connect-redis\": \"^3.0.2\",\n",
            "    \"debug\": \"^2.2.0\",\n",
            "    \"dot\": \"^1.0.3\",\n",
            "    \"express\": \"^4.13.4\",\n",
            "    \"express-session\": \"^1.13.0\",\n",
            "    \"http-status\": \"^0.2.1\",\n",
            "    \"log4js\": \"^0.6.36\",\n",
            "    \"mongoose\": \"^4.4.15\",\n",
            "    \"passport\": \"^0.3.2\",\n",
            "    \"passport-facebook\": \"^2.1.0\",\n",
            "    \"passport-google-oauth2\": \"^0.1.6\",\n",
            "    \"passport-twitter\": \"^1.0.4\",\n",
            "    \"redis\": \"^2.6.0-2\"\n",
            "  },\n",
            "  \"devDependencies\": {\n",
            "    \"angular\": \"^1.5.3\",\n",
            "    \"angular-mocks\": \"^1.5.3\",\n",
            "    \"angular-touch\": \"^1.5.3\",\n",
            "    \"async\": \"^1.5.2\",\n",
            "    \"browser-sync\": \"^2.12.5\",\n",
            "    \"browserify\": \"^13.0.1\",\n",
            "    \"browserify-istanbul\": \"^2.0.0\",\n",
            "    \"chai\": \"^3.5.0\",\n",
            "    \"comma-separated-values\": \"^3.6.4\",\n",
            "    \"eslint\": \"^2.9.0\",\n",
            "    \"exorcist\": \"^0.4.0\",\n",
            "    \"gulp\": \"^3.9.1\",\n",
            "    \"gulp-autoprefixer\": \"^3.1.0\",\n",
            "    \"gulp-clean-css\": \"^2.0.7\",\n",
            "    \"gulp-notify\": \"^2.2.0\",\n",
            "    \"gulp-plumber\": \"^1.1.0\",\n",
            "    \"gulp-sass\": \"^2.2.0\",\n",
            "    \"gulp-sourcemaps\": \"^2.0.0-alpha\",\n",
            "    \"htmlhint\": \"^0.9.13\",\n",
            "    \"istanbul\": \"^0.4.3\",\n",
            "    \"jquery\": \"^2.2.3\",\n",
            "    \"js-yaml\": \"^3.6.0\",\n",
            "    \"karma\": \"^0.13.22\",\n",
            "    \"karma-browserify\": \"^5.0.5\",\n",
            "    \"karma-chrome-launcher\": \"^0.2.3\",\n",
            "    \"karma-coverage\": \"^0.5.5\",\n",
            "    \"karma-firefox-launcher\": \"^0.1.7\",\n",
            "    \"karma-ie-launcher\": \"^0.2.0\",\n",
            "    \"karma-mocha\": \"^0.2.2\",\n",
            "    \"karma-mocha-reporter\": \"^2.0.3\",\n",
            "    \"karma-phantomjs-launcher\": \"^1.0.0\",\n",
            "    \"mocha\": \"^2.4.5\",\n",
            "    \"node-inspector\": \"^0.12.8\",\n",
            "    \"nodemon\": \"^1.9.2\",\n",
            "    \"phantomjs-prebuilt\": \"^2.1.7\",\n",
            "    \"request\": \"^2.72.0\",\n",
            "    \"sass-lint\": \"^1.7.0\",\n",
            "    \"supertest\": \"^1.2.0\",\n",
            "    \"svgo\": \"^0.6.6\",\n",
            "    \"uglify-js\": \"^2.6.2\",\n",
            "    \"watchify\": \"^3.7.0\"\n",
            "  }\n",
            "}\n",
            "\n",
            "[plans]\n",
            "user_id plan_id.\n",
            "├── app.js\n",
            "├── bin\n",
            "├── routes\n",
            "├── views\n",
            "└── public\n",
            "    ├── images\n",
            "    ├── javascripts\n",
            "    └── stylesheets\n",
            ".\n",
            "├── server.js\n",
            "├── config\n",
            "├── modules\n",
            "│   ├── core\n",
            "│   │   ├── client\n",
            "│   │   ├── server\n",
            "│   │   └── tests\n",
            "│   └── users\n",
            "│       ├── client\n",
            "│       ├── server\n",
            "│       └── tests\n",
            "└── public\n",
            "    └── lib\n",
            "        ├── angular\n",
            "        └── ...\n",
            ".\n",
            "├── .gitignore\n",
            "├── .eslintrc.yml\n",
            "├── .sass-lint.yml\n",
            "├── gulpfile.js\n",
            "├── karma.conf.js\n",
            "├── package.json\n",
            "├── README.md\n",
            "├── Vagrantfile\n",
            "├── bin\n",
            "│   ├── www\n",
            "│   └── ...\n",
            "├── config\n",
            "│   ├── default.json\n",
            "│   ├── development.json\n",
            "│   ├── production.json\n",
            "│   ├── staging.json\n",
            "│   └── test.json\n",
            "├── public\n",
            "│   ├── app.css\n",
            "│   ├── app.js\n",
            "│   ├── favicon.ico\n",
            "│   └── robots.txt\n",
            "├── app\n",
            "│   ├── assets\n",
            "│   │   ├── images\n",
            "│   │   │   └── ...\n",
            "│   │   └── stylesheets\n",
            "│   │       ├── _config.scss\n",
            "│   │       ├── app.scss\n",
            "│   │       ├── basics\n",
            "│   │       │   ├── reset.scss\n",
            "│   │       │   └── ...\n",
            "│   │       ├── layouts\n",
            "│   │       │   ├── navi.scss\n",
            "│   │       │   └── ...\n",
            "│   │       └── modules\n",
            "│   │           ├── navi.scss\n",
            "│   │           └── ...\n",
            "│   ├── client\n",
            "│   │   ├── app.js\n",
            "│   │   ├── controllers\n",
            "│   │   │   ├── app_controller.js\n",
            "│   │   │   └── ...\n",
            "│   │   ├── directives\n",
            "│   │   │   └── ...\n",
            "│   │   └── services\n",
            "│   │       └── ...\n",
            "│   └── server\n",
            "│       ├── app.js\n",
            "│       ├── models\n",
            "│       │   └── ...\n",
            "│       ├── routes\n",
            "│       │   └── ...\n",
            "│       ├── services\n",
            "│       │   ├── logger.js\n",
            "│       │   └── ...\n",
            "│       └── views\n",
            "│           └── ...\n",
            "└── test\n",
            "    ├── mocha.opts\n",
            "    ├── assets\n",
            "    │   └── stylesheets\n",
            "    │       ├── navi.html\n",
            "    │       └── ...\n",
            "    ├── client\n",
            "    │   ├── services\n",
            "    │   │   └── ...\n",
            "    │   ├── fixtures\n",
            "    │   │   └── ...\n",
            "    │   └── helpers\n",
            "    │       └── ...\n",
            "    └── server\n",
            "        ├── models\n",
            "        │   └── ...\n",
            "        ├── routes\n",
            "        │   └── ...\n",
            "        ├── fixtures\n",
            "        │   └── ...\n",
            "        └── helpers\n",
            "            └── ...\n",
            "\n",
            "bin/www\n",
            "#!/usr/bin/env node\n",
            "'use strict';\n",
            "app/assets/stylesheets/app.scss\n",
            "@charset \"UTF-8\";\n",
            "@import \"./basics/reset\";\n",
            "// ...\n",
            "@import \"./layouts/navi\";\n",
            "// ...\n",
            "@import \"./modules/navi\";\n",
            "// ...\n",
            "\n",
            "\n",
            "app/client/app.js\n",
            "'use strict';'use strict';try {\n",
            "  setTimeout(function() {\n",
            "    throw new Error('async error');\n",
            "  }, 0);\n",
            "} catch (e) {\n",
            "  console.log(e); // ここに来そうに見えるけど来ない\n",
            "}\n",
            "describe('Planモデル', function() {\n",
            "  it('ラベルが空文字なら更新できない', function() {\n",
            "    //...\n",
            "  });\n",
            "});\n",
            "describe('Plan model', function() {\n",
            "  it('should not update with empty label', function() {\n",
            "    //...\n",
            "  });\n",
            "});\n",
            "\n",
            "karma.conf.js\n",
            "'use strict';\n",
            "app/server/services/template.js\n",
            "'use strict';\n",
            "app/server/views/hello.dot\n",
            "<!DOCTYPE html>\n",
            "<title>Hello</title>\n",
            "<p>{%! it.message %}\n",
            "\n",
            "\n",
            "app/server/app.js\n",
            "'use strict';$ sudo useradd -s /bin/nologin -M planetter\n",
            "\n",
            "/etc/systemd/system/planetter.service\n",
            "[Unit]\n",
            "Description=planetter backend\n",
            "After=network.target remote-fs.target nss-lookup.target$ sudo systemctl start planetter\n",
            "$ systemctl status planetter\n",
            "● planetter.service - planetter backend\n",
            "   Loaded: loaded (/etc/systemd/system/planetter.service; disabled; vendor preset: disabled)\n",
            "   Active: active (running) since ...\n",
            "$ sudo systemctl enable planetter\n",
            "Created symlink from /etc/systemd/system/multi-user.target.wants/planetter.service to /etc/systemd/system/planetter.service.\n",
            "$ systemctl status planetter\n",
            "● planetter.service - planetter backend\n",
            "  Loaded: loaded (/etc/systemd/system/planetter.service; enabled; vendor preset: disabled)\n",
            "  Active: active (running) since ...\n",
            "$ sudo systemctl stop planetter\n",
            "$ sudo systemctl disable planetter\n",
            "Removed symlink /etc/systemd/system/multi-user.target.wants/planetter.service.\n",
            "$ systemctl status planetter\n",
            "● planetter.service - planetter backend\n",
            "   Loaded: loaded (/etc/systemd/system/planetter.service; disabled; vendor preset: disabled)\n",
            "   Active: inactive (dead)\n",
            "app.get('/exit', function(req, res) {\n",
            "  process.exit(1);\n",
            "});\n",
            "HTTP/1.1 200 OK\n",
            "# pythonのバージョン確認\n",
            "!python -V\n",
            "Python 3.5.2 :: Anaconda custom (x86_64)\n",
            "# 一般的にpandasはpdと名前を付けてimportされる\n",
            "import pandas as pd\n",
            "# read_csvメソッドでお弁当需要予想に関するデータをcsvファイルとして読み込む\n",
            "df = pd.read_csv('./data/lunch_box.csv', sep=',')\n",
            "# データの確認をする（最初の3行を表示）\n",
            "df.head(3)\n",
            "# 先頭の5行を表示\n",
            "df.head()\n",
            "# 最後尾の5行を表示\n",
            "df.tail()\n",
            "print('dataframeの行数・列数の確認==>\\n', df.shape)\n",
            "print('indexの確認==>\\n', df.index)\n",
            "print('columnの確認==>\\n', df.columns)\n",
            "print('dataframeの各列のデータ型を確認==>\\n', df.dtypes)\n",
            "dataframeの行数・列数の確認==>\n",
            " (207, 12)\n",
            "indexの確認==>\n",
            " RangeIndex(start=0, stop=207, step=1)\n",
            "columnの確認==>\n",
            " Index(['datetime', 'y', 'week', 'soldout', 'name', 'kcal', 'remarks', 'event',\n",
            "       'payday', 'weather', 'precipitation', 'temperature'],\n",
            "      dtype='object')\n",
            "dataframeの各列のデータ型を確認==>\n",
            " datetime          object\n",
            "y                  int64\n",
            "week              object\n",
            "soldout            int64\n",
            "name              object\n",
            "kcal             float64\n",
            "remarks           object\n",
            "event             object\n",
            "payday           float64\n",
            "weather           object\n",
            "precipitation     object\n",
            "temperature      float64\n",
            "dtype: object\n",
            "# 任意の列だけ取り出したい場合\n",
            "df[['name', 'kcal']].head()\n",
            "# 100行目から105行目まで表示したい場合\n",
            "df[100:106]\n",
            "# indexが100の行だけ取得したい場合\n",
            "df.loc[100]\n",
            "datetime         2014-4-22\n",
            "y                       78\n",
            "week                     火\n",
            "soldout                  1\n",
            "name                 マーボ豆腐\n",
            "kcal                   382\n",
            "remarks                NaN\n",
            "event                  NaN\n",
            "payday                 NaN\n",
            "weather                  曇\n",
            "precipitation           --\n",
            "temperature           18.8\n",
            "Name: 100, dtype: object\n",
            "# もっとピンポイントに抽出したい場合\n",
            "# 例: 1,2,4 行目と 0-2 列目を取得\n",
            "df.iloc[[1,2,4],[0,2]]\n",
            "# 条件を指定して抽出\n",
            "df[df['kcal'] > 450]\n",
            "# queryメソッドを使うと、複数条件の指定で、特定カラムだけ出力もできる\n",
            "df[['name', 'kcal']].query('kcal > 450 and name == \"豚肉の生姜焼\"') #query内のシングル/ダブルクオーテーションの使い方に注意\n",
            "# 'remarks(備考)'には例えばどんなデータが入っているか確認\n",
            "df['remarks'].unique()\n",
            "array([nan, '鶏のレモンペッパー焼（50食）、カレー（42食）', '酢豚（28食）、カレー（85食）', 'お楽しみメニュー',\n",
            "       '料理長のこだわりメニュー', '手作りの味', 'スペシャルメニュー（800円）'], dtype=object)\n",
            "# datatime単位で重複したデータが存在しないか確認\n",
            "print(len(df) == len(df['datetime'].unique()))\n",
            "True\n",
            "#行方向で重複行を削除\n",
            "df.drop_duplicates() \n",
            "print(df.shape) # 重複が存在しないので数は変わらないはず\n",
            "(207, 12)\n",
            "# 要約統計量の表示\n",
            "df.describe()\n",
            "# datetime列をindexにする\n",
            "df.set_index('datetime', inplace=True)\n",
            "df.head()\n",
            "df.index\n",
            "Index(['2013-11-18', '2013-11-19', '2013-11-20', '2013-11-21', '2013-11-22',\n",
            "       '2013-11-25', '2013-11-26', '2013-11-27', '2013-11-28', '2013-11-29',\n",
            "       ...\n",
            "       '2014-9-16', '2014-9-17', '2014-9-18', '2014-9-19', '2014-9-22',\n",
            "       '2014-9-24', '2014-9-25', '2014-9-26', '2014-9-29', '2014-9-30'],\n",
            "      dtype='object', name='datetime', length=207)\n",
            "# カラム名を変更する（y を sales に変換）\n",
            "df.rename(columns={'y': 'sales'}, inplace=True)\n",
            "df.head()\n",
            "# 'sales'列を降順で並び替えもできる\n",
            "df.sort_values(by=\"sales\", ascending=True).head() # ascending=Trueで昇順\n",
            "# sort_valuesは複数の列に対しても実行できる\n",
            "df.sort_values(['sales', 'temperature'], ascending=False).head() # ascending=Falseで昇順\n",
            "# indexのデータ型を確認してみる\n",
            "df.index #  dtype='object'であることがわかる\n",
            "Index(['2013-11-18', '2013-11-19', '2013-11-20', '2013-11-21', '2013-11-22',\n",
            "       '2013-11-25', '2013-11-26', '2013-11-27', '2013-11-28', '2013-11-29',\n",
            "       ...\n",
            "       '2014-9-16', '2014-9-17', '2014-9-18', '2014-9-19', '2014-9-22',\n",
            "       '2014-9-24', '2014-9-25', '2014-9-26', '2014-9-29', '2014-9-30'],\n",
            "      dtype='object', name='datetime', length=207)\n",
            "# indexであるdatetimeのdtype='object' を dtype='datetime64[ns]' に変更\n",
            "df.index = pd.to_datetime(df.index, format='%Y-%m-%d')\n",
            "df.index # dtype='datetime64[ns]'になった\n",
            "DatetimeIndex(['2013-11-18', '2013-11-19', '2013-11-20', '2013-11-21',\n",
            "               '2013-11-22', '2013-11-25', '2013-11-26', '2013-11-27',\n",
            "               '2013-11-28', '2013-11-29',\n",
            "               ...\n",
            "               '2014-09-16', '2014-09-17', '2014-09-18', '2014-09-19',\n",
            "               '2014-09-22', '2014-09-24', '2014-09-25', '2014-09-26',\n",
            "               '2014-09-29', '2014-09-30'],\n",
            "              dtype='datetime64[ns]', name='datetime', length=207, freq=None)\n",
            "# indexに対してsortを行う\n",
            "df.sort_index().head() # object型のままだと正しくsortされない\n",
            "# resampleメソッドで、日単位や月単位で簡単に集計できる\n",
            "df.resample('M').mean() #月単位で平均値を出力\n",
            "# 簡単にmonth列やday列を作れるようになる\n",
            "df['month'] = list(pd.Series(df.index).apply(lambda x: x.month))\n",
            "df['day'] = list(pd.Series(df.index).apply(lambda x: x.day))# cutメソッドでヒストグラムのビン指定的な処理ができる\n",
            "labels = ['上旬', '中旬', '下旬']\n",
            "df['period'] = pd.cut(list(df['day']),  bins=[0,10,20,31], labels=labels, right=True) # 0<day≦10, 10<day≦20, 20<day≦31# 列単位で 欠損値NaN(not a number)が入っている個数をカウントする （正確には、isnull()でtrueが返ってくる個数をカウントしている）\n",
            "df.isnull().sum()\n",
            "sales              0\n",
            "week               0\n",
            "soldout            0\n",
            "name               0\n",
            "kcal              41\n",
            "remarks          186\n",
            "event            193\n",
            "payday           197\n",
            "weather            0\n",
            "precipitation      0\n",
            "temperature        0\n",
            "month              0\n",
            "day                0\n",
            "period             0\n",
            "dtype: int64\n",
            "# 1つでもNaNが含まれる行だけを抽出（最初の5行のみ表示）\n",
            "print(df[df.isnull().any(axis=1)].shape)\n",
            "df[df.isnull().any(axis=1)].head()\n",
            "(207, 14)\n",
            "# 'payday'列にあるNaNを'0.0'に置き換える\n",
            "df.fillna(value={'payday': 0.0}, inplace=True)\n",
            "df.head()\n",
            "df.isnull().sum() # 'payday'が0になった\n",
            "sales              0\n",
            "week               0\n",
            "soldout            0\n",
            "name               0\n",
            "kcal              41\n",
            "remarks          186\n",
            "event            193\n",
            "payday             0\n",
            "weather            0\n",
            "precipitation      0\n",
            "temperature        0\n",
            "month              0\n",
            "day                0\n",
            "period             0\n",
            "dtype: int64\n",
            "# 'kcal'列にNaNがある行を削除する\n",
            "df.dropna(subset=['kcal'], axis=0, inplace=True)\n",
            "print(df.shape) # 207-166=41行のデータを削除した\n",
            "(166, 14)\n",
            "df.isnull().sum() # kcal'がNaNがなくなって0になっていることがわかる\n",
            "sales              0\n",
            "week               0\n",
            "soldout            0\n",
            "name               0\n",
            "kcal               0\n",
            "remarks          158\n",
            "event            155\n",
            "payday             0\n",
            "weather            0\n",
            "precipitation      0\n",
            "temperature        0\n",
            "month              0\n",
            "day                0\n",
            "period             0\n",
            "dtype: int64\n",
            "# 'precipitation' 列の '--' を 0に置き換える\n",
            "df['precipitation'] = df['precipitation'].replace('--', 0).astype(float)\n",
            "df.head()\n",
            "# maskメソッドを使う例。'sales'列が80よりも大きければ、その値を100に置換する\n",
            "pd.DataFrame(df['sales'].mask(df['sales'] > 80, 100)).head()\n",
            "# 'remarks(備考)'はデータとして不要な気がするので、データから列ごと削除\n",
            "df.drop(['remarks'], axis=1, inplace=True)\n",
            "df.head()\n",
            "# weather列の集計\n",
            "df['weather'].value_counts()\n",
            "曇     44\n",
            "晴れ    41\n",
            "快晴    34\n",
            "薄曇    23\n",
            "雨     22\n",
            "雷電     1\n",
            "雪      1\n",
            "Name: weather, dtype: int64\n",
            "# groupbyメソッドで、'week'列ごとに'soldout'の数をカウントする\n",
            "df.groupby(['week'])['soldout'].count()\n",
            "week\n",
            "月    34\n",
            "木    37\n",
            "水    37\n",
            "火    35\n",
            "金    23\n",
            "Name: soldout, dtype: int64\n",
            "# groupbyメソッドは複数列に対しても行える\n",
            "# groupbyメソッドで、'month', 'period'列ごとに'sales'の数を合計する\n",
            "df.groupby(['month', 'period'])['sales'].sum()\n",
            "month  period\n",
            "1      上旬         475\n",
            "       中旬         581\n",
            "       下旬        1018\n",
            "2      上旬         725\n",
            "       中旬         715\n",
            "       下旬         609\n",
            "3      上旬         614\n",
            "       中旬         698\n",
            "       下旬         369\n",
            "4      上旬         792\n",
            "       中旬         344\n",
            "       下旬         457\n",
            "5      上旬         316\n",
            "       中旬         406\n",
            "       下旬         517\n",
            "6      上旬         467\n",
            "       中旬         440\n",
            "       下旬         311\n",
            "7      上旬         460\n",
            "       中旬         277\n",
            "       下旬         406\n",
            "8      上旬         288\n",
            "       中旬         250\n",
            "       下旬         334\n",
            "9      上旬         468\n",
            "       中旬         249\n",
            "       下旬         234\n",
            "Name: sales, dtype: int64\n",
            "# wetherごとにtemperatureの平均値を出す\n",
            "df.groupby(['weather'])['temperature'].mean()\n",
            "weather\n",
            "快晴    15.294118\n",
            "晴れ    22.558537\n",
            "曇     19.377273\n",
            "薄曇    23.926087\n",
            "雨     18.813636\n",
            "雪      1.200000\n",
            "雷電    19.000000\n",
            "Name: temperature, dtype: float64\n",
            "# 前行との差分が欲しい時は .diff() を使う\n",
            "df['temperature_diff'] = df['temperature'].diff(periods=1)\n",
            "df[['temperature','temperature_diff']].head()\n",
            "# 'temperature'列について、頭からwindowサイズ3で移動平均を計算する\n",
            "df['temperature_rolling_mean'] = df['temperature'].rolling(window=3).mean()\n",
            "df[['temperature', 'temperature_diff', 'temperature_rolling_mean']].head()\n",
            "df['temperature_pct_change'] = df['temperature'].pct_change()\n",
            "df[['temperature', 'temperature_diff', 'temperature_rolling_mean', 'temperature_pct_change']].head()\n",
            "# 'temperature'列に一つでもNaNがある行を削除\n",
            "df.dropna(subset=['temperature_diff', 'temperature_rolling_mean', 'temperature_pct_change'], axis=0, inplace=True)\n",
            "# 改めて、各列にNaNが入っていないか確認\n",
            "df.isnull().sum() \n",
            "sales                         0\n",
            "week                          0\n",
            "soldout                       0\n",
            "name                          0\n",
            "kcal                          0\n",
            "event                       153\n",
            "payday                        0\n",
            "weather                       0\n",
            "precipitation                 0\n",
            "temperature                   0\n",
            "month                         0\n",
            "day                           0\n",
            "period                        0\n",
            "temperature_diff              0\n",
            "temperature_rolling_mean      0\n",
            "temperature_pct_change        0\n",
            "dtype: int64\n",
            "df.head()\n",
            "#jupyternotebook内でmatplotlibで図を描写するときの必須のおまじない\n",
            "%matplotlib inline \n",
            "import matplotlib.pyplot as plt# 日本語フォントの設定\n",
            "from matplotlib.font_manager import FontProperties\n",
            "font_path = '/Users/ysdyt/Downloads/TakaoPGothic.ttf'  #DLしたフォントのパスを指定\n",
            "font_prop = FontProperties(fname=font_path)\n",
            "#超単純な折れ線グラフ\n",
            "df['sales'].plot()\n",
            "# 少しだけ情報量の多い折れ線グラフ\n",
            "df.plot(y=['temperature', 'temperature_rolling_mean', 'temperature_pct_change'],  figsize=(16,4), alpha=0.5) # x軸がindexである場合は明記しなくても可\n",
            "plt.title('気温変化に関する図', fontproperties=font_prop) # fontpropertiesは日本語表示に必要\n",
            "# ヒストグラム\n",
            "df.plot(kind='hist', y='sales' , bins=10, figsize=(16,4), alpha=0.5)\n",
            "# 散布図\n",
            "df.plot(kind='scatter', x='kcal', y='sales')\n",
            "# ちなみに相関係数はcorrメソッドで簡単に表示できる\n",
            "df[['kcal', 'sales']].corr()\n",
            "# ちょっとだけ複雑な集計 例: 月別・期間別の売上個数の状況を可視化したい場合\n",
            "monthly_df = pd.DataFrame(df.groupby(['month', 'period'])['sales'].sum())\n",
            "pivot_monthly_df = monthly_df.reset_index().pivot(index='month', columns='period', values='sales')\n",
            "pivot_monthly_df\n",
            "# 棒グラフ\n",
            "pivot_monthly_df.plot(kind='bar', alpha=0.6, figsize=(12,3)).legend(prop=font_prop) # legend(prop=font_prop)は日本語表示に必要\n",
            "plt.title('月別・期間別の売上個数', fontproperties=font_prop) # fontpropertiesは日本語表示に必要\n",
            "# 現在のcolumnを確認\n",
            "print(df.columns)\n",
            "print(len(df.columns))\n",
            "Index(['sales', 'week', 'soldout', 'name', 'kcal', 'event', 'payday',\n",
            "       'weather', 'precipitation', 'temperature', 'month', 'day', 'period',\n",
            "       'temperature_diff', 'temperature_rolling_mean',\n",
            "       'temperature_pct_change'],\n",
            "      dtype='object')\n",
            "16\n",
            "# テキストデータが入っている列全てをdummy化する\n",
            "dummy_df = pd.get_dummies(df, columns=['week', 'name', 'event', 'weather', 'period'])\n",
            "dummy_df.head()\n",
            "# jupyter notebookにおいて、pandasで表示が省略されるのを防ぐ\n",
            "pd.set_option('display.max_columns', 160) # 160番目の列まで全て強制表示\n",
            "print(dummy_df.shape)\n",
            "dummy_df.head()\n",
            "(164, 157)\n",
            "# 改めて、各列にNaNが入っていないか確認\n",
            "print(dummy_df.isnull().sum()) \n",
            "dummy_df.head()\n",
            "sales                       0\n",
            "soldout                     0\n",
            "kcal                        0\n",
            "payday                      0\n",
            "precipitation               0\n",
            "temperature                 0\n",
            "month                       0\n",
            "day                         0\n",
            "temperature_diff            0\n",
            "temperature_rolling_mean    0\n",
            "temperature_pct_change      0\n",
            "week_月                      0\n",
            "week_木                      0\n",
            "week_水                      0\n",
            "week_火                      0\n",
            "week_金                      0\n",
            "name_いか天ぷら                  0\n",
            "name_かじきの甘辛煮                0\n",
            "name_きのこソースハンバーグ            0\n",
            "name_さわら焼味噌掛け               0\n",
            "name_さんま辛味焼                 0\n",
            "name_たっぷりベーコンフライ            0\n",
            "name_ぶりレモンペッパー焼き            0\n",
            "name_ますのマスタードソース            0\n",
            "name_アジ唐揚げ南蛮ソース             0\n",
            "name_エビフライ                  0\n",
            "name_カレイの唐揚げ                0\n",
            "name_カレイ唐揚げ 甘酢あん            0\n",
            "name_カレイ唐揚げ夏野菜あん            0\n",
            "name_カレイ唐揚げ野菜あんかけ           0\n",
            "                           ..\n",
            "name_鶏のから揚げねぎ塩炒めソース         0\n",
            "name_鶏のから揚げスイートチリソース        0\n",
            "name_鶏のカッシュナッツ炒め            0\n",
            "name_鶏のトマトシチュー              0\n",
            "name_鶏のピリ辛焼き                0\n",
            "name_鶏のレモンペッパー焼orカレー        0\n",
            "name_鶏の味噌漬け焼き               0\n",
            "name_鶏の唐揚げ                  0\n",
            "name_鶏の唐揚げおろしソース            0\n",
            "name_鶏の塩から揚げ                0\n",
            "name_鶏の天ぷら                  0\n",
            "name_鶏の照り焼きマスタード            0\n",
            "name_鶏の照り焼きマヨ               0\n",
            "name_鶏の親子煮                  0\n",
            "name_鶏チリソース                 0\n",
            "name_鶏肉とカシューナッツ炒め           0\n",
            "name_鶏肉のカレー唐揚               0\n",
            "name_鶏肉の山賊焼き                0\n",
            "event_キャリアアップ支援セミナー         0\n",
            "event_ママの会                  0\n",
            "weather_快晴                  0\n",
            "weather_晴れ                  0\n",
            "weather_曇                   0\n",
            "weather_薄曇                  0\n",
            "weather_雨                   0\n",
            "weather_雪                   0\n",
            "weather_雷電                  0\n",
            "period_上旬                   0\n",
            "period_中旬                   0\n",
            "period_下旬                   0\n",
            "dtype: int64\n",
            "# 出来たデータをutf-8で保存する\n",
            "dummy_df.to_csv('./data/processed_datamart.csv', encoding='utf-8')\n",
            "\n",
            "セットアップ\n",
            "# TensorFlowのインストール\n",
            "pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\n",
            "mnist_beginner.py\n",
            "# -*- coding: utf-8 -*-\n",
            "from __future__ import absolute_import, unicode_literals\n",
            "import input_data\n",
            "import tensorflow as tf\n",
            "# mnistデータ読み込み\n",
            "print \"****MNISTデータ読み込み****\"\n",
            "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
            "実行結果\n",
            ">>>python ./mnist_beginner.py\n",
            "****MNISTデータ読み込み****\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "****Start Tutorial****\n",
            "****init****\n",
            "can't determine number of CPU cores: assuming 4\n",
            "I tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 4\n",
            "can't determine number of CPU cores: assuming 4\n",
            "I tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 4\n",
            "****1000回学習と結果表示****\n",
            "0.9098\n",
            "mnist_expert.py\n",
            "# -*- coding: utf-8 -*-\n",
            "from __future__ import absolute_import, unicode_literals\n",
            "import input_data\n",
            "import tensorflow as tf\n",
            "実行結果（実行に1時間くらい掛かった）\n",
            ">>>python ./mnist_expert.py\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "can't determine number of CPU cores: assuming 4\n",
            "I tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 4\n",
            "can't determine number of CPU cores: assuming 4\n",
            "I tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 4\n",
            "0.9092\n",
            "step 0, training accuracy 0.06\n",
            "step 100, training accuracy 0.68\n",
            "step 200, training accuracy 0.9\n",
            "step 300, training accuracy 0.98\n",
            "step 400, training accuracy 0.9\n",
            "step 500, training accuracy 0.94\n",
            "step 600, training accuracy 0.92\n",
            "step 700, training accuracy 0.84\n",
            "step 800, training accuracy 0.92\n",
            "step 900, training accuracy 0.94\n",
            "step 1000, training accuracy 0.98\n",
            "step 1100, training accuracy 0.96\n",
            "step 1200, training accuracy 0.98\n",
            "step 1300, training accuracy 0.96\n",
            "step 1400, training accuracy 0.98\n",
            "step 1500, training accuracy 0.98\n",
            "step 1600, training accuracy 0.96\n",
            "step 1700, training accuracy 0.96\n",
            "step 1800, training accuracy 0.96\n",
            "....\n",
            "step 19600, training accuracy 1\n",
            "step 19700, training accuracy 0.98\n",
            "step 19800, training accuracy 1\n",
            "step 19900, training accuracy 1\n",
            "test accuracy 0.992import numpy as np\n",
            "import pandas as pd\n",
            "train = pd.read_csv(\"../input/train.csv\")\n",
            "test = pd.read_csv(\"../input/test.csv\")\n",
            "gender_submission = pd.read_csv(\"../input/gender_submission.csv\")\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "clf = LogisticRegression(penalty='l2', solver=\"sag\", random_state=0)\n",
            "clf.fit(X_train, y_train)\n",
            "y_pred = clf.predict(X_test)\n",
            "sub = pd.DataFrame(pd.read_csv(\"../input/test.csv\")['PassengerId'])\n",
            "sub['Survived'] = list(map(int, y_pred))\n",
            "sub.to_csv(\"submission.csv\", index = False)\n",
            "age_avg = data['Age'].mean()\n",
            "age_std = data['Age'].std()np.random.seed(seed=777)\n",
            "clf = LogisticRegression(penalty='l2', solver=\"sag\", random_state=0)\n",
            "\n",
            "[1]\n",
            "# data analysis and wrangling\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import random as rnd#data analysis and wrangling\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import random as rnd\n",
            "# visualization\n",
            "import seaborn as sns\n",
            "import matplotlib.pyplot as plt\n",
            "%matplotlib inline\n",
            "%matplotlib inline\n",
            "# machine learning\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.svm import SVC, LinearSVC\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.neighbors import KNeighborsClassifier\n",
            "from sklearn.naive_bayes import GaussianNB\n",
            "from sklearn.linear_model import Perceptron\n",
            "from sklearn.linear_model import SGDClassifier\n",
            "from sklearn.tree import DecisionTreeClassifier\n",
            "\n",
            "[2]\n",
            "train_df = pd.read_csv('../input/train.csv')\n",
            "test_df = pd.read_csv('../input/test.csv')\n",
            "combine = [train_df, test_df]\n",
            "\n",
            "print(train_df.columns.values)\n",
            "['PassengerId' 'Survived' 'Pclass' 'Name' 'Sex' 'Age' 'SibSp' 'Parch'\n",
            " 'Ticket' 'Fare' 'Cabin' 'Embarked']\n",
            "# preview the data\n",
            "train_df.head()\n",
            "train_df.tail()\n",
            "train_df.info()\n",
            "print('_'*40)\n",
            "test_df.info()\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 12 columns):\n",
            "PassengerId    891 non-null int64\n",
            "Survived       891 non-null int64\n",
            "Pclass         891 non-null int64\n",
            "Name           891 non-null object\n",
            "Sex            891 non-null object\n",
            "Age            714 non-null float64\n",
            "SibSp          891 non-null int64\n",
            "Parch          891 non-null int64\n",
            "Ticket         891 non-null object\n",
            "Fare           891 non-null float64\n",
            "Cabin          204 non-null object\n",
            "Embarked       889 non-null object\n",
            "dtypes: float64(2), int64(5), object(5)\n",
            "memory usage: 83.6+ KB\n",
            "________________________________________\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 418 entries, 0 to 417\n",
            "Data columns (total 11 columns):\n",
            "PassengerId    418 non-null int64\n",
            "Pclass         418 non-null int64\n",
            "Name           418 non-null object\n",
            "Sex            418 non-null object\n",
            "Age            332 non-null float64\n",
            "SibSp          418 non-null int64\n",
            "Parch          418 non-null int64\n",
            "Ticket         418 non-null object\n",
            "Fare           417 non-null float64\n",
            "Cabin          91 non-null object\n",
            "Embarked       418 non-null object\n",
            "dtypes: float64(2), int64(4), object(5)\n",
            "memory usage: 36.0+ KB\n",
            "train_df.describe()\n",
            "# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.\n",
            "# Review Parch distribution using `percentiles=[.75, .8]`\n",
            "# SibSp distribution `[.68, .69]`\n",
            "# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`\n",
            "train_df.describe(include=['O'])\n",
            "train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n",
            "train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n",
            "train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n",
            "train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n",
            "\n",
            "intro.py\n",
            "import tensorflow as tf\n",
            "import numpy as np\n",
            "import time\n",
            "intron.py\n",
            "import tensorflow as tf\n",
            "import numpy as np\n",
            "import random\n",
            "import math\n",
            "import time\n",
            "soft_max_regression.py\n",
            "from tensorflow.examples.tutorials.mnist import input_data\n",
            "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
            "sk_learn_sample.py\n",
            "# -*- coding: utf-8 -*-\n",
            "from sklearn.svm import LinearSVC\n",
            "from sklearn.ensemble import AdaBoostClassifier,ExtraTreesClassifier ,GradientBoostingClassifier, RandomForestClassifier\n",
            "from sklearn.decomposition import TruncatedSVD\n",
            "from sklearn import datasets\n",
            "from sklearn.cross_validation import cross_val_scorefrom chainer import Link, Chain, ChainList\n",
            "import chainer.functions as F\n",
            "import chainer.links as L    def __call__(self, x):\n",
            "        h = F.sigmoid(self.l1(x))\n",
            "        o = self.l2(h)\n",
            "        return o\n",
            "from chainer.functions.loss.mean_squared_error import mean_squared_error# Setup optimizer\n",
            "optimizer = optimizers.Adam()\n",
            "optimizer.setup(model)serializers.save_hdf5('my.model', model)\n",
            "serializers.load_hdf5('my.model', model)\n",
            "# import関連\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pylab as plt\n",
            "import seaborn as sns# 2016年の合計を計算する\n",
            "# 前々日終値に比べて前日終値が高い場合は、買い、低い場合は売りで入ります\n",
            "sum_2016 = 0\n",
            "for i in range(2,len(data16)): # len()で要素数を取得しています\n",
            "    if data16.iloc[i-2,4] <= data16.iloc[i-1,4]:\n",
            "        sum_2016 += data16.iloc[i,5]\n",
            "    else:\n",
            "        sum_2016 -= data16.iloc[i,5]# 2017年の合計を計算する\n",
            "# 前々日終値に比べて前日終値が高い場合は、買い、低い場合は売りで入ります\n",
            "sum_2017 = 0\n",
            "for i in range(2,len(data17)): # len()で要素数を取得しています\n",
            "    if data17.iloc[i-2,4] <= data17.iloc[i-1,4]:\n",
            "        sum_2017 += data17.iloc[i,5]\n",
            "    else:\n",
            "        sum_2017 -= data17.iloc[i,5]# 2016年のデータをプロットしてみます\n",
            "plt.style.use('seaborn-darkgrid')\n",
            "plt.plot(data16['Close'])\n",
            "plt.ylim([95,125])\n",
            "# 2017年からのデータをプロットしてみます\n",
            "plt.plot(data17['Close'])\n",
            "plt.ylim([95,125])\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "from scipy import optimize\n",
            "MAIST-beginner.py\n",
            "train_step = tf.train.GradientDescentOptimizer(1e-4).minimize(cross_entropy)\n",
            "#学習回数が多いと発散してしまうので、学習レートを1e-4に変更\n",
            "特徴検出.py\n",
            " x_image = tf.reshape(x, [-1,28,28,1])\n",
            "活性化.py\n",
            "def bias_variable(shape):                       \n",
            "  initial = tf.constant(0.1, shape=shape)\n",
            "  return tf.Variable(initial)\n",
            "活性化はこんな感じ.py\n",
            "-> x\n",
            "[  1.43326855 -10.14613152   2.10967159   6.07900429  -3.25419664  \n",
            "-1.93730605  -8.57098293  10.21759605   1.16319525   2.90590048]\n",
            "次元削減.py\n",
            "def max_pool_2x2(x): \n",
            "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],                 \n",
            "                         strides=[1, 2, 2, 1], padding='SAME') \n",
            "h_pool1 = max_pool_2x2(h_conv1)\n",
            "\n",
            "\n",
            "2回目.py\n",
            "W_conv2 = weight_variable([5, 5, 32, 64])                   \n",
            "b_conv2 = bias_variable([64])                              \n",
            "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)  \n",
            "h_pool2 = max_pool_2x2(h_conv2)                           \n",
            "\n",
            "\n",
            "Hidden隠れ層.py\n",
            "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])             \n",
            "W_fc1 = weight_variable([3136, 1024]) #[7*7*64, 1024] 3136はTensorのsize, 1024は適当。 業界的に大抵は1024もしくは1024*nの倍数らしい。           \n",
            "b_fc1 = bias_variable([1024])                               \n",
            "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) \n",
            "#Dropout                                    \n",
            "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) \n",
            "\n",
            "\n",
            "10000steps.py\n",
            "simple_maist 10000 steps accuracy 0.287933        \n",
            "学習発散防止.py\n",
            "L =  1e-3 #学習レート\n",
            "train_step = tf.train.AdamOptimizer(L).minimize(cross_entropy)                                                             \n",
            "for i in range(20000):                                               \n",
            "  batch = random_index(50)                                           \n",
            "  if i == 1000:   \n",
            "    L = 1e-4      \n",
            "  if i == 5000:   \n",
            "    L = 1e-5      \n",
            "  if i == 10000:  \n",
            "    L = 1e-6      \n",
            "過学習防止.py\n",
            "for i in range(20000):\n",
            "  batch = random_index(50)\n",
            "#tune the learning rate\n",
            "  if i == 1000: \n",
            "    L = 1e-4    \n",
            "  if i == 3000: \n",
            "    L = 1e-5    \n",
            "  if i == 7000: \n",
            "    L = 1e-6    \n",
            "  if i == 10000:\n",
            "    L = 1e-7    \n",
            "  if i == 14000:\n",
            "    L = 1e-8    \n",
            "  if i == 19000:\n",
            "    L = 1e-9    A = np.repeat('Cat_A', 15)\n",
            "B = np.repeat('Cat_B', 5)\n",
            "C = np.repeat('Cat_C', 135)\n",
            "D = np.repeat('Cat_D', 23)\n",
            "E = np.repeat('Cat_E', 86)\n",
            "F = np.repeat('Cat_F', 44)\n",
            "G = np.repeat('Cat_G', 13)\n",
            "H = np.repeat('Cat_H', 3)\n",
            "I = np.repeat('Cat_I', 3)\n",
            "J = np.repeat('Cat_J', 2)\n",
            "K = np.repeat('Cat_K', 2)\n",
            "data = np.concatenate((A, B, C, D, E, F, G, H, I, J, K))\n",
            "data = pd.Series(data)\n",
            "# シンプルに集計し、可視化\n",
            "plt.figure(figsize=(15, 3))\n",
            "sns.countplot(data)\n",
            "t = data.value_counts()# カテゴリ毎の合計\n",
            "r = t/t.sum()# 割合に変換\n",
            "r_ = r.cumsum()# 累積割合に変換\n",
            "# 上記で集計したカテゴリ枚の合計（t）と累積割合（r_）を可視化\n",
            "# 全体構成80%のボーダーライン\n",
            "fig, ax1 = plt.subplots()\n",
            "t.plot.bar(figsize=(15, 3), color='blue', ax=ax1)\n",
            "ax2 = ax1.twinx()\n",
            "r_.plot(figsize=(15, 3), color='orange', ax=ax2, marker='o')\n",
            "plt.hlines(y=0.8, xmin=-1, xmax=len(t), lw=.7, color='indianred', linestyle='--')\n",
            "# sklearnのボストン住宅価格のデータを利用\n",
            "from sklearn.datasets import load_boston\n",
            "boston = load_boston()\n",
            "data = boston.data\n",
            "# y(住宅価格)を予測する変数としてRM（部屋数）とDIS（職業訓練施設からの距離）を利用\n",
            "y = boston.target\n",
            "x1 = data[:, 5]#RM\n",
            "x2 = data[:, 7]#DISplt.figure(figsize=(10, 5))\n",
            "plt.subplot(121)\n",
            "sns.regplot(x=x1, y=y, ci=95, order=1, \n",
            "            line_kws={\"linewidth\": .7}, scatter_kws={'s': 4}, color='green')\n",
            "plt.subplot(122)\n",
            "sns.regplot(x=x2, y=y, ci=95, order=3, \n",
            "            line_kws={\"linewidth\": .7}, scatter_kws={'s': 4}, color='green')\n",
            "# 適当なノイズを加えた時系列データを作成\n",
            "x = np.arange(0, 365)\n",
            "y1 = np.sin(0.1*x)*5\n",
            "y2 = np.sin(1*x)+5\n",
            "y3 = np.sin(0.01*x)*10\n",
            "n = np.random.rand(len(x))*50\n",
            "y = y1 + y2  + y3+ n\n",
            "# 時系列データの可視化\n",
            "plt.figure(figsize=(15, 3))\n",
            "plt.plot(y)\n",
            "import statsmodels.api as sm\n",
            "res = sm.tsa.seasonal_decompose(y, freq=90)\n",
            "# freq(feaquency)はデータ特性から任意に指定\n",
            "plt.subplots_adjust(hspace=0.3)\n",
            "plt.figure(figsize=(15, 9))\n",
            "plt.subplot(411)\n",
            "plt.plot(res.observed, lw=.6, c='darkblue')\n",
            "plt.title('observed')\n",
            "plt.subplot(412)\n",
            "plt.plot(res.trend, lw=.6, c='indianred')\n",
            "plt.title('trend')\n",
            "plt.subplot(413)\n",
            "plt.plot(res.seasonal, lw=.6, c='indianred')\n",
            "plt.title('seasonal')\n",
            "plt.subplot(414)\n",
            "plt.plot(res.resid, lw=.6, c='indianred')\n",
            "plt.title('residual')\n",
            "ts1 = y# ↑で生成した時系列データの再利用resid_mat = pd.DataFrame()\n",
            "for ts in[ts1, ts2, ts3, ts4, ts5]:\n",
            "    res = sm.tsa.seasonal_decompose(ts, freq=90)\n",
            "    resid_mat = pd.concat([resid_mat, pd.Series(res.resid)], axis=1)from sklearn.datasets import load_iris\n",
            "# Pandas のデータフレームとして表示\n",
            "iris = load_iris()\n",
            "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
            "x = iris.data\n",
            "y = iris.targetfrom sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import accuracy_score\\nabla f = \\frac{d f({\\bf x})}{d {\\bf x}} = \\left[ \\begin{array}{r} \\frac{\\partial f}{\\partial x_1} \\\\ ... \\\\ \\frac{\\partial f}{\\partial x_2} \\end{array} \\right]\n",
            "x_{i+1} = x_i - \\eta \\nabla f\n",
            "y_n = \\alpha x_n + \\beta + \\epsilon_n　　　(\\alpha=1, \\beta=0)\\\\\n",
            "\\epsilon_n ∼ N(0, 2)　　　　　　　　　　　　　　　　\n",
            "\n",
            "E({\\bf w})=\\sum_{n=1}^{N} E_n({\\bf w}) = \\sum_{n=1}^{N} (y_n -\\alpha x_n - \\beta)^2 \\\\\n",
            "{\\bf w} = (\\alpha, \\beta)^{\\rm T}　　　　　　　　　　　　　　　　　　　　　　　\n",
            "\n",
            "\\frac{\\partial E({\\bf w})}{\\partial \\alpha} = \\sum_{n=1}^{N} \\frac{\\partial E_n({\\bf w})}{\\partial \\alpha} = \n",
            "\\sum_{n=1}^N (2 x_n^2  \\alpha +  2 x_n \\beta - 2 x_n y_n )\\\\\\nabla E({\\bf w}) = \\frac{d E({\\bf w}) }{d {\\bf w}} = \\left[ \\begin{array}{r} \\frac{\\partial E({\\bf w})}{\\partial \\alpha} \\\\ \\frac{\\partial E({\\bf w})}{\\partial \\beta} \\end{array} \\right] = \\sum_{n=1}^{N}\n",
            "\\nabla E_n({\\bf w}) = \\sum_{n=1}^{N} \\left[ \\begin{array}{r} \\frac{\\partial E_n({\\bf w})}{\\partial \\alpha} \\\\ \\frac{\\partial E_n({\\bf w})}{\\partial \\beta} \\end{array} \\right]=\n",
            "\\left[ \\begin{array}{r} \n",
            "2 x_n^2  \\alpha +  2 x_n \\beta - 2 x_n y_n \\\\ \n",
            "2\\beta + 2  x_n \\alpha - 2y_n 　\n",
            "\\end{array} \\right]\n",
            "y_n = f(x_n, \\alpha, \\beta) + \\epsilon_n　　　　(n =1,2,...,N)\n",
            "\\epsilon_n = y_n - f(x_n, \\alpha, \\beta) 　　　　(n =1,2,...,N)  \\\\\n",
            "\\epsilon_n = y_n -\\alpha x_n - \\beta　　　　　　　　　　　　　　　　　　　\n",
            "\n",
            "L = \\prod_{n=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{\\epsilon_n^2}{2\\sigma^2} \\right)\n",
            "\n",
            "\\log L = -\\frac{N}{2} \\log (2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2} \\sum_{n=1}^{N} \\epsilon_n^2\n",
            "l(\\alpha, \\beta) = \\sum_{n=1}^{N} \\epsilon_n^2 = \\sum_{n=1}^{N} (y_n -\\alpha x_n - \\beta)^2\n",
            "\n",
            "E({\\bf w})=\\sum_{n=1}^{N} E_n({\\bf w}) = \\sum_{n=1}^{N} (y_n -\\alpha x_n - \\beta)^2 \\\\\n",
            "{\\bf w} = (\\alpha, \\beta)^{\\rm T}　　　　　　　　　　　　　　　　　　　　　　　\n",
            "% Compute covariance matrix\n",
            "sigma = (X'*X)/size(X,1);\n",
            "% Singular value decomposition\n",
            "[U,S,V] = svd(sigma);\n",
            "\n",
            "J(\\theta) = \\frac{1}{2m} (X\\theta - y)^{\\mathrm{T}}(X\\theta - y)J = 0;\n",
            "for i = 1:m\n",
            "    for k = 1:n\n",
            "        J = J + (X(i,k)*theta(k) - y(i))^2;\n",
            "    end\n",
            "end\n",
            "J = J/(2*m);\n",
            "J = ((X*theta - y)'*(X*theta - y))/(2*m);\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "from keras.datasets import cifar10import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "from keras.datasets import cifar10\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "import time\n",
            "出力\n",
            "Elapsed[s] :  329.31931829452515\n",
            "Train : 0.41996\n",
            "Test : 0.4072\n",
            "\n",
            "from sklearn.svm import LinearSVC\n",
            "svc = LinearSVC()\n",
            "\n",
            "出力\n",
            "Elapsed[s] :  275.0674293041229\n",
            "Train : 0.45122\n",
            "Test : 0.4062\n",
            "\n",
            "# ノルムで標準化\n",
            "x_train = x_train / np.linalg.norm(x_train, ord=2, axis=1, keepdims=True)\n",
            "x_test = x_test / np.linalg.norm(x_test, ord=2, axis=1, keepdims=True)\n",
            "\n",
            "SVM標準化なし\n",
            "Elapsed[s] :  3692.342868089676\n",
            "Train : 0.4203\n",
            "Test : 0.3093\n",
            "\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "rf = RandomForestClassifier()\n",
            "\n",
            "出力\n",
            "# デフォルト→Overfitting\n",
            "Elapsed[s] :  30.51237440109253\n",
            "Train : 0.99394\n",
            "Test : 0.3576\n",
            "\n",
            "rf = RandomForestClassifier(max_depth=8)\n",
            "\n",
            "オーバーフィッティング対策\n",
            "# max_depth=8\n",
            "Elapsed[s] :  18.324826955795288\n",
            "Train : 0.434\n",
            "Test : 0.3717\n",
            "\n",
            "# グレースケール化\n",
            "def to_grayscale(tensor):\n",
            "    return 0.2126*tensor[:, :, :, 0] + 0.7152*tensor[:, :, :, 1] + 0.0722*tensor[:, :, :, 2]\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "from PIL import Imageimport numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "from keras.datasets import cifar10\n",
            "from sklearn.svm import LinearSVC\n",
            "import time\n",
            "出力\n",
            "Elapsed[s] :  91.28059148788452\n",
            "Train : 0.3313\n",
            "Test : 0.2901\n",
            "\n",
            "\\begin{align}\n",
            "F^H = \\left[\\begin{array}{rrr}-1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{array}\\right], F^V = \\left[\\begin{array}{rrr}-1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{array}\\right]\n",
            "\\end{align}\n",
            "\\begin{align}\n",
            "g_{i, j}^H=\\sum_{k=0}^2\\sum_{l=0}^2 I_{i+k, j+l}F_{k,l}^H \\\\\n",
            "g_{i, j}^V=\\sum_{k=0}^2\\sum_{l=0}^2 I_{i+k, j+l}F_{k,l}^V\n",
            "\\end{align}\n",
            "\\begin{align}\n",
            "I = \\left[\\begin{array}{rrr}0 & 0 & 1 & 0 & 0 \\\\\n",
            "0 & 1 & 1 & 1 & 0 \\\\\n",
            "1 & 1 & 1 & 1 & 1 \\\\\n",
            "0 & 1 & 1 & 1 & 0 \\\\\n",
            "0 & 0 & 1 & 0 & 0\n",
            "\\end{array}\\right]\n",
            "\\end{align}\n",
            "\\begin{align}\n",
            "g^H = \\left[\\begin{array}{rrr}\n",
            "3 & 0 & -3 \\\\\n",
            "2 & 0 & -2 \\\\\n",
            "3 & 0 & -3\n",
            "\\end{array}\\right],g^V = \\left[\\begin{array}{rrr}\n",
            "3 & 2 & 3 \\\\\n",
            "0 & 0 & 0 \\\\\n",
            "-3 & -2 & -3\n",
            "\\end{array}\\right] \n",
            "\\end{align}\n",
            "\\begin{align}\n",
            "G = \\left[\\begin{array}{rrr}\n",
            "4.24 & 2.00 & 4.24 \\\\\n",
            "2.00 & 0 & 2.00 \\\\\n",
            "4.24 & 2.00 & 4.24\n",
            "\\end{array}\\right] \n",
            "\\end{align}\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "from PIL import Imageif __name__ == \"__main__\":\n",
            "    # 画像の読み込み\n",
            "    img = np.array(Image.open(\"lenna.png\"))\n",
            "    # 畳み込みサンプル\n",
            "    conv_sample(img)\n",
            "start_time = time.time()\n",
            "# データの読み込み\n",
            "(x_train_origin, y_train), (x_test_origin, y_test) = cifar10.load_data()\n",
            "# Infを出さないように255で割る\n",
            "x_train_origin = x_train_origin / 255\n",
            "x_test_origin = x_test_origin / 255\n",
            "# データ数\n",
            "m_train, m_test = x_train_origin.shape[0], x_test_origin.shape[0]\n",
            "# 畳み込み変換用\n",
            "x_train = np.zeros((m_train, x_train_origin.shape[1]-2, x_train_origin.shape[2]-2, 3))\n",
            "x_test = np.zeros((m_test, x_test_origin.shape[1]-2, x_test_origin.shape[2]-2, 3))\n",
            "# 畳み込み\n",
            "for i in range(m_train):\n",
            "    x_train[i, :, :, :] = convolution_filter(x_train_origin[i, :, :, :])\n",
            "    x_train[i, :, :, :] = x_train[i, :, :, :] / np.max(x_train[i, :, :, :], axis=(0,1))\n",
            "for i in range(m_test):\n",
            "    x_test[i, :, :, :] = convolution_filter(x_test_origin[i, :, :, :])\n",
            "    x_test[i, :, :, :] = x_test[i, :, :, :] / np.max(x_test[i, :, :, :], axis=(0,1))\n",
            "# ベクトル化\n",
            "x_train, x_test = x_train.reshape(m_train, -1), x_test.reshape(m_test, -1)\n",
            "# ノルムで標準化\n",
            "x_train = x_train / np.linalg.norm(x_train, ord=2, axis=1, keepdims=True)\n",
            "x_test = x_test / np.linalg.norm(x_test, ord=2, axis=1, keepdims=True)\n",
            "出力\n",
            "Elapsed[s] :  1282.1308102607727\n",
            "Train : 0.26964\n",
            "Test : 0.2153\n",
            "\n",
            "\\begin{align}\n",
            "I = \\left[\\begin{array}{rrr}\n",
            "9 & 4 & -2 & 0 \\\\\n",
            "7 & 2 & 3 & -5 \\\\\n",
            "1 & 0 & -7 & 3\\\\\n",
            "4 & 5 & -4 & 7\n",
            "\\end{array}\\right] \\to O = \\left[\\begin{array}{rrr}\n",
            "9 & 3 \\\\\n",
            "5 & 7 \n",
            "\\end{array}\\right] \n",
            "\\end{align}\n",
            "# 画像をプーリング（パディング処理は未実装）\n",
            "def pooling(img):\n",
            "    #3x3でプーリング、strideも3\n",
            "    kernel_size = 3\n",
            "    out_img = np.zeros((int(img.shape[0]/kernel_size), int(img.shape[1]/kernel_size), 3))\n",
            "    for i in range(out_img.shape[0]):\n",
            "        for j in range(out_img.shape[1]):\n",
            "            img_slice = img[(i*kernel_size):((i+1)*kernel_size), (j*kernel_size):((j+1)*kernel_size), :]\n",
            "            out_img[i, j, :] = np.max(img_slice, axis=(1,2))\n",
            "    return out_imgdef preprocess(img):\n",
            "    out_img = convolution_filter(img)\n",
            "    out_img = pooling(out_img)\n",
            "    out_img = convolution_filter(out_img)\n",
            "    out_img = out_img / np.max(out_img, axis=(0,1))\n",
            "    return out_img\n",
            "# 畳み込み\n",
            "for i in range(m_train):\n",
            "    x_train[i, :, :, :] = preprocess(x_train_origin[i, :, :, :])\n",
            "for i in range(m_test):\n",
            "    x_test[i, :, :, :] = preprocess(x_test_origin[i, :, :, :])\n",
            "\n",
            "出力\n",
            "Elapsed[s] :  1036.8586645126343\n",
            "Train : 0.22648\n",
            "Test : 0.2222\n",
            "\n",
            "from keras.models import Sequential\n",
            "from keras.layers import Dense\n",
            "from keras.optimizers import Adam\n",
            "from keras.datasets import cifar10\n",
            "from keras.utils.np_utils import to_categorical\n",
            "import numpy as np\n",
            "import time\n",
            "import matplotlib.pyplot as plt_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #\n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 768)               2360064\n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 192)               147648\n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1930\n",
            "=================================================================\n",
            "Total params: 2,509,642\n",
            "Trainable params: 2,509,642\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "出力\n",
            "# epoch=30の場合\n",
            "Elapsed[s] :  1366.77405834198\n",
            "10000/10000 [==============================] - 2s 167us/step\n",
            "train accuracy : 0.68696\n",
            "test accuracy : 0.5358from keras.layers import Dense, Conv2D, MaxPool2D, Activation, Flatten, BatchNormalization_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #\n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 30, 30, 10)        280\n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 30, 30, 10)        0\n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 10)        0\n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 8, 8, 20)          1820\n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 8, 8, 20)          0\n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 8, 8, 20)          80\n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1280)              0\n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                12810\n",
            "=================================================================\n",
            "Total params: 14,990\n",
            "Trainable params: 14,950\n",
            "Non-trainable params: 40\n",
            "_________________________________________________________________\n",
            "\n",
            "出力\n",
            "# BatchNormあり、epoch=30\n",
            "Elapsed[s] :  848.2327120304108\n",
            "10000/10000 [==============================] - 3s 283us/step\n",
            "train accuracy : 0.70732\n",
            "test accuracy : 0.6358# VGG16の読み込み\n",
            "base_model = VGG16(include_top=False, input_shape=x_train.shape[1:])\n",
            "# 転移学習用にVGGを訓練不可にする\n",
            "for layer in base_model.layers:\n",
            "    layer.trainable = False\n",
            "# Flatten以降を作る\n",
            "top_model = Sequential()\n",
            "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
            "top_model.add(Dense(100, activation=\"relu\"))\n",
            "top_model.add(Dense(10, activation=\"softmax\"))_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #\n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 64, 64, 3)         0\n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792\n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928\n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0\n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856\n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584\n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0\n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168\n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080\n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080\n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0\n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160\n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808\n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808\n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0\n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808\n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808\n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808\n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0\n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 10)                205910\n",
            "=================================================================\n",
            "Total params: 14,920,598\n",
            "Trainable params: 205,910\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "# データの読み込み\n",
            "(x_train_orig, y_train), (x_test_orig, y_test) = cifar10.load_data()\n",
            "# 小数化\n",
            "x_train_orig = x_train_orig / 255.0\n",
            "x_test_orig = x_test_orig / 255.0\n",
            "# データ数\n",
            "m_train, m_test = x_train_orig.shape[0], x_test_orig.shape[0]\n",
            "# yをOneHotVector化\n",
            "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
            "# VGG16に読み込めるように32x32→64x64にリサイズ\n",
            "x_train, x_test = np.zeros((m_train, 64, 64, 3), dtype=\"float\"), np.zeros((m_test, 64, 64, 3), dtype=\"float\")\n",
            "# Nearest-Neighbor\n",
            "for i in range(x_train_orig.shape[1]):\n",
            "    for j in range(x_train_orig.shape[2]):\n",
            "        x_train[:, i*2, j*2, :] = x_train_orig[:, i, j, :]\n",
            "        x_train[:, i*2, j*2+1, :] = x_train_orig[:, i, j, :]\n",
            "        x_train[:, i*2+1, j*2, :] = x_train_orig[:, i, j, :]\n",
            "        x_train[:, i*2+1, j*2+1, :] = x_train_orig[:, i, j, :]\n",
            "        x_test[:, i*2, j*2, :] = x_test_orig[:, i, j, :]\n",
            "        x_test[:, i*2, j*2+1, :] = x_test_orig[:, i, j, :]\n",
            "        x_test[:, i*2+1, j*2, :] = x_test_orig[:, i, j, :]\n",
            "        x_test[:, i*2+1, j*2+1, :] = x_test_orig[:, i, j, :]\n",
            "# メモリばかぐいするのでオリジナルを解法\n",
            "x_test_orig, x_train_orig = None, None\n",
            "# コンパイル\n",
            "model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
            "出力\n",
            "# 2 epoch\n",
            "Elapsed[s] :  4225.965629577637\n",
            "10000/10000 [==============================] - 419s 42ms/step\n",
            "train accuracy : 0.70494\n",
            "test accuracy : 0.6898\n",
            "\n",
            "\n",
            "出力\n",
            "# 25 epoch\n",
            "#acc: 0.9172\n",
            "#Elapsed[s] :  55869.90407681465\n",
            "#10000/10000 [==============================] - 459s 46ms/step\n",
            "#train accuracy : 0.91724\n",
            "#test accuracy : 0.6987\n",
            "\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import time\n",
            "import os\n",
            "from keras.applications.vgg16 import VGG16\n",
            "from keras.models import Sequential\n",
            "from keras.layers import Dense, Dropout\n",
            "from keras.optimizers import Adam\n",
            "from keras.datasets import cifar10\n",
            "from keras.utils.np_utils import to_categorical# 転移学習部分のモデル\n",
            "print(enc_train.shape)\n",
            "model = Sequential()\n",
            "model.add(Dense(512, activation=\"relu\", input_shape=enc_train.shape[1:]))\n",
            "model.add(Dropout(0.5))\n",
            "model.add(Dense(100, activation=\"relu\", input_shape=enc_train.shape[1:]))\n",
            "model.add(Dense(10, activation=\"softmax\", input_shape=enc_train.shape[1:]))\n",
            "# コンパイル\n",
            "model.compile(optimizer=Adam(lr=0.01), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
            "\n",
            "出力\n",
            "Elapsed[s] :  1907.3146982192993\n",
            "10000/10000 [==============================] - 1s 85us/step\n",
            "train accuracy : 0.71752\n",
            "test accuracy : 0.6928\n",
            "\n",
            "\n",
            "出力\n",
            "Elapsed[s] :  1753.4181559085846\n",
            "10000/10000 [==============================] - 1s 73us/step\n",
            "train accuracy : 0.81536\n",
            "test accuracy : 0.6797\n",
            "\n",
            "import numpy as np\n",
            "import time\n",
            "from sklearn.svm import LinearSVC\n",
            "出力\n",
            "Elapsed[s] :  26.870912075042725\n",
            "Train : 0.73596\n",
            "Test : 0.7074\n",
            "\n",
            "build/tools/convert_imageset train_dataset/ train.txt train_leveldb 1 -backend leveldb 28 28\n",
            "build/tools/caffe train -solver solver.prototxt\n",
            "net = caffe.Classifier('_deploy.prototxt','_iter_10000.caffemodel', image_dims = (33, 33))\n",
            "net.set_phase_test()# test phaseで定義されたcaffe netを使用する\n",
            "net.set_mode_gpu()# GPUモード指定\n",
            "net.set_raw_scale('data', 255)  # data layerに入力として与えられる画像の輝度上限を指定\n",
            "[(k, v.data.shape) for k, v in net.blobs.items()]\n",
            "[(k, v[0].data.shape) for k, v in net.params.items()]\n",
            "scores = net.predict([caffe.io.load_image('car.jpg', color = False, )], oversample=False)\n",
            "net.blobs['data'].data[0]\n",
            "net.params['conv1'][0].data\n",
            "import leveldb\n",
            "db = leveldb.LevelDB('./test_leveldb/')\n",
            "test_data_raw =  [v for v in db.RangeIter()]\n",
            "# データを生成してプロットする\n",
            "np.random.seed(0)\n",
            "X, y = sklearn.datasets.make_moons(200, noise=0.20)\n",
            "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)\n",
            "# ロジスティック回帰モデルを学習させる\n",
            "clf = sklearn.linear_model.LogisticRegressionCV()\n",
            "clf.fit(X, y) num_examples = len(X) # 学習用データサイズ\n",
            " nn_input_dim = 2 # インプット層の次元数\n",
            " nn_output_dim = 2 # アウトプット層の次元数# 全Lossを計算するためのHelper function \n",
            "def calculate_loss(model):\n",
            "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
            "    # 予測を算出するためのForward propagation\n",
            "    z1 = X.dot(W1) + b1\n",
            "    a1 = np.tanh(z1)\n",
            "    z2 = a1.dot(W2) + b2\n",
            "    exp_scores = np.exp(z2)\n",
            "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
            "    # Lossを計算\n",
            "    corect_logprobs = -np.log(probs[range(num_examples), y])\n",
            "    data_loss = np.sum(corect_logprobs)\n",
            "    # Lossにregulatization termを与える (optional)\n",
            "    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
            "    return 1./num_examples * data_loss\n",
            "# Helper function to predict an output (0 or 1)\n",
            "def predict(model, x):\n",
            "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
            "    # Forward propagation\n",
            "    z1 = x.dot(W1) + b1\n",
            "    a1 = np.tanh(z1)\n",
            "    z2 = a1.dot(W2) + b2\n",
            "    exp_scores = np.exp(z2)\n",
            "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
            "    return np.argmax(probs, axis=1)\n",
            "# This function learns parameters for the neural network and returns the model.\n",
            "# - nn_hdim: Number of nodes in the hidden layer\n",
            "# - num_passes: Number of passes through the training data for gradient descent\n",
            "# - print_loss: If True, print the loss every 1000 iterations\n",
            "def build_model(nn_hdim, num_passes=20000, print_loss=False):# 3次元の隠れ層を持つモデルを構築\n",
            "model = build_model(3, print_loss=True)plt.figure(figsize=(16, 32))\n",
            "hidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50]\n",
            "for i, nn_hdim in enumerate(hidden_layer_dimensions):\n",
            "    plt.subplot(5, 2, i+1)\n",
            "    plt.title('Hidden Layer size %d' % nn_hdim)\n",
            "    model = build_model(nn_hdim)\n",
            "    plot_decision_boundary(lambda x: predict(model, x))\n",
            "plt.show()\n",
            "import autograd.numpy as np # Numpy用の薄いラッパ\n",
            "from autograd import grad\n",
            "def taylor_sine(x):   # サイン関数のテイラー近似（マクローリン展開）\n",
            "    ans = currterm = x\n",
            "    i = 0\n",
            "    while np.abs(currterm) > 0.001:\n",
            "        currterm = -currterm * x**2 / ((2 * i + 3) * (2 * i + 2))\n",
            "        ans = ans + currterm\n",
            "        i += 1\n",
            "    return ans\n",
            "import matplotlib.pyplot as plt\n",
            "%matplotlib inline\n",
            "# 損失関数\n",
            "xs = np.arange(0, 2 * np.pi, 0.01)\n",
            "ys = np.asarray([taylor_sine(x) for x in xs])\n",
            "plt.plot(xs, ys);\n",
            "plt.ylim([-1.0, 1.0]);\n",
            "# grad関数に損失関数を渡して、勾配関数を受け取る\n",
            "grad_sine = grad(taylor_sine)\n",
            "#  パラメータの各点における勾配を計算して描画\n",
            "dydxs = np.asarray([grad_sine(x) for x in xs])\n",
            "plt.plot(xs, dydxs);\n",
            "plt.ylim([-1.0, 1.0]);\n",
            "import autograd.numpy as np\n",
            "from autograd import grad# 小さなデータセットを作成\n",
            "inputs = np.array([[0.52, 1.12,  0.77],\n",
            "                   [0.88, -1.08, 0.15],\n",
            "                   [0.52, 0.06, -1.30],\n",
            "                   [0.74, -2.49, 1.39]])\n",
            "targets = np.array([True, True, False, True])\n",
            "# autogradを用いて、トレーニング用損失関数のモデルパラメータに対する勾配を求める\n",
            "training_gradient_fun = grad(training_loss)\n",
            "# モデルパラメータを勾配法を用いて逐次最適化plt.plot(losses)\n",
            "plt.ylabel(\"Training loss\")\n",
            "plt.xlabel(\"Iterations\")\n",
            "jupyter notebook\n",
            "# python-poloniexのインストール\n",
            "!pip3 install https://github.com/s4w3d0ff/python-poloniex/archive/v0.4.6.zip\n",
            "import poloniex\n",
            "import timedef returnChartData(self, currencyPair, period=False, start=False, end=False):\n",
            "    \"\"\" Returns candlestick chart data. Parameters are \"currencyPair\",\n",
            "    \"period\" (candlestick period in seconds; valid values are 300, 900,\n",
            "    1800, 7200, 14400, and 86400), \"start\", and \"end\". \"Start\" and \"end\"\n",
            "    are given in UNIX timestamp format and used to specify the date range\n",
            "    for the data returned (default date range is start='1 day ago' to\n",
            "    end='now') \"\"\"\n",
            "# pandasのインポート\n",
            "import pandas as pdModuleNotFoundError: No module named 'pandas'\n",
            "!pip3 install pandas  # !を付ければJupyter Notebook内でもインストールできる\n",
            "df.head(10)\n",
            "# 短期線：窓幅1日（5分×12×24）\n",
            "data_s = pd.rolling_mean(df['close'], 12 * 24) # matplotlibの読み込み（エラーが出た時はpip or pip3でインストール）\n",
            "import matplotlib.pyplot as plt# 描画を綺麗に表示する\n",
            "from matplotlib.pylab import rcParams\n",
            "import seaborn as sns\n",
            "rcParams['figure.figsize'] = 15, 6\n",
            "# プロットの色を指定しよう（color）\n",
            "plt.plot(df['close'], color='#7f8c8d')\n",
            "plt.show()\n",
            "# 短期線と長期線もプロット\n",
            "plt.plot(df['close'], color='#7f8c8d')\n",
            "plt.plot(data_s, color='#f1c40f')  # 短期線\n",
            "plt.plot(data_l, color='#2980b9')  # 長期線\n",
            "plt.show()\n",
            "# 線形代数の演算でよく使うnumpyの読み込み\n",
            "import numpy as np# numpyの形式に変換する（何かと便利なため）\n",
            "x = np.array(x)\n",
            "t = np.array(t).reshape(len(t), 1)  # reshapeは後々のChainerでエラーが出ない対策\n",
            "# 70%を訓練用、30%を検証用\n",
            "N_train = int(N * 0.7)\n",
            "x_train, x_test = x[:N_train], x[N_train:]\n",
            "t_train, t_test = t[:N_train], t[N_train:]\n",
            "# scikit-learnのlinear_modelを読み込み\n",
            "from sklearn import linear_model# 訓練データ\n",
            "reg.score(x_train, t_train)# 訓練データ\n",
            "plt.plot(t_train, color='#2980b9')  # 実測値は青色\n",
            "plt.plot(reg.predict(x_train), color='#f39c12')  # 予測値はオレンジ\n",
            "plt.show()\n",
            "# 検証データ\n",
            "plt.plot(t_test, color='#2980b9')  # 実測値は青色\n",
            "plt.plot(reg.predict(x_test), color='#f39c12')  # 予測値はオレンジ\n",
            "plt.show()\n",
            "# 検証用の一部を見てみる\n",
            "plt.plot(t_test, color='#2980b9')  # 実測値は青色\n",
            "plt.plot(reg.predict(x_test), color='#f39c12')  # 予測値はオレンジ\n",
            "plt.xlim(200, 300)  # 特徴がわかりやすい一部\n",
            "plt.show()\n",
            "# 前サンプルとの差分を取る\n",
            "t_diff = t[:-1] - t[1:]# binの数を増やし、kde(ガウシアンカーネル密度比推定)のプロットをオフ\n",
            "sns.distplot(t_diff, bins=3000, kde=False)\n",
            "plt.xlim(-0.00075, 0.00075)\n",
            "plt.ylim(0, 750)\n",
            "plt.show()\n",
            "import chainer\n",
            "import chainer.links as L\n",
            "import chainer.functions as F\n",
            "from chainer import Chain, Variable, datasets, optimizers\n",
            "from chainer import report, training\n",
            "from chainer.training import extensions\n",
            "class LSTM(Chain):\n",
            "    # モデルの構造を明記\n",
            "    def __init__(self, n_units, n_output):\n",
            "        super().__init__()\n",
            "        with self.init_scope():\n",
            "            self.l1 = L.LSTM(None, n_units) # LSTMの層を追加\n",
            "            self.l2 = L.Linear(None, n_output)class LSTMUpdater(training.StandardUpdater):\n",
            "    def __init__(self, data_iter, optimizer, device=None):\n",
            "        super(LSTMUpdater, self).__init__(data_iter, optimizer, device=None)\n",
            "        self.device = device# chainer用のデータセットでメモリに乗る程度であれば、list(zip(...))を推奨\n",
            "# ↑ PFNの開発者推奨の方法\n",
            "train = list(zip(x_train, t_train))\n",
            "test  = list(zip(x_test,  t_test))\n",
            "# 再現性確保\n",
            "np.random.seed(1)\n",
            "# モデルの宣言\n",
            "model = LSTM(30, 1)trainer.run()\n",
            "class LSTM(Chain):\n",
            "    def __init__(self, n_units, n_output):\n",
            "        super().__init__()\n",
            "        with self.init_scope():\n",
            "            self.l1 = L.LSTM(None, n_units)\n",
            "            self.l2 = L.Linear(None, n_output)# 予測値の計算\n",
            "model.reset_state()\n",
            "y_train = model.predict(Variable(x_train)).data# 予測値の計算\n",
            "model.reset_state()\n",
            "y_test = model.predict(Variable(x_test)).data# 検証用の一部を見てみる\n",
            "plt.plot(t_test, color='#2980b9')  # 実測値は青色\n",
            "plt.plot(y_test, color='#f39c12')  # 予測値はオレンジ\n",
            "plt.xlim(200, 300)  # 特徴がわかりやすい一部\n",
            "plt.show()\n",
            "import matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "from matplotlib_venn import venn3import matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "from sklearn.cluster import AgglomerativeClustering\n",
            "from scipy.spatial import ConvexHullimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "from matplotlib_venn import venn3import matplotlib.pyplot as plt\n",
            "import matplotlib.lines as mlines\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import matplotlib.lines as mlines\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "from scipy.spatial import ConvexHullimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "from scipy.spatial import ConvexHullimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import matplotlib.patches as mpatches\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "import randomimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import matplotlib.patches as patches\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import matplotlib.patches as mpatches\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "from dateutil.parser import parse\n",
            "from scipy.stats import semimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "from dateutil.parser import parseimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "from math import piimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport calmap\n",
            "import matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "from dateutil.parser import parseimport joypy\n",
            "import matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snsimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "from pywaffle import Waffleimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "import squarifyimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "import scipy.cluster.hierarchy as shcimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "from matplotlib_venn import venn3import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "from matplotlib_venn import venn3, venn3_circlesimport matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "from pandas.plotting import parallel_coordinatesmatplotlib==2.0.0\n",
            "numpy==1.14.2\n",
            "tensorflow==1.7.0\n",
            "import tensorflow as tfimport tensorflow as tfimport tensorflow as tf{プレースホルダー1: 値, プレースホルダー2: 値, ...}\n",
            "{'v1': 3, 'v2': 5}\n",
            "{v1: 3, v2: 5}\n",
            "print(session.run(add, {v1: [1, 2], v2: [3, 4]}))  # => [4, 6]\n",
            "print(session.run(add, {v1: [1, 2], v2: [3, 4, 5]}))  # => InvalidArgumentError\n",
            "y = a x + b\n",
            "y_\\mathrm{model} = a_\\mathrm{model} x + b_\\mathrm{model}\n",
            "y_\\mathrm{answer} = a_\\mathrm{answer} x + b_\\mathrm{answer}\n",
            "y_\\mathrm{model} \\simeq y_\\mathrm{answer}\n",
            "\\mathrm{RMSE}(y_\\mathrm{model}, y_\\mathrm{answer}) = \\sqrt{\\frac{\\Sigma_i^n \\left(y_\\mathrm{model}^{(i)} - y_\\mathrm{answer}^{(i)} \\right)^2}{n}}\n",
            "# Step 1. Prepare data\n",
            "x_data = np.linspace(0., 1., 6)  # => => [0.  0.2 0.4 0.6 0.8 1. ]\n",
            "a_answer = 1.5\n",
            "b_answer = .1\n",
            "y_data = a_answer * x_data + b_answer  # => [0.1 0.4 0.7 1.  1.3 1.6]\n",
            "a_\\mathrm{answer} = 1.5\n",
            "\\\\\n",
            "b_\\mathrm{answer} = 0.1\n",
            "# Step 2. Define operation\n",
            "x = tf.placeholder(tf.float32)\n",
            "y_answer = tf.placeholder(tf.float32)y_model = a_model * x + b_model\n",
            "loss = tf.sqrt(tf.reduce_mean((y_model - y_answer)**2))\n",
            "train = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)import tensorflow as tftrain = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\n",
            "import tensorflow as tf\n",
            "import numpy as npfor i in range(20000):\n",
            "    session.run(train, {x: x_data, y_answer: y_data})\n",
            "Loss: 0.3893273174762726\n",
            "y_model: [8.9871704e-05 2.0010187e-01 4.0011388e-01 6.0012591e-01 8.0013788e-01 1.0001498e+00], y_answer: [0.1 0.4 0.7 1.  1.3 1.6]\n",
            "Loss: 3.246817868785001e-05\n",
            "y_model: [0.09997483 0.39997205 0.69996923 0.9999665  1.2999637  1.5999609 ], y_answer: [0.1 0.4 0.7 1.  1.3 1.6]\n",
            "\\boldsymbol{y} = W \\boldsymbol{x} + \\boldsymbol{b}\n",
            "\\begin{align}\n",
            "W_{\\mathrm{answer}} &= \\left(\\begin{array}{cc}\n",
            "1 & 2\n",
            "\\\\\n",
            "3 & 4\n",
            "\\end{array}\\right)\n",
            "\\\\\n",
            "\\boldsymbol{b}_{\\mathrm{answer}} &= \\left(\\begin{array}{c}\n",
            "-1\n",
            "\\\\\n",
            "5\n",
            "\\end{array}\\right)\n",
            "\\end{align}\n",
            "def init_weight_variable(shape):\n",
            "    \"\"\"Initialize variable in a suitable way for weights.\"\"\"\n",
            "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
            "    return tf.Variable(initial)w = init_weight_variable((2, 5))\n",
            "print(w)  # => <tf.Variable 'Variable:0' shape=(2, 5) dtype=float32_ref>\n",
            "import tensorflow as tf\n",
            "import numpy as np(2).__class__  # => <class 'int'>\n",
            "(2,).__class__  # => <class 'tuple'>\n",
            "Loss: 9.206093818647787e-05\n",
            "w, b: \n",
            "[[1.0000002 2.0000005]\n",
            " [2.9999332 3.9999335]], [-0.9999999  4.999933 ]\n",
            "y = \\sin(x)\n",
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import tensorflow as tfimport numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "raw = pd.read_csv('sample.csv', encoding='shift_jis')\n",
            "raw.head()\n",
            "raw.info()\n",
            "raw['Date'] = pd.to_datetime(raw['Date'])\n",
            "raw['Age_cat'] = pd.cut(raw['Age'], bins=np.arange(0, 100, 10), right=False)\n",
            "raw = raw.set_index('PassengerId')\n",
            "# raw = raw.set_index(raw['PassengerId'])#元の列名が残る方法\n",
            "raw.describe(include='all')\n",
            "# 数値データ用ヒストグラム\n",
            "def num_vis(data):\n",
            "    data.hist(figsize=(5, 4), color='darkblue', alpha=.7)\n",
            "    mean = data.mean()\n",
            "    median = data.median()\n",
            "    ymax = pd.cut(data, 10).value_counts().max()\n",
            "    plt.vlines(x=mean, ymin=0, ymax=ymax, colors='red', linestyles='--', lw=.7)# 平均値の直線追加\n",
            "    plt.annotate('Mean: {}'.format(round(mean, 2)),xy=(mean, ymax), color='red')\n",
            "    plt.vlines(x=median, ymin=0, ymax=ymax, colors='orange', linestyles='--', lw=.7)# 中央値の直線追加\n",
            "    plt.annotate('Median: {}'.format(round(median, 2)),xy=(mean, ymax*0.8), color='orange')\n",
            "    plt.title(data.name)\n",
            "    plt.show()#異常値として除去する対象を表示（確認）\n",
            "raw[raw['Age'] > 100]\n",
            "#対象のIndexを取得して、pd.Dataframe.dropで除去\n",
            "raw = raw.drop(raw[raw['Age'] > 100].index, axis=0)\n",
            "def replace_male(x):\n",
            "    if pd.isna(x):\n",
            "        pass\n",
            "    else:\n",
            "        return x.replace('man', 'male')# data = raw.dropna()\n",
            "# 欠損値除去後のデータは別名にして、元データは残しておくと後々便利\n",
            "raw['Age'] = raw['Age'].fillna(raw['Age'].mean())\n",
            "raw['Fare'] = raw['Fare'].fillna(raw['Fare'].mean())\n",
            "sns.pairplot(data=raw, vars=['Age', 'Fare'], size=3)\n",
            "sns.pairplot(data=raw, vars=['Age', 'Fare'], hue='Sex', kind='reg', diag_kind='kde', size=3,\n",
            "            plot_kws={'scatter_kws': {'alpha': 0.4},'line_kws': {'linestyle': '--', 'linewidth': .7}})\n",
            "plt.figure(figsize=(6, 5))\n",
            "sns.heatmap(raw[['Age', 'Fare']].corr(), fmt=\"1.2f\", annot=True, lw=0.7, cmap='YlGnBu')\n",
            "sns.boxplot(data=raw, x='Label', y='Age', palette=\"Set3\")\n",
            "plt.figure(figsize=(20, 7))\n",
            "plt.subplots_adjust(wspace=0.1, hspace=0.4)\n",
            "i = 1\n",
            "for x in ['Label', 'Sex', 'Embarked', 'Age_cat']:\n",
            "    for y in ['Age', 'Fare']:\n",
            "        plt.subplot(3, 3,i)\n",
            "        sns.boxplot(data=raw, x=x, y=y, palette=\"Set3\")\n",
            "        i += 1\n",
            "sns.heatmap(pd.crosstab(raw['Label'], raw['Sex']), fmt=\"1.1f\", annot=True, lw=0.7, cmap='Blues')\n",
            "plt.figure(figsize=(20, 7))\n",
            "plt.subplots_adjust(wspace=0.1, hspace=0.4)\n",
            "i = 1\n",
            "for label_1 in ['Label', 'Sex', 'Embarked']: \n",
            "    for label_2 in ['Label', 'Sex', 'Embarked']:\n",
            "        plt.subplot(3, 3,i)\n",
            "        sns.heatmap(pd.crosstab(raw[label_1], raw[label_2]), fmt=\"1.1f\", annot=True, lw=0.7, cmap='Blues')\n",
            "        i += 1\n",
            "import pandas_profiling as pdp\n",
            "report = pdp.ProfileReport(raw)\n",
            "report# notebook上にレポートを表示\n",
            "# report.to_html# html形式でレポートを出力\n",
            "import matplotlib.pyplot as plt #Visulization\n",
            "import seaborn as sns #Visulization\n",
            "%matplotlib inline\n",
            "plt.hist(train['quality'], bins=12)\n",
            "plt.title(\"quality_Histgram\")\n",
            "plt.xlabel('quality')\n",
            "plt.ylabel('count')\n",
            "plt.show()\n",
            "random.seed(0)\n",
            "plt.figure(figsize=(20, 6))\n",
            "plt.hist(np.random.randn(10**5)*10 + 50, bins=60,range=(20,80))\n",
            "plt.grid(True)\n",
            "plt.figure(figsize=(4,3),facecolor=\"white\")left = np.array([1, 2, 3, 4, 5])labels = 'Frogs', 'Hogs', 'Dogs', 'Logs'\n",
            "sizes = [15, 30, 45, 10]\n",
            "colors = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral']\n",
            "explode = (0, 0.1, 0, 0)  # 円から切り離して表示させることが可能N = 25#　シード値の固定\n",
            "random.seed(0)\n",
            "x = np.random.randn(50) # x軸のデータ\n",
            "y = np.sin(x) + np.random.randn(50) # y軸のデータ\n",
            "plt.figure(figsize=(16, 6)) # グラフの大きさ指定# シード値の指定\n",
            "np.random.seed(0)\n",
            "# データの範囲\n",
            "numpy_data_x = np.arange(1000)f(x) = x^2 + 2x +1\n",
            "# 関数の定義\n",
            "def sample_function(x):\n",
            "    return (x**2 + 2*x + 1)k = 13 # 表示する特徴量の数\n",
            "corrmat = train.corr()\n",
            "cols = corrmat.nlargest(k, 'quality').index # リストの最大値から順にk個の要素の添字(index)を取得\n",
            "# df_train[cols].head()\n",
            "cm = np.corrcoef(train[cols].values.T) # 相関関数行列を求める ※転置が必要\n",
            "sns.set(font_scale=1.25)\n",
            "f, ax = plt.subplots(figsize=(16, 12))\n",
            "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 8}, yticklabels=cols.values, xticklabels=cols.values)\n",
            "plt.show()\n",
            "sns.set()\n",
            "cols = ['quality', 'alcohol', 'citric.acid', 'free.sulfur.dioxide', 'sulphates', 'pH', 'total.sulfur.dioxide'] # プロットしたい特徴量\n",
            "sns.pairplot(train[cols], size = 2.0)\n",
            "plt.show()\n",
            "sns.jointplot(x=\"quality\", y=\"alcohol\", data=train, ratio=3, color=\"r\", size=6)\n",
            "plt.show()\n",
            "def plot_feature_importance(model):\n",
            "  n_features = X.shape[1]\n",
            "  plt.barh(range(n_features), model.feature_importances_, align='center')\n",
            "  plt.yticks(np.arange(n_features), X.columns)\n",
            "  plt.xlabel('Feature importance')\n",
            "  plt.ylabel('Feature')# ランダムフォレスト\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "forest = RandomForestClassifier(n_estimators=100, random_state=20181101) # n_estimatorsは構築する決定木の数\n",
            "forest.fit(X_train, y_train)import lightgbm as lgb\n",
            "# 可視化（modelはlightgbmで学習させたモデル）\n",
            "lgb.plot_importance(model, figsize=(12, 8))\n",
            "plt.show()\n",
            "\n",
            "crawling.py\n",
            "import requests\n",
            "r = requests.get('https://ja.wikipedia.org/wiki/Python')\n",
            "r.text\n",
            "\n",
            "\n",
            "scraping.py\n",
            "from bs4 import BeautifulSoup\n",
            "soup = BeautifulSoup(r.content, 'html.parser')\n",
            "soup.find(class_='mw-redirect').string\n",
            "\n",
            ">>> 'マルチパラダイム'\n",
            "\n",
            "get_github_data.py\n",
            "import requests\n",
            "import pandas as pdpip install janome\n",
            "\n",
            "janome_test.py\n",
            "# -*- coding: utf-8 -*-\n",
            "from janome.tokenizer import Tokenizer\n",
            "t = Tokenizer()これ\n",
            "は\n",
            "テスト\n",
            "データ\n",
            "です\n",
            "\n",
            "liner_reg_sample.py\n",
            "import numpy as np\n",
            "from sklearn import linear_model\n",
            "liner_reg_sample.py\n",
            "# テストデータ\n",
            "x_test = x_data[:71]\n",
            "y_test = y_data[:71]>>> score: 0.714080213722\n",
            "\n",
            "liner_reg_sample.py\n",
            "from sklearn.metrics import mean_squared_error\n",
            "from math import sqrt>>> corr: 0.895912443712\n",
            ">>> RMSE: 0.6605862235679646\n",
            "\n",
            "liner_reg_sample.py\n",
            "plt.scatter(x_test, y_test, color='blue')\n",
            "plt.plot(x_test, pred, color='red')\n",
            "plt.show()\n",
            "\n",
            "from tpot import TPOTClassifier\n",
            "from sklearn.datasets import load_digits\n",
            "from sklearn.model_selection import train_test_splitGeneration 1 - Current best internal CV score: 0.9651912264496657                                                                                                   \n",
            "Generation 2 - Current best internal CV score: 0.9822200910854291                                                                                                   \n",
            "Generation 3 - Current best internal CV score: 0.9822200910854291                                                                                                   \n",
            "Generation 4 - Current best internal CV score: 0.9822200910854291                                                                                                   \n",
            "Generation 5 - Current best internal CV score: 0.9822200910854291                                                                                                   import autokeras as ak\n",
            "from keras.datasets import mnist+----------------------------------------------+\n",
            "|               Training model 8               |\n",
            "+----------------------------------------------+>>> import featuretools as ft\n",
            ">>> es = ft.demo.load_mock_customer(return_entityset=True)\n",
            ">>> es\n",
            "Entityset: transactions\n",
            "  Entities:\n",
            "    transactions [Rows: 500, Columns: 5]\n",
            "    products [Rows: 5, Columns: 2]\n",
            "    sessions [Rows: 35, Columns: 4]\n",
            "    customers [Rows: 5, Columns: 3]\n",
            "  Relationships:\n",
            "    transactions.product_id -> products.product_id\n",
            "    transactions.session_id -> sessions.session_id\n",
            "    sessions.customer_id -> customers.customer_id\n",
            ">>> feature_matrix, features_defs = ft.dfs(entityset=es, target_entity=\"customers\")\n",
            ">>> feature_matrix.head(5)\n",
            "            zip_code  COUNT(sessions)                  ...                   MODE(sessions.MONTH(session_start)) MODE(sessions.WEEKDAY(session_start))\n",
            "customer_id                                            ...                                                                                            \n",
            "1              60091               10                  ...                                                     1                                     2\n",
            "2              02139                8                  ...                                                     1                                     2\n",
            "3              02139                5                  ...                                                     1                                     2\n",
            "4              60091                8                  ...                                                     1                                     2\n",
            "5              02139                4                  ...                                                     1                                     2\n",
            ">>> pprint(features_defs)\n",
            "[<Feature: zip_code>,\n",
            " <Feature: COUNT(sessions)>,\n",
            " <Feature: NUM_UNIQUE(sessions.device)>,\n",
            " <Feature: MODE(sessions.device)>,\n",
            " <Feature: SUM(transactions.amount)>,\n",
            " <Feature: STD(transactions.amount)>,\n",
            " <Feature: MAX(transactions.amount)>,\n",
            " <Feature: SKEW(transactions.amount)>,\n",
            " <Feature: MIN(transactions.amount)>,\n",
            " <Feature: MEAN(transactions.amount)>,\n",
            " <Feature: COUNT(transactions)>,\n",
            " ...\n",
            " <Feature: MODE(sessions.WEEKDAY(session_start))>]\n",
            "{\\bf a}\\cdot{\\bf b} = \\|{\\bf a}\\|\\|{\\bf b}\\|\\ \\cos\\theta\n",
            "\n",
            "{\\bf a} \\cdot {\\bf b} = a_1b_1+\\cdots+a_nb_n = \\sum_{i=1}^n a_ib_i\n",
            "\\|{\\bf b}-{\\bf a}\\|^2 = \\|{\\bf a}\\|^2 + \\|{\\bf b}\\|^2 - 2\\|{\\bf a}\\|\\|{\\bf b}\\|\\ \\cos\\theta　　　　\\cdots (*)\n",
            "\\|{\\bf a}\\|^2 = \\left(\\sqrt{a_1\\cdot a_1 + \\cdots + a_n\\cdot a_n}\\right)^2 =a_1\\cdot a_1 + \\cdots + a_n\\cdot a_n\\\\\n",
            "=\\sum_{i=1}^n x_i^2 = {\\bf a} \\cdot {\\bf a}　\n",
            "\\|{\\bf b}-{\\bf a}\\|^2 = ({\\bf b}-{\\bf a})\\cdot({\\bf b}-{\\bf a}) = {\\bf a}\\cdot{\\bf a} +  {\\bf b}\\cdot{\\bf b} - 2 {\\bf a}\\cdot{\\bf b} \\\\\n",
            "= \\|{\\bf a}\\|^2 + \\|{\\bf b}\\|^2 - 2 {\\bf a}\\cdot{\\bf b} 　　　　　　　　\n",
            "\\|{\\bf b}-{\\bf a}\\|^2 = \\|{\\bf a}\\|^2 + \\|{\\bf b}\\|^2 - 2\\|{\\bf a}\\|\\|{\\bf b}\\|\\ cos\\theta\\\\\n",
            "\\Rightarrow \\|{\\bf a}\\|^2 + \\|{\\bf b}\\|^2 - 2 {\\bf a}\\cdot{\\bf b} = \\|{\\bf a}\\|^2 + \\|{\\bf b}\\|^2 - 2\\|{\\bf a}\\|\\|{\\bf b}\\|\\ cos\\theta\\\\\n",
            " - 2 {\\bf a}\\cdot{\\bf b} = - 2\\|{\\bf a}\\|\\|{\\bf b}\\|\\ cos\\theta \\\\\n",
            "\\Rightarrow {\\bf a}\\cdot{\\bf b} = \\|{\\bf a}\\|\\|{\\bf b}\\|\\ cos\\theta\n",
            "\\|{\\bf a}\\|\\|{\\bf b}\\|\\ \\cos \\theta\n",
            "\\cos \\theta = \\frac{\\|c\\|}{\\|r\\|}\n",
            "\\|c\\| = \\|r\\|\\cos \\theta\n",
            "\\|{\\bf a}\\|\\|{\\bf b}\\|\\ \\cos \\theta\n",
            "{\\bf a}\\cdot{\\bf b} = \\|{\\bf a}\\|\\|{\\bf b}\\|\\ \\cos\\theta\n",
            "\\cos \\theta = \\frac{\\|c\\|}{\\|r\\|} = \\|c\\|\n",
            "{\\bf x}\\cdot{\\bf r} = \\|{\\bf x}\\|\\|{\\bf r}\\|\\ \\cos\\theta　=　1 \\cdot 1 \\cdot \\cos\\theta　=　\\cos\\theta \n",
            "class FitbitClient(object):\n",
            "    def __init__(self):\n",
            "        self.client_id = os.getenv('FITBIT_CLIENT_ID')\n",
            "        self.client_secret = os.getenv('FITBIT_CLIENT_SECRET')\n",
            "        token_file_name = os.getenv('TOKEN_FILE_NAME')\n",
            "        tokens = open(token_file_name).read()\n",
            "        token_dict = literal_eval(tokens)\n",
            "        self.access_token = token_dict['access_token']\n",
            "        self.refresh_token = token_dict['refresh_token']class PhysicalHealthChecker(object):\n",
            "    def __init__(self, provider, event_emitter):\n",
            "        self.event_emitter = event_emitter\n",
            "        self.heart_beat_provider = provider\n",
            "        seconds, beats = self.heart_beat_provider.request_heart_beats('today')\n",
            "        expected_beat = np.polyval(np.polyfit(seconds, beats, 1), seconds[-1] + 60)\n",
            "        if self.is_alive(expected_beat):\n",
            "            print 'still alive.'\n",
            "        else:\n",
            "            timestamp_file_name = 'death_time.txt'\n",
            "            death_time = open(timestamp_file_name, 'r').read()\n",
            "            if not death_time:\n",
            "                print 'dead.'\n",
            "                self.event_emitter.emit('death')\n",
            "                with open(timestamp_file_name, 'w') as file:\n",
            "                    file.write(datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\"))class IftttEventEmitter(object):\n",
            "    def __init__(self, secret_key):\n",
            "        self.secret_key = secret_keyimport pandas as pd # ライブラリ pandas をインポートし、以下 pd と呼ぶことにする。\n",
            "# https://www.kaggle.com/abcsds/pokemon から取得した Pokemon.csv を読み込む。\n",
            "df = pd.read_csv(\"Pokemon.csv\") # df とは、 pandas の DataFrame 形式のデータを入れる変数として命名\n",
            "df.head() # 先頭５行を表示\n",
            "df.head(50).style.bar(subset=['Total', 'HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed'])\n",
            "df.describe() # 平均値、標準偏差、最小値、25%四分値、中央値、75%四分値、最大値をまとめて表示\n",
            "# Jupyter 上で絵を表示するためのマジックコマンド\n",
            "%matplotlib inline \n",
            "import matplotlib.pyplot as plt # matplotlib.pyplot をインポートし、以下 plt と呼ぶ。\n",
            "# 散布図行列\n",
            "from pandas.tools import plotting \n",
            "# from pandas import plotting # 新しいバージョンではこちらを\n",
            "plotting.scatter_matrix(df.iloc[:, 4:11], figsize=(8, 8)) \n",
            "plt.show()\n",
            "type1s = list(set(list(df['Type 1'])))\n",
            "print(len(type1s), type1s)\n",
            "18 ['Ground', 'Bug', 'Grass', 'Fire', 'Normal', 'Fighting', 'Psychic', 'Electric', 'Water', 'Ice', 'Poison', 'Dark', 'Fairy', 'Rock', 'Steel', 'Flying', 'Ghost', 'Dragon']\n",
            "cmap = plt.get_cmap('coolwarm')\n",
            "colors = [cmap((type1s.index(c) + 1) / (len(type1s) + 2)) for c in df['Type 1'].tolist()]\n",
            "plotting.scatter_matrix(df.iloc[:, 4:11], figsize=(8, 8), color=colors, alpha=0.5) \n",
            "plt.show()\n",
            "from matplotlib.colors import LinearSegmentedColormap\n",
            "dic = {'red':   ((0, 0, 0), (0.5, 1, 1), (1, 1, 1)), \n",
            "       'green': ((0, 0, 0), (0.5, 1, 1), (1, 0, 0)), \n",
            "       'blue':  ((0, 1, 1), (0.5, 0, 0), (1, 0, 0))}cmap = tricolor_cmap\n",
            "colors = [cmap((type1s.index(c) + 1) / (len(type1s) + 2)) for c in df['Type 1'].tolist()]\n",
            "plotting.scatter_matrix(df.iloc[:, 4:11], figsize=(8, 8), color=colors) \n",
            "plt.show()\n",
            "import numpy as np\n",
            "pd.DataFrame(np.corrcoef(df.iloc[:, 4:11].T.values.tolist()), \n",
            "             columns=df.iloc[:, 4:11].columns, index=df.iloc[:, 4:11].columns)\n",
            "corrcoef = np.corrcoef(df.iloc[:, 4:11].T.values.tolist())\n",
            "plt.imshow(corrcoef, interpolation='nearest', cmap=plt.cm.coolwarm)\n",
            "plt.colorbar(label='correlation coefficient')\n",
            "tick_marks = np.arange(len(corrcoef))\n",
            "plt.xticks(tick_marks, df.iloc[:, 4:11].columns, rotation=90)\n",
            "plt.yticks(tick_marks, df.iloc[:, 4:11].columns)\n",
            "plt.tight_layout()\n",
            "corrcoef = np.corrcoef(df.iloc[:, 4:11].T.values.tolist())\n",
            "plt.imshow(corrcoef, interpolation='nearest', cmap=tricolor_cmap)\n",
            "plt.colorbar(label='correlation coefficient')\n",
            "tick_marks = np.arange(len(corrcoef))\n",
            "plt.xticks(tick_marks, df.iloc[:, 4:11].columns, rotation=90)\n",
            "plt.yticks(tick_marks, df.iloc[:, 4:11].columns)\n",
            "plt.tight_layout()\n",
            "# バイオリンプロット\n",
            "fig = plt.figure()\n",
            "ax = fig.add_subplot(111)\n",
            "ax.violinplot(df.iloc[:, 4:11].values.T.tolist())\n",
            "ax.set_xticks([1, 2, 3, 4, 5, 6, 7]) #データ範囲のどこに目盛りが入るかを指定する\n",
            "ax.set_xticklabels(df.columns[4:11], rotation=90)\n",
            "plt.grid()\n",
            "plt.show()\n",
            "for index, type1 in enumerate(type1s):\n",
            "    df2 = df[df['Type 1'] == type1]\n",
            "    fig = plt.figure(figsize=(8, 4))\n",
            "    ax = fig.add_subplot(1, 1, 1)\n",
            "    plt.title(type1)\n",
            "    ax.set_ylim([0, 260])\n",
            "    ax.violinplot(df2.iloc[:, 5:11].values.T.tolist())\n",
            "    ax.set_xticks([1, 2, 3, 4, 5, 6]) #データ範囲のどこに目盛りが入るかを指定する\n",
            "    ax.set_xticklabels(df2.columns[5:11], rotation=90)\n",
            "    plt.grid()\n",
            "    plt.show()\n",
            "from sklearn.decomposition import PCA #主成分分析器\n",
            "#主成分分析の実行\n",
            "pca = PCA()\n",
            "pca.fit(df.iloc[:, 5:11])\n",
            "# データを主成分空間に写像 = 次元圧縮\n",
            "feature = pca.transform(df.iloc[:, 5:11])\n",
            "# 第一主成分と第二主成分でプロットする\n",
            "plt.figure(figsize=(8, 8))\n",
            "plt.scatter(feature[:, 0], feature[:, 1], alpha=0.8)\n",
            "plt.xlabel('PC1')\n",
            "plt.ylabel('PC2')\n",
            "plt.grid()\n",
            "plt.show()\n",
            "# 累積寄与率を図示する\n",
            "import matplotlib.ticker as ticker\n",
            "import numpy as np\n",
            "plt.gca().get_xaxis().set_major_locator(ticker.MaxNLocator(integer=True))\n",
            "plt.plot([0] + list( np.cumsum(pca.explained_variance_ratio_)), \"-o\")\n",
            "plt.xlabel(\"Number of principal components\")\n",
            "plt.ylabel(\"Cumulative contribution ratio\")\n",
            "plt.grid()\n",
            "plt.show()\n",
            "#主成分分析の実行\n",
            "pca = PCA()\n",
            "pca.fit(df.iloc[:, 5:11])\n",
            "# データを主成分空間に写像 = 次元圧縮\n",
            "feature = pca.transform(df.iloc[:, 5:11])\n",
            "# 第一主成分と第二主成分でプロットする\n",
            "plt.figure(figsize=(8, 8))\n",
            "for type1 in type1s:\n",
            "    plt.scatter(feature[df['Type 1'] == type1, 0], feature[df['Type 1'] == type1, 1], alpha=0.8, label=type1)\n",
            "plt.xlabel('PC1')\n",
            "plt.ylabel('PC2')\n",
            "plt.legend(loc = 'upper right',\n",
            "          bbox_to_anchor = (0.7, 0.7, 0.5, 0.1),\n",
            "          borderaxespad = 0.0)\n",
            "plt.grid()\n",
            "plt.show()\n",
            "#主成分分析の実行\n",
            "pca = PCA()\n",
            "pca.fit(df.iloc[:, 5:11])\n",
            "# データを主成分空間に写像 = 次元圧縮\n",
            "feature = pca.transform(df.iloc[:, 5:11])\n",
            "# 第一主成分と第二主成分でプロットする\n",
            "plt.figure(figsize=(8, 8))\n",
            "for generation in range(0, 7):\n",
            "    plt.scatter(feature[df['Generation'] == generation, 0], feature[df['Generation'] == generation, 1], alpha=0.8, label=generation)\n",
            "plt.xlabel('PC1')\n",
            "plt.ylabel('PC2')\n",
            "plt.legend(loc = 'upper right',\n",
            "          bbox_to_anchor = (0.7, 0.7, 0.5, 0.1),\n",
            "          borderaxespad = 0.0)\n",
            "plt.grid()\n",
            "plt.show()\n",
            "#主成分分析の実行\n",
            "pca = PCA()\n",
            "pca.fit(df.iloc[:, 5:11])\n",
            "# データを主成分空間に写像 = 次元圧縮\n",
            "feature = pca.transform(df.iloc[:, 5:11])\n",
            "# 第一主成分と第二主成分でプロットする\n",
            "plt.figure(figsize=(8, 8))\n",
            "for binary in [True, False]:\n",
            "    plt.scatter(feature[df['Legendary'] == binary, 0], feature[df['Legendary'] == binary, 1], alpha=0.8, label=binary)\n",
            "plt.xlabel('PC1')\n",
            "plt.ylabel('PC2')\n",
            "plt.legend(loc = 'upper right',\n",
            "          bbox_to_anchor = (0.7, 0.7, 0.5, 0.1),\n",
            "          borderaxespad = 0.0)\n",
            "plt.grid()\n",
            "plt.show()\n",
            "# 第一主成分と第二主成分における観測変数の寄与度をプロットする\n",
            "plt.figure(figsize=(8, 8))\n",
            "for x, y, name in zip(pca.components_[0], pca.components_[1], df.columns[5:11]):\n",
            "    plt.text(x, y, name)\n",
            "plt.scatter(pca.components_[0], pca.components_[1])\n",
            "plt.grid()\n",
            "plt.xlabel(\"PC1\")\n",
            "plt.ylabel(\"PC2\")\n",
            "plt.show()\n",
            "from sklearn.decomposition import FactorAnalysis\n",
            "fa = FactorAnalysis(n_components=2, max_iter=500)\n",
            "factors = fa.fit_transform(df.iloc[:, 5:11])\n",
            "plt.figure(figsize=(8, 8))\n",
            "for binary in [True, False]:\n",
            "    plt.scatter(factors[df['Legendary'] == binary, 0], factors[df['Legendary'] == binary, 1], alpha=0.8, label=binary)\n",
            "plt.xlabel(\"Factor 1\")\n",
            "plt.ylabel(\"Factor 2\")\n",
            "plt.legend(loc = 'upper right',\n",
            "          bbox_to_anchor = (0.7, 0.7, 0.5, 0.1),\n",
            "          borderaxespad = 0.0)\n",
            "plt.grid()\n",
            "plt.show()\n",
            "# 第一主成分と第二主成分における観測変数の寄与度をプロットする\n",
            "plt.figure(figsize=(8, 8))\n",
            "for x, y, name in zip(fa.components_[0], fa.components_[1], df.columns[5:11]):\n",
            "    plt.text(x, y, name)\n",
            "plt.scatter(fa.components_[0], fa.components_[1])\n",
            "plt.grid()\n",
            "plt.xlabel(\"Factor 1\")\n",
            "plt.ylabel(\"Factor 2\")\n",
            "plt.show()\n",
            "# 行列の正規化\n",
            "dfs = df.iloc[:, 5:11].apply(lambda x: (x-x.mean())/x.std(), axis=0)\n",
            "# metric は色々あるので、ケースバイケースでどれかひとつ好きなものを選ぶ。\n",
            "# method も色々あるので、ケースバイケースでどれかひとつ好きなものを選ぶ。\n",
            "from scipy.cluster.hierarchy import linkage, dendrogram\n",
            "result1 = linkage(dfs, \n",
            "                  #metric = 'braycurtis', \n",
            "                  #metric = 'canberra', \n",
            "                  #metric = 'chebyshev', \n",
            "                  #metric = 'cityblock', \n",
            "                  #metric = 'correlation',\n",
            "                  #metric = 'cosine', \n",
            "                  metric = 'euclidean', \n",
            "                  #metric = 'hamming', \n",
            "                  #metric = 'jaccard', \n",
            "                  #method= 'single')\n",
            "                  method = 'average')\n",
            "                  #method= 'complete')\n",
            "                  #method='weighted')\n",
            "plt.figure(figsize=(8, 128))\n",
            "dendrogram(result1, orientation='right', labels=list(df['Name']), color_threshold=2)\n",
            "plt.title(\"Dedrogram\")\n",
            "plt.xlabel(\"Threshold\")\n",
            "plt.grid()\n",
            "plt.show()\n",
            "# 指定したクラスタ数でクラスタを得る関数を作る。\n",
            "def get_cluster_by_number(result, number):\n",
            "    output_clusters = []\n",
            "    x_result, y_result = result.shape\n",
            "    n_clusters = x_result + 1\n",
            "    cluster_id = x_result + 1\n",
            "    father_of = {}\n",
            "    x1 = []\n",
            "    y1 = []\n",
            "    x2 = []\n",
            "    y2 = []\n",
            "    for i in range(len(result) - 1):\n",
            "        n1 = int(result[i][0])\n",
            "        n2 = int(result[i][1])\n",
            "        val = result[i][2]\n",
            "        n_clusters -= 1\n",
            "        if n_clusters >= number:\n",
            "            father_of[n1] = cluster_id\n",
            "            father_of[n2] = cluster_idclusterIDs = get_cluster_by_number(result1, 50)\n",
            "print(clusterIDs)\n",
            "[0, 0, 1, 2, 0, 3, 4, 4, 2, 0, 0, 1, 2, 0, 0, 3, 0, 0, 3, 5, 0, 0, 3, 4, 0, 3, 0, 3, 0, 3, 0, 3, 0, 0, 0, 0, 3, 0, 0, 3, 0, 1, 0, 3, 6, 7, 0, 3, 0, 0, 1, 0, 0, 0, 3, 8, 9, 0, 3, 0, 3, 0, 3, 0, 4, 0, 0, 3, 10, 10, 4, 11, 0, 0, 12, 0, 0, 12, 13, 1, 14, 14, 15, 3, 3, 0, 1, 16, 14, 17, 0, 0, 3, 0, 1, 0, 12, 14, 18, 10, 10, 4, 11, 19, 0, 1, 14, 12, 10, 9, 14, 12, 14, 0, 20, 20, 1, 14, 17, 14, 15, 21, 17, 22, 22, 14, 17, 0, 3, 10, 3, 13, 3, 4, 3, 3, 12, 23, 3, 8, 22, 24, 1, 0, 0, 1, 4, 20, 0, 14, 17, 14, 12, 3, 5, 25, 1, 4, 4, 0, 3, 24, 4, 26, 26, 1, 0, 0, 1, 0, 3, 4, 0, 0, 3, 0, 3, 0, 1, 0, 13, 0, 0, 3, 0, 1, 8, 0, 6, 14, 0, 0, 3, 0, 0, 1, 27, 1, 0, 1, 14, 1, 0, 0, 3, 0, 0, 1, 3, 0, 12, 4, 28, 3, 1, 3, 0, 29, 3, 14, 15, 1, 30, 31, 31, 0, 12, 3, 12, 24, 32, 22, 23, 9, 0, 12, 0, 33, 0, 12, 0, 0, 12, 0, 13, 30, 0, 3, 4, 1, 0, 15, 1, 3, 0, 0, 20, 10, 3, 3, 22, 21, 4, 22, 1, 0, 0, 24, 24, 28, 2, 1, 0, 3, 3, 4, 0, 0, 4, 4, 0, 0, 12, 24, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 9, 0, 17, 0, 0, 4, 2, 0, 0, 0, 12, 0, 3, 34, 14, 9, 35, 0, 0, 3, 0, 25, 0, 36, 0, 0, 0, 33, 0, 33, 14, 15, 31, 31, 0, 3, 3, 0, 3, 4, 3, 3, 3, 3, 3, 0, 1, 37, 4, 4, 7, 7, 0, 12, 27, 15, 0, 1, 0, 35, 0, 3, 0, 12, 0, 1, 1, 3, 12, 3, 3, 0, 3, 0, 12, 0, 1, 0, 1, 0, 12, 8, 1, 3, 1, 0, 12, 20, 36, 33, 1, 3, 12, 4, 0, 0, 3, 4, 0, 1, 1, 14, 17, 17, 15, 10, 0, 0, 4, 24, 0, 0, 24, 24, 31, 38, 39, 1, 2, 4, 2, 2, 40, 24, 41, 42, 26, 1, 43, 43, 39, 44, 0, 0, 12, 0, 3, 3, 0, 0, 1, 0, 0, 3, 0, 3, 0, 3, 0, 0, 4, 0, 4, 37, 45, 36, 39, 0, 0, 0, 0, 0, 0, 1, 3, 0, 3, 0, 3, 0, 1, 3, 0, 7, 0, 3, 5, 4, 12, 0, 3, 0, 0, 3, 36, 33, 14, 13, 6, 3, 33, 0, 3, 22, 24, 25, 0, 4, 4, 0, 15, 14, 30, 0, 3, 12, 0, 3, 13, 0, 3, 27, 5, 17, 1, 15, 15, 4, 4, 1, 3, 30, 17, 30, 22, 4, 20, 23, 39, 33, 3, 3, 17, 17, 17, 17, 17, 28, 1, 4, 2, 2, 2, 24, 42, 42, 28, 3, 1, 4, 1, 4, 42, 1, 0, 3, 3, 0, 3, 12, 0, 0, 12, 0, 3, 0, 0, 22, 0, 3, 0, 3, 0, 3, 0, 3, 0, 1, 0, 0, 3, 0, 3, 14, 14, 15, 0, 3, 0, 22, 1, 28, 0, 12, 15, 0, 0, 3, 25, 22, 0, 0, 3, 0, 14, 3, 0, 3, 0, 3, 3, 0, 0, 22, 0, 22, 46, 12, 0, 15, 0, 33, 3, 14, 33, 14, 15, 12, 4, 0, 3, 0, 4, 0, 3, 0, 0, 1, 47, 47, 1, 0, 3, 0, 0, 1, 0, 3, 3, 0, 15, 0, 1, 0, 1, 7, 0, 3, 36, 33, 0, 0, 30, 0, 0, 12, 0, 1, 0, 0, 17, 0, 12, 12, 0, 12, 13, 14, 9, 1, 0, 4, 12, 0, 12, 0, 12, 12, 0, 22, 0, 22, 12, 30, 0, 0, 4, 0, 4, 30, 22, 1, 4, 3, 4, 4, 2, 24, 4, 4, 42, 24, 42, 4, 4, 2, 22, 4, 0, 0, 15, 0, 3, 4, 0, 3, 3, 0, 3, 0, 0, 3, 0, 0, 3, 0, 3, 0, 0, 2, 0, 1, 0, 12, 3, 0, 3, 3, 14, 15, 48, 39, 0, 1, 0, 3, 0, 3, 0, 15, 0, 1, 0, 1, 0, 4, 0, 15, 0, 1, 1, 3, 3, 39, 0, 1, 2, 3, 0, 12, 0, 0, 0, 0, 30, 30, 15, 15, 14, 31, 0, 3, 42, 42, 1, 39, 49, 2, 40, 2]\n",
            "plt.hist(clusterIDs, bins=50)\n",
            "plt.grid()\n",
            "plt.show()\n",
            "for i in range(max(clusterIDs) + 1):\n",
            "    cluster = []\n",
            "    for j, k in enumerate(clusterIDs):\n",
            "        if i == k:\n",
            "            cluster.append(j)\n",
            "    fig = plt.figure()\n",
            "    print(\"Cluster {}: {} samples\".format(i + 1, len(cluster)))\n",
            "    for j in cluster:\n",
            "        labels = list(df.columns[5:11])\n",
            "        values = list(df.iloc[j, 5:11])\n",
            "        angles = np.linspace(0, 2 * np.pi, len(labels) + 1, endpoint=True)\n",
            "        values = np.concatenate((values, [values[0]]))  # 閉じた多角形にする\n",
            "        ax = fig.add_subplot(111, polar=True)\n",
            "        ax.plot(angles, values, 'o-', label=df.iloc[j, :]['Name'] + \" (\" + df.iloc[j, :]['Type 1'] + \")\")  # 外枠\n",
            "        #ax.fill(angles, values, alpha=0.25)  # 塗りつぶし\n",
            "        ax.set_thetagrids(angles[:-1] * 180 / np.pi, labels)  # 軸ラベル\n",
            "        ax.set_rlim(0 ,250)\n",
            "    plt.legend( loc = 'center right',\n",
            "          bbox_to_anchor = (1.5, 0.5, 0.5, 0.1),\n",
            "          borderaxespad = 0.0)\n",
            "    plt.show()\n",
            "X = df.iloc[:, 5:11]\n",
            "y = df['Total']\n",
            "from sklearn import linear_model\n",
            "regr = linear_model.LinearRegression()\n",
            "regr.fit(X, y) # 予測モデルを作成回帰係数=  [1. 1. 1. 1. 1. 1.]\n",
            "切片=  5.684341886080802e-14\n",
            "決定係数=  1.0\n",
            "df.head()\n",
            "df.columns[[5, 6, 7, 10]]\n",
            "Index(['HP', 'Attack', 'Defense', 'Speed'], dtype='object')\n",
            "X = df.iloc[:, [5, 6, 7, 10]]\n",
            "y = df['Sp. Atk']\n",
            "from sklearn import linear_model\n",
            "regr = linear_model.LinearRegression()\n",
            "regr.fit(X, y) # 予測モデルを作成回帰係数=  [0.28468407 0.10004721 0.12673883 0.4439368 ]\n",
            "切片=  5.529675896993538\n",
            "決定係数=  0.3333279772212076\n",
            "#from sklearn.cross_validation import train_test_split # 訓練データとテストデータに分割\n",
            "from sklearn.model_selection import train_test_split\n",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4) # 訓練データ・テストデータへのランダムな分割\n",
            "from sklearn import linear_model\n",
            "regr = linear_model.LinearRegression()\n",
            "regr.fit(X_train, y_train) # 予測モデルを作成回帰係数=  [0.28050284 0.13629344 0.09994709 0.48036096]\n",
            "切片=  4.510621382128264\n",
            "決定係数(train)=  0.3494685274982846\n",
            "決定係数(test)=  0.2558419802439882\n",
            "Xs = X.apply(lambda x: (x-x.mean())/x.std(), axis=0)\n",
            "ys = list(pd.DataFrame(y).apply(lambda x: (x-x.mean())/x.std()).values.reshape(len(y),))\n",
            "from sklearn import linear_model\n",
            "regr = linear_model.LinearRegression()標準回帰係数=  [0.22215171 0.0992372  0.12077883 0.39425762]\n",
            "切片=  3.2105349393792207e-16\n",
            "決定係数=  0.33332797722120766\n",
            "pd.DataFrame(regr.coef_, index=list(df.columns[[5, 6, 7, 10]])).sort_values(0, ascending=False).style.bar(subset=[0])\n",
            "df2 = pd.get_dummies(df.iloc[:, 2:], dummy_na=True)\n",
            "df2.head()\n",
            "X = df2\n",
            "del X['Total']\n",
            "del X['Sp. Atk']\n",
            "del X['Sp. Def']\n",
            "del X['Legendary']\n",
            "X.head()\n",
            "#from sklearn.cross_validation import train_test_split # 訓練データとテストデータに分割\n",
            "from sklearn.model_selection import train_test_split\n",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4) # 訓練データ・テストデータへのランダムな分割\n",
            "from sklearn import linear_model\n",
            "regr = linear_model.LinearRegression()\n",
            "regr.fit(X_train, y_train) # 予測モデルを作成回帰係数=  [ 2.28431738e-01  2.97742060e-01  8.02614939e-02  2.20997067e-01\n",
            " -1.55704518e-01 -1.47132780e+01 -5.84090849e+00  2.19195137e+00\n",
            "  1.77819922e+01  1.91024750e+01 -2.45614311e+01  1.85155570e+01\n",
            "  7.95924485e+00  9.10518038e+00  3.54309623e+00 -2.24760486e+01\n",
            "  8.41016188e+00 -1.78568717e+01 -2.51439443e+00  2.62609157e+01\n",
            " -1.41838440e+01 -1.43994552e+01  3.67565689e+00 -1.61076024e-15\n",
            " -5.36512027e+00 -1.06042187e+01  1.22074186e+01  4.27173718e+00\n",
            "  7.26562566e+00 -2.00570257e+01  2.25619608e+01 -4.07020702e+00\n",
            "  3.76104637e+00 -7.93072478e+00 -1.15056412e+01  1.91256670e+01\n",
            " -3.63271870e+00  6.58290356e+00  1.54416323e+01 -1.33194463e+01\n",
            " -1.09096992e+01  3.44875964e+00 -7.27194918e+00]\n",
            "切片=  17.69492260426813\n",
            "決定係数(train)=  0.5716379265230409\n",
            "決定係数(test)=  0.4658252989421211\n",
            "from sklearn.ensemble import RandomForestRegressor\n",
            "regr = RandomForestRegressor(max_depth=2, random_state=0, n_estimators=100)\n",
            "regr.fit(X_train, y_train)\n",
            "print(\"決定係数(train)= \", regr.score(X_train, y_train))\n",
            "print(\"決定係数(test)= \", regr.score(X_test, y_test))\n",
            "決定係数(train)=  0.37839205866915593\n",
            "決定係数(test)=  0.3265735023596227\n",
            "regr.feature_importances_\n",
            "array([0.3960066 , 0.04625405, 0.05236526, 0.42465029, 0.        ,\n",
            "       0.00299188, 0.        , 0.00357449, 0.00097514, 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.004711  , 0.        ,\n",
            "       0.        , 0.        , 0.05131166, 0.        , 0.00934274,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.00412505, 0.        , 0.00098141, 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.00122454, 0.        , 0.00148591, 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        ])\n",
            "pd.DataFrame(regr.feature_importances_, index=list(X.columns)).sort_values(0, ascending=False).head(10).style.bar(subset=[0])\n",
            "df2 = pd.get_dummies(df.iloc[:, 2:], dummy_na=True)\n",
            "X = df2\n",
            "del X['Total']\n",
            "del X['Sp. Atk']\n",
            "del X['Sp. Def']\n",
            "del X['Legendary']\n",
            "X.head()\n",
            "y = df['Legendary']\n",
            "#from sklearn.cross_validation import train_test_split # 訓練データとテストデータに分割\n",
            "from sklearn.model_selection import train_test_split\n",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4) # 訓練データ・テストデータへのランダムな分割\n",
            "from sklearn.linear_model import LogisticRegression # ロジスティック回帰\n",
            "clf = LogisticRegression(solver='lbfgs', max_iter=10000) #モデルの生成\n",
            "clf.fit(X_train, y_train) #学習\n",
            "print(\"正解率(train): \", clf.score(X_train,y_train))\n",
            "print(\"正解率(test): \", clf.score(X_test,y_test))\n",
            "正解率(train):  0.9479166666666666\n",
            "正解率(test):  0.93125\n",
            "from sklearn.metrics import confusion_matrix # 混合行列\n",
            "# 予測結果と、正解（本当の答え）がどのくらい合っていたかを表す混合行列\n",
            "pd.DataFrame(confusion_matrix(clf.predict(X_test), y_test), index=['predicted 0', 'predicted 1'], columns=['real 0', 'real 1'])\n",
            "from sklearn.neural_network import MLPClassifier\n",
            "clf = MLPClassifier(max_iter=10000)\n",
            "clf.fit(X_train, y_train) #学習\n",
            "print(\"正解率(train): \", clf.score(X_train,y_train))\n",
            "print(\"正解率(test): \", clf.score(X_test,y_test))\n",
            "正解率(train):  0.9625\n",
            "正解率(test):  0.934375\n",
            "from sklearn.metrics import confusion_matrix # 混合行列\n",
            "# 予測結果と、正解（本当の答え）がどのくらい合っていたかを表す混合行列\n",
            "pd.DataFrame(confusion_matrix(clf.predict(X_test), y_test), index=['predicted 0', 'predicted 1'], columns=['real 0', 'real 1'])\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "clf = RandomForestClassifier(n_estimators=100)\n",
            "clf.fit(X_train, y_train) #学習\n",
            "print(\"正解率(train): \", clf.score(X_train,y_train))\n",
            "print(\"正解率(test): \", clf.score(X_test,y_test))\n",
            "正解率(train):  1.0\n",
            "正解率(test):  0.953125\n",
            "from sklearn.metrics import confusion_matrix # 混合行列\n",
            "# 予測結果と、正解（本当の答え）がどのくらい合っていたかを表す混合行列\n",
            "pd.DataFrame(confusion_matrix(clf.predict(X_test), y_test), index=['predicted 0', 'predicted 1'], columns=['real 0', 'real 1'])\n",
            "pd.DataFrame(clf.feature_importances_, index=list(X.columns)).sort_values(0, ascending=False).style.bar(subset=[0])\n",
            "from sklearn.metrics import roc_curve, precision_recall_curve, auc, classification_report, confusion_matrix\n",
            "# AUCスコアを出す。\n",
            "probas = clf.fit(X_train, y_train).predict_proba(X_test)\n",
            "fpr, tpr, thresholds = roc_curve(y_test, probas[:, 1])\n",
            "roc_auc = auc(fpr, tpr)\n",
            "print (\"ROC score : \",  roc_auc)\n",
            "ROC score :  0.9632107023411371\n",
            "# ROC curve を描く\n",
            "plt.clf()\n",
            "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
            "plt.plot([0, 1], [0, 1], 'k--')\n",
            "plt.xlim([0.0, 1.0])\n",
            "plt.ylim([0.0, 1.0])\n",
            "plt.xlabel('False Positive Rate')\n",
            "plt.ylabel('True Positive Rate')\n",
            "plt.title('Receiver operating characteristic example')\n",
            "plt.legend(loc=\"lower right\")\n",
            "plt.show()\n",
            "# AUPRスコアを出す\n",
            "precision, recall, thresholds = precision_recall_curve(y_test, probas[:, 1])\n",
            "area = auc(recall, precision)\n",
            "print (\"AUPR score: \" , area)\n",
            "AUPR score:  0.760277018572971\n",
            "# PR curve を描く\n",
            "plt.clf()\n",
            "plt.plot(recall, precision, label='Precision-Recall curve')\n",
            "plt.xlabel('Recall')\n",
            "plt.ylabel('Precision')\n",
            "plt.ylim([0.0, 1.05])\n",
            "plt.xlim([0.0, 1.0])\n",
            "plt.title('Precision-Recall example: AUPR=%0.2f' % area)\n",
            "plt.legend(loc=\"lower left\")\n",
            "plt.show()\n",
            "<!DOCTYPE html>\n",
            "<html>\n",
            "  <head>\n",
            "    <meta charset=\"utf-8\">\n",
            "    <title>D3.js</title>\n",
            "    <script type=\"text/javascript\" src=\"/d3/d3.min.js\"></script>\n",
            "  </head>\n",
            "  <body>\n",
            "    <h1>D3.js</h1>\n",
            "    <div id=\"result\"></div>\n",
            "    <script src=\"js/d3js.my.js\"></script>\n",
            "  </body>\n",
            "</html>\n",
            "value\n",
            "200\n",
            "100\n",
            "150\n",
            "200\n",
            "...\n",
            "d3.csv(\"./data.csv\", function(error, list){\n",
            "    d3.select(\"#result\")\n",
            "        .append(\"table\")\n",
            "        .selectAll(\"tr\")\n",
            "        .data(list)\n",
            "        .enter()\n",
            "        .append(\"tr\")\n",
            "        .append(\"td\")\n",
            "        .text(function(d){\n",
            "            return d[\"value\"];\n",
            "        })\n",
            "});\n",
            "<svg width=\"500\" height=\"300\"></svg>\n",
            "// CSV の読み込み\n",
            "d3.csv('data.csv', function(csvdata) {\n",
            "    var dataset = [];\n",
            "    for (var i = 0; i < csvdata.length; i++) {\n",
            "        dataset.push(csvdata[i]['value']);\n",
            "    };\n",
            "    make(dataset);\n",
            "});{\n",
            " \"name\": \"適当なデータ\",\n",
            " \"children\": [\n",
            "  {\n",
            "   \"name\": \"20140713\",\n",
            "   \"children\": [\n",
            "    {\"name\": \"りんご\", \"size\": 8258},\n",
            "    {\"name\": \"ゴリラ\", \"size\": 10001},\n",
            "    {\"name\": \"ラッパ\", \"size\": 8217},\n",
            "    {\"name\": \"パイナップル\", \"size\": 12555},\n",
            "    {\"name\": \"Ruby\", \"size\": 2324},\n",
            "    {\"name\": \"Python\", \"size\": 10993},\n",
            "    {\n",
            "     \"name\": \"20140714\",\n",
            "     \"children\": [\n",
            "      {\"name\": \"バナナ\", \"size\": 9354},\n",
            "      {\"name\": \"梨\", \"size\": 1233}\n",
            "     ]\n",
            "    },\n",
            "    {\"name\": \"いちご\", \"size\": 335},\n",
            "    {\"name\": \"ごりら\", \"size\": 383},\n",
            "    {\"name\": \"らっぱ\", \"size\": 874},\n",
            "    {\n",
            "     \"name\": \"20140715\",\n",
            "     \"children\": [\n",
            "      {\"name\": \"珈琲\", \"size\": 3165},\n",
            "      {\"name\": \"紅茶\", \"size\": 2815},\n",
            "      {\"name\": \"緑茶\", \"size\": 3366}\n",
            "     ]\n",
            "    },\n",
            "    {\"name\": \"抹茶\", \"size\": 17705},\n",
            "    {\"name\": \"烏龍茶\", \"size\": 1486},\n",
            "    {\n",
            "     \"name\": \"20140716\",\n",
            "     \"children\": [\n",
            "      {\"name\": \"月\", \"size\": 6367},\n",
            "      {\"name\": \"火\", \"size\": 1229},\n",
            "      {\"name\": \"水\", \"size\": 2059},\n",
            "      {\"name\": \"木\", \"size\": 2291}\n",
            "     ]\n",
            "    },\n",
            "    {\"name\": \"火星\", \"size\": 5559},\n",
            "    {\"name\": \"水星\", \"size\": 19118},\n",
            "    {\"name\": \"金星\", \"size\": 6887},\n",
            "    {\"name\": \"木星\", \"size\": 6557},\n",
            "    {\"name\": \"土星\", \"size\": 22026}\n",
            "   ]\n",
            "  }\n",
            " ]\n",
            "}\n",
            "var diameter = 960,\n",
            "    format = d3.format(\",d\"),\n",
            "    color = d3.scale.category20c();%%bash\n",
            "pip install chainer\n",
            "%%bash\n",
            "pip install cupy-cuda90\n",
            "%%bash\n",
            "python -c 'import chainer; chainer.print_runtime_info()'\n",
            "Chainer: 4.0.0\n",
            "NumPy: 1.14.0\n",
            "CuPy:\n",
            "  CuPy Version          : 4.0.0\n",
            "  CUDA Root             : /usr/local/cuda\n",
            "  CUDA Build Version    : 9000\n",
            "  CUDA Driver Version   : 9000\n",
            "  CUDA Runtime Version  : 9000\n",
            "  cuDNN Build Version   : 7102\n",
            "  cuDNN Version         : 7005\n",
            "  NCCL Build Version    : 2104\n",
            "%%bash\n",
            "pip install matplotlib\n",
            "from chainer.datasets import mnist# matplotlibを使ったグラフ描画結果がnotebook内に表示されるようにします。\n",
            "%matplotlib inline\n",
            "import matplotlib.pyplot as pltlabel: 5\n",
            "from chainer.datasets import split_dataset_randomprint('Training dataset size:', len(train))\n",
            "print('Validation dataset size:', len(valid))\n",
            "Training dataset size: 50000\n",
            "Validation dataset size: 10000\n",
            "from chainer import iteratorsimport chainer.links as L\n",
            "import chainer.functions as F\n",
            "import random\n",
            "import numpy\n",
            "import chainerimport chainer\n",
            "import chainer.links as L\n",
            "import chainer.functions as Fprint('1つ目の全結合層のバイアスパラメータの形は、', net.l1.b.shape)\n",
            "print('初期化直後のその値は、', net.l1.b.array)\n",
            "1つ目の全結合層のバイアスパラメータの形は、 (100,)\n",
            "初期化直後のその値は、 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "print(net.l1.W.array)\n",
            "None\n",
            "from chainer import optimizersimport numpy as np\n",
            "from chainer.dataset import concat_examples\n",
            "from chainer.cuda import to_cpuepoch:01 train_loss:0.9393 val_loss:0.9708 val_accuracy:0.8019\n",
            "epoch:02 train_loss:0.6163 val_loss:0.5335 val_accuracy:0.8650\n",
            "epoch:03 train_loss:0.4596 val_loss:0.4236 val_accuracy:0.8835\n",
            "epoch:04 train_loss:0.5109 val_loss:0.3750 val_accuracy:0.8931\n",
            "epoch:05 train_loss:0.3167 val_loss:0.3449 val_accuracy:0.9017\n",
            "epoch:06 train_loss:0.4418 val_loss:0.3263 val_accuracy:0.9074\n",
            "epoch:07 train_loss:0.2389 val_loss:0.3103 val_accuracy:0.9122\n",
            "epoch:08 train_loss:0.4076 val_loss:0.2977 val_accuracy:0.9149\n",
            "epoch:09 train_loss:0.3687 val_loss:0.2930 val_accuracy:0.9151\n",
            "epoch:10 train_loss:0.3271 val_loss:0.2800 val_accuracy:0.9190\n",
            "test_accuracy:0.9375\n",
            "with chainer.using_config('train', False):\n",
            "    --- 何か推論処理 ---\n",
            "chainer.config.train = Falsefrom chainer import serializers# ちゃんと保存されていることを確認\n",
            "%ls -la my_mnist.model\n",
            "-rw-rw-r-- 1 shunta shunta 333944 Apr 22 00:52 my_mnist.model\n",
            "# まず同じネットワークのオブジェクトを作る\n",
            "infer_net = MLP()gpu_id = 0  # CPUで計算をしたい場合は、-1を指定してください元の形： (784,) -> ミニバッチの形にしたあと： (1, 784)\n",
            "ネットワークの予測: 7\n",
            "reset_seed(0)# ---------- 学習の1イテレーション ----------\n",
            "train_batch = train_iter.next()\n",
            "x, t = concat_examples(train_batch, gpu_id)from chainer import trainingmax_epoch = 10from chainer.training import extensionstrainer.run()\n",
            "epoch       main/loss   main/accuracy  val/main/loss  val/main/accuracy  l1/W/data/std  elapsed_time\n",
            "1           1.6691      0.599884       0.93909        0.805182           0.0359232      2.04951       \n",
            "2           0.672972    0.843211       0.518216       0.866891           0.0366046      4.94805       \n",
            "3           0.459943    0.878826       0.415205       0.88657            0.0370376      7.98442       \n",
            "4           0.389622    0.893163       0.368696       0.896756           0.0372996      10.9229       \n",
            "5           0.353016    0.900895       0.341141       0.904173           0.0374883      13.7746       \n",
            "6           0.329993    0.907171       0.324052       0.907733           0.0376418      16.5909       \n",
            "7           0.31232     0.911065       0.307601       0.912876           0.037766       20.3998       \n",
            "8           0.298098    0.914383       0.294673       0.917128           0.0378814      24.4631       \n",
            "9           0.28597     0.918059       0.283434       0.918414           0.0379859      27.2953       \n",
            "10          0.275202    0.920836       0.273564       0.921479           0.038085       30.1442       \n",
            "from IPython.display import Image\n",
            "Image(filename='mnist_result/loss.png')\n",
            "Image(filename='mnist_result/accuracy.png')\n",
            "%%bash\n",
            "dot -Tpng mnist_result/cg.dot -o mnist_result/cg.png\n",
            "Image(filename='mnist_result/cg.png')\n",
            "test_evaluator = extensions.Evaluator(test_iter, net, device=gpu_id)\n",
            "results = test_evaluator()\n",
            "print('Test accuracy:', results['main/accuracy'])\n",
            "Test accuracy: 0.9257318\n",
            "reset_seed(0)予測ラベル: 7\n",
            "class MyNet(chainer.Chain):from chainer.datasets import cifarnet = train(MyNet(10), gpu_id=0)\n",
            "epoch       main/loss   main/accuracy  val/main/loss  val/main/accuracy  elapsed_time  lr        \n",
            "1           1.92566     0.304976       1.72248        0.388672           9.64744       0.01        \n",
            "2           1.60993     0.423584       1.52611        0.470703           17.999        0.01        \n",
            "3           1.46872     0.474114       1.43699        0.495898           26.0586       0.01        \n",
            "4           1.39049     0.502264       1.40534        0.49707            33.2387       0.01        \n",
            "5           1.33        0.524261       1.35359        0.515039           41.7211       0.01        \n",
            "6           1.26348     0.547718       1.30809        0.533594           50.5788       0.01        \n",
            "7           1.2179      0.566618       1.30321        0.541211           59.2517       0.01        \n",
            "8           1.16269     0.587447       1.23584        0.566406           69.3356       0.01        \n",
            "9           1.11824     0.603738       1.22467        0.564258           78.8699       0.01        \n",
            "10          1.07353     0.62017        1.20699        0.572852           89.5009       0.01        \n",
            "11          1.02482     0.636586       1.19412        0.577734           97.487        0.01        \n",
            "12          0.984335    0.652355       1.17003        0.591406           108.51        0.01        \n",
            "13          0.942885    0.666171       1.14992        0.595898           117.591       0.01        \n",
            "14          0.9011      0.681646       1.15492        0.598828           126.562       0.01        \n",
            "15          0.853943    0.698531       1.20026        0.5875             135.419       0.01        \n",
            "16          0.812528    0.713319       1.18745        0.597656           145.828       0.01        \n",
            "17          0.765726    0.73149        1.20555        0.594922           154.648       0.01        \n",
            "18          0.727469    0.74343        1.18027        0.603125           165.423       0.01        \n",
            "19          0.672888    0.764423       1.23294        0.596484           176.338       0.01        \n",
            "20          0.633698    0.776855       1.21022        0.598633           186.021       0.01        \n",
            "Test accuracy: 0.605716\n",
            "Image(filename='MyNet_cifar10_result/loss.png')\n",
            "Image(filename='MyNet_cifar10_result/accuracy.png')\n",
            "cls_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
            "             'dog', 'frog', 'horse', 'ship', 'truck']predicted_label: airplane\n",
            "answer: airplane\n",
            "predicted_label: automobile\n",
            "answer: truck\n",
            "predicted_label: dog\n",
            "answer: dog\n",
            "predicted_label: horse\n",
            "answer: horse\n",
            "predicted_label: truck\n",
            "answer: truck\n",
            "class ConvBlock(chainer.Chain):class DeepCNN(chainer.ChainList):chainer.cuda.set_max_workspace_size(512 * 1024 * 1024)\n",
            "chainer.config.autotune = True\n",
            "reset_seed(0)epoch       main/loss   main/accuracy  val/main/loss  val/main/accuracy  elapsed_time  lr        \n",
            "1           2.66805     0.163374       2.14499        0.189648           19.3804       0.1         \n",
            "2           2.0508      0.237837       2.03172        0.23125            33.5836       0.1         \n",
            "3           1.90109     0.285345       1.81864        0.307617           48.0566       0.1         \n",
            "4           1.74244     0.34608        2.01173        0.273633           64.1632       0.1         \n",
            "5           1.6333      0.389111       1.629          0.397266           79.1679       0.1         \n",
            "6           1.50544     0.4466         1.61811        0.431445           92.7088       0.1         \n",
            "7           1.35314     0.505876       1.606          0.423242           104.438       0.1         \n",
            "8           1.19823     0.567893       1.19526        0.575195           116.22        0.1         \n",
            "9           1.0728      0.618208       1.31838        0.549023           129.618       0.1         \n",
            "10          0.974313    0.655938       1.00564        0.647266           140.631       0.1         \n",
            "11          0.897739    0.686523       0.845641       0.702734           152.881       0.1         \n",
            "12          0.837259    0.709046       0.964008       0.673437           164.286       0.1         \n",
            "13          0.780534    0.732888       1.1975         0.609375           176.39        0.1         \n",
            "14          0.743671    0.745259       0.736362       0.741602           188.494       0.1         \n",
            "15          0.717165    0.757191       0.739799       0.743555           201.239       0.1         \n",
            "16          0.684576    0.76814        0.796289       0.72207            212.544       0.1         \n",
            "17          0.661639    0.778032       0.83021        0.719922           224.69        0.1         \n",
            "18          0.633936    0.785423       1.17993        0.651758           237.316       0.1         \n",
            "19          0.624017    0.790264       0.981198       0.69043            249.088       0.1         \n",
            "20          0.608494    0.794966       0.837451       0.725781           261.854       0.1         \n",
            "21          0.585992    0.803864       1.01166        0.696484           274.849       0.1         \n",
            "22          0.568304    0.810369       0.861089       0.70625            288.799       0.1         \n",
            "23          0.570798    0.810564       1.01788        0.688281           301.476       0.1         \n",
            "24          0.559657    0.811901       0.841228       0.728711           313.685       0.1         \n",
            "25          0.542451    0.819713       0.851603       0.723047           326.528       0.1         \n",
            "26          0.536695    0.823161       0.793133       0.753516           340.287       0.1         \n",
            "27          0.525463    0.824774       0.821281       0.742383           352.561       0.1         \n",
            "28          0.527928    0.824475       0.629371       0.796094           363.692       0.1         \n",
            "29          0.51218     0.830211       0.64138        0.794922           375.972       0.1         \n",
            "30          0.513306    0.830373       0.648403       0.7875             387.832       0.1         \n",
            "31          0.320646    0.891246       0.394016       0.870898           399.874       0.01        \n",
            "32          0.246237    0.916088       0.3797         0.880469           411.885       0.01        \n",
            "33          0.223746    0.924006       0.382387       0.883008           424.689       0.01        \n",
            "34          0.201898    0.930908       0.378948       0.882422           438.225       0.01        \n",
            "35          0.186824    0.93563        0.370399       0.883594           451.346       0.01        \n",
            "36          0.173529    0.940541       0.375686       0.886328           463.98        0.01        \n",
            "37          0.154772    0.946559       0.413009       0.882812           474.836       0.01        \n",
            "38          0.150695    0.947177       0.399807       0.880078           487.226       0.01        \n",
            "39          0.139391    0.951011       0.417676       0.884766           498.364       0.01        \n",
            "40          0.131167    0.954568       0.405573       0.886719           511.015       0.01        \n",
            "41          0.121253    0.958097       0.423057       0.885156           523.911       0.01        \n",
            "42          0.120638    0.957866       0.410141       0.883594           536.018       0.01        \n",
            "43          0.109189    0.961448       0.441353       0.879297           548.274       0.01        \n",
            "44          0.107569    0.961961       0.432192       0.884766           559.796       0.01        \n",
            "45          0.102009    0.964378       0.432181       0.886328           572.656       0.01        \n",
            "46          0.0996161   0.964988       0.443999       0.87793            585.535       0.01        \n",
            "47          0.0932453   0.967041       0.452139       0.881445           597.528       0.01        \n",
            "48          0.0879048   0.969262       0.450908       0.884766           609.156       0.01        \n",
            "49          0.0933322   0.967352       0.452228       0.883008           621.725       0.01        \n",
            "50          0.0883782   0.96895        0.46483        0.880078           634.555       0.01        \n",
            "51          0.0873798   0.969729       0.471411       0.875195           646.78        0.01        \n",
            "52          0.0899555   0.968683       0.464493       0.882617           658.161       0.01        \n",
            "53          0.0848383   0.971221       0.455103       0.882422           669.137       0.01        \n",
            "54          0.0794684   0.972701       0.454608       0.879883           680.626       0.01        \n",
            "55          0.082094    0.971577       0.491964       0.875              693.268       0.01        \n",
            "56          0.0799809   0.972523       0.526578       0.868359           705.112       0.01        \n",
            "57          0.0795879   0.972856       0.538439       0.864844           717.415       0.01        \n",
            "58          0.0773325   0.973357       0.478144       0.871875           730.885       0.01        \n",
            "59          0.075146    0.972945       0.520115       0.872656           743.595       0.01        \n",
            "60          0.0758757   0.973892       0.549528       0.869336           756.815       0.01        \n",
            "61          0.0472821   0.98422        0.440967       0.886133           768.614       0.001       \n",
            "62          0.0299252   0.990362       0.438435       0.890234           779.477       0.001       \n",
            "63          0.0247615   0.99221        0.435327       0.895703           790.3         0.001       \n",
            "64          0.0221926   0.993345       0.441702       0.896875           803.134       0.001       \n",
            "65          0.0204117   0.993586       0.448859       0.895703           815.305       0.001       \n",
            "66          0.0188024   0.994695       0.453645       0.895703           825.476       0.001       \n",
            "67          0.0156325   0.99566        0.462118       0.896289           837.684       0.001       \n",
            "68          0.0161309   0.994984       0.462184       0.897461           850.523       0.001       \n",
            "69          0.0151435   0.995237       0.467102       0.89707            861.725       0.001       \n",
            "70          0.0136216   0.995983       0.469565       0.896875           872.419       0.001       \n",
            "71          0.0140849   0.995682       0.469501       0.897852           883.692       0.001       \n",
            "72          0.0139553   0.995783       0.474553       0.89668            896.62        0.001       \n",
            "73          0.0131952   0.996138       0.481529       0.896094           909.393       0.001       \n",
            "74          0.0142982   0.995526       0.480778       0.898242           922.065       0.001       \n",
            "75          0.0121332   0.996138       0.478623       0.899805           933.689       0.001       \n",
            "76          0.0111401   0.996728       0.482796       0.899805           947.017       0.001       \n",
            "77          0.0105382   0.99676        0.484633       0.894922           960.926       0.001       \n",
            "78          0.00964496  0.997329       0.485977       0.897266           974.333       0.001       \n",
            "79          0.00995056  0.996826       0.497885       0.897852           988.168       0.001       \n",
            "80          0.0108769   0.996661       0.498564       0.898047           1000.01       0.001       \n",
            "81          0.0108558   0.996982       0.495446       0.896094           1011.5        0.001       \n",
            "82          0.0102108   0.996871       0.494618       0.896484           1024.42       0.001       \n",
            "83          0.0106756   0.996617       0.497753       0.89707            1037.54       0.001       \n",
            "84          0.0100215   0.996893       0.501453       0.897461           1049.84       0.001       \n",
            "85          0.00844761  0.997529       0.493311       0.898438           1065.14       0.001       \n",
            "86          0.00905977  0.997448       0.499073       0.895117           1078.29       0.001       \n",
            "87          0.00930665  0.997062       0.508955       0.894727           1090.16       0.001       \n",
            "88          0.00867714  0.997559       0.504844       0.899219           1102.02       0.001       \n",
            "89          0.00861321  0.99727        0.502856       0.898047           1113.88       0.001       \n",
            "90          0.00728278  0.997952       0.506968       0.898633           1125.68       0.001       \n",
            "91          0.00845109  0.997603       0.503209       0.89707            1138.37       0.0001      \n",
            "92          0.00766607  0.997952       0.507215       0.896875           1151.05       0.0001      \n",
            "93          0.00709555  0.997936       0.508333       0.89707            1164.28       0.0001      \n",
            "94          0.00712828  0.997863       0.501388       0.896094           1175.9        0.0001      \n",
            "95          0.00740915  0.998224       0.507065       0.896094           1188.58       0.0001      \n",
            "96          0.00704248  0.997975       0.502469       0.896875           1199.79       0.0001      \n",
            "97          0.00837506  0.997559       0.50369        0.897461           1212.92       0.0001      \n",
            "98          0.00636488  0.998091       0.501123       0.898633           1225.74       0.0001      \n",
            "99          0.00658172  0.99813        0.496533       0.89707            1239.14       0.0001      \n",
            "100         0.00759665  0.997891       0.50088        0.897656           1251.94       0.0001      \n",
            "Test accuracy: 0.9019976\n",
            "Image(filename='DeepCNN_cifar10_result/loss.png')\n",
            "Image(filename='DeepCNN_cifar10_result/accuracy.png')\n",
            "class CIFAR10Augmented(chainer.dataset.DatasetMixin):reset_seed(0)epoch       main/loss   main/accuracy  val/main/loss  val/main/accuracy  elapsed_time  lr        \n",
            "1           2.66932     0.155096       2.07969        0.218945           17.7946       0.1         \n",
            "2           1.95089     0.252242       2.16529        0.249609           33.6332       0.1         \n",
            "3           1.80029     0.304087       2.41118        0.232617           48.9252       0.1         \n",
            "4           1.68941     0.352428       1.74426        0.346289           64.1672       0.1         \n",
            "5           1.51953     0.427662       1.64889        0.411523           80.7169       0.1         \n",
            "6           1.33687     0.510121       1.33771        0.523438           96.9036       0.1         \n",
            "7           1.19523     0.572338       1.24248        0.573242           113.866       0.1         \n",
            "8           1.08409     0.616544       1.24074        0.562109           129.411       0.1         \n",
            "9           1.0027      0.64917        1.1433         0.617969           146.937       0.1         \n",
            "10          0.929279    0.677974       0.990748       0.664648           163.91        0.1         \n",
            "11          0.872005    0.699929       0.968499       0.68418            181.605       0.1         \n",
            "12          0.8138      0.722111       0.978409       0.675781           199.057       0.1         \n",
            "13          0.778692    0.73746        0.941924       0.691602           215.46        0.1         \n",
            "14          0.748357    0.749332       0.954998       0.686523           233           0.1         \n",
            "15          0.723875    0.757147       1.03263        0.671484           248.429       0.1         \n",
            "16          0.70128     0.763844       0.922967       0.697852           264.35        0.1         \n",
            "17          0.686641    0.770774       0.932868       0.717188           281.515       0.1         \n",
            "18          0.667459    0.777033       0.761607       0.753125           296.919       0.1         \n",
            "19          0.657883    0.783787       0.696336       0.775586           314.209       0.1         \n",
            "20          0.643969    0.787509       0.788956       0.729883           330.866       0.1         \n",
            "21          0.62682     0.791867       0.747854       0.749414           348.447       0.1         \n",
            "22          0.617861    0.793413       0.724289       0.757422           366.256       0.1         \n",
            "23          0.615907    0.792824       0.760725       0.749609           383.692       0.1         \n",
            "24          0.608608    0.799316       0.626627       0.794141           399.339       0.1         \n",
            "25          0.581148    0.804532       0.724842       0.766016           415.612       0.1         \n",
            "26          0.586145    0.805578       0.758912       0.765039           432.683       0.1         \n",
            "27          0.580653    0.806974       1.53115        0.567773           449.495       0.1         \n",
            "28          0.58332     0.808249       0.638048       0.792773           465.692       0.1         \n",
            "29          0.575656    0.809171       0.597601       0.795898           482.273       0.1         \n",
            "30          0.554497    0.815193       0.92648        0.694141           499.854       0.1         \n",
            "31          0.412056    0.862948       0.353921       0.882812           516.331       0.01        \n",
            "32          0.330784    0.886841       0.323626       0.890625           532.822       0.01        \n",
            "33          0.309464    0.893932       0.314378       0.897461           549.781       0.01        \n",
            "34          0.292333    0.900613       0.298411       0.902148           566.772       0.01        \n",
            "35          0.276572    0.904848       0.294187       0.901758           584.074       0.01        \n",
            "36          0.264323    0.909113       0.306759       0.900781           601.223       0.01        \n",
            "37          0.253866    0.913996       0.300004       0.901172           618.957       0.01        \n",
            "38          0.244404    0.915461       0.289046       0.904883           636.233       0.01        \n",
            "39          0.235541    0.918581       0.302103       0.900781           654.649       0.01        \n",
            "40          0.231642    0.919545       0.309698       0.899219           670.247       0.01        \n",
            "41          0.228038    0.921964       0.294209       0.904102           687.482       0.01        \n",
            "42          0.225327    0.923233       0.300517       0.905273           705.459       0.01        \n",
            "43          0.21667     0.925471       0.323513       0.896875           722.654       0.01        \n",
            "44          0.211944    0.927017       0.323403       0.894141           740.125       0.01        \n",
            "45          0.203643    0.92911        0.306549       0.901367           757.691       0.01        \n",
            "46          0.201874    0.930622       0.316553       0.898633           774.53        0.01        \n",
            "47          0.197785    0.93093        0.321795       0.901172           790.748       0.01        \n",
            "48          0.19062     0.933872       0.326009       0.898438           807.37        0.01        \n",
            "49          0.189966    0.932994       0.302136       0.902148           823.616       0.01        \n",
            "50          0.188014    0.934792       0.351239       0.89082            841.241       0.01        \n",
            "51          0.182187    0.935764       0.330658       0.897461           859.206       0.01        \n",
            "52          0.180877    0.937744       0.331513       0.898242           875.571       0.01        \n",
            "53          0.177914    0.937967       0.315606       0.900977           892.888       0.01        \n",
            "54          0.177183    0.938565       0.30771        0.906055           909.576       0.01        \n",
            "55          0.172863    0.939815       0.310448       0.899414           926.626       0.01        \n",
            "56          0.171187    0.940252       0.334125       0.896875           943.697       0.01        \n",
            "57          0.17084     0.941251       0.321309       0.900586           960.606       0.01        \n",
            "58          0.166495    0.941462       0.313928       0.905078           977.788       0.01        \n",
            "59          0.170402    0.94043        0.336456       0.896094           994.226       0.01        \n",
            "60          0.165143    0.941306       0.333355       0.901758           1011.56       0.01        \n",
            "61          0.125434    0.95641        0.277837       0.916797           1028.05       0.001       \n",
            "62          0.0972963   0.967036       0.279862       0.918164           1044.82       0.001       \n",
            "63          0.0909571   0.968373       0.281997       0.915625           1063.31       0.001       \n",
            "64          0.0854286   0.970197       0.279891       0.916406           1078.31       0.001       \n",
            "65          0.0803817   0.972368       0.283583       0.921484           1094.7        0.001       \n",
            "66          0.078539    0.972945       0.277332       0.917383           1111.61       0.001       \n",
            "67          0.0718806   0.975539       0.289384       0.916406           1128.09       0.001       \n",
            "68          0.0706877   0.975608       0.289274       0.917969           1145.55       0.001       \n",
            "69          0.0701198   0.975405       0.287858       0.91875            1161.81       0.001       \n",
            "70          0.0672646   0.976562       0.289723       0.918555           1178.87       0.001       \n",
            "71          0.0635346   0.978009       0.293459       0.917773           1196.21       0.001       \n",
            "72          0.0641605   0.977583       0.296556       0.914258           1213.21       0.001       \n",
            "73          0.0624464   0.978094       0.297749       0.916406           1229.1        0.001       \n",
            "74          0.0622282   0.978543       0.29798        0.916797           1245.15       0.001       \n",
            "75          0.0624434   0.978538       0.302791       0.917578           1261.09       0.001       \n",
            "76          0.062236    0.978655       0.303235       0.917383           1278.4        0.001       \n",
            "77          0.0583369   0.979759       0.294987       0.91582            1295.95       0.001       \n",
            "78          0.0563044   0.980725       0.30498        0.91875            1310.97       0.001       \n",
            "79          0.0551139   0.980846       0.304319       0.91582            1327.2        0.001       \n",
            "80          0.0538306   0.981548       0.301928       0.917578           1343.2        0.001       \n",
            "81          0.0498568   0.982599       0.303399       0.916406           1359.95       0.001       \n",
            "82          0.0522237   0.981845       0.306208       0.916602           1377.32       0.001       \n",
            "83          0.0487175   0.983284       0.307845       0.917773           1392.65       0.001       \n",
            "84          0.0507888   0.982622       0.305499       0.917383           1409.53       0.001       \n",
            "85          0.0502483   0.982394       0.309835       0.919727           1425.66       0.001       \n",
            "86          0.0495274   0.983043       0.313392       0.917578           1441.89       0.001       \n",
            "87          0.0492812   0.982594       0.308399       0.917969           1458.86       0.001       \n",
            "88          0.0467606   0.983643       0.317299       0.916211           1476.23       0.001       \n",
            "89          0.0451622   0.984042       0.320555       0.918945           1493.84       0.001       \n",
            "90          0.0457013   0.984197       0.313627       0.917773           1510.37       0.001       \n",
            "91          0.0436299   0.984952       0.311594       0.91875            1527.8        0.0001      \n",
            "92          0.0405673   0.986178       0.311017       0.917773           1545.44       0.0001      \n",
            "93          0.0399707   0.986306       0.313432       0.917969           1562.86       0.0001      \n",
            "94          0.0375223   0.987892       0.313193       0.917578           1580.18       0.0001      \n",
            "95          0.0403957   0.986461       0.3141         0.917969           1594.47       0.0001      \n",
            "96          0.0385853   0.987024       0.311208       0.918555           1610.3        0.0001      \n",
            "97          0.0380802   0.986972       0.309954       0.918359           1628.06       0.0001      \n",
            "98          0.0383922   0.986994       0.311925       0.920117           1644.28       0.0001      \n",
            "99          0.0389155   0.986979       0.309176       0.919727           1660.87       0.0001      \n",
            "100         0.0371039   0.98766        0.311089       0.919141           1676.64       0.0001      \n",
            "Test accuracy: 0.9186115\n",
            "Image(filename='DeepCNN_cifar10_augmented_result/loss.png')\n",
            "Image(filename='DeepCNN_cifar10_augmented_result/accuracy.png')\n",
            "from chainer.datasets import TransformDataset%%bash\n",
            "pip install chainercv\n",
            "Requirement already satisfied: chainercv in /home/shunta/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages\n",
            "Requirement already satisfied: chainer>=4.0 in /home/shunta/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages (from chainercv)\n",
            "Requirement already satisfied: Pillow in /home/shunta/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages (from chainercv)\n",
            "Requirement already satisfied: six>=1.9.0 in /home/shunta/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages (from chainer>=4.0->chainercv)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /home/shunta/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages (from chainer>=4.0->chainercv)\n",
            "Requirement already satisfied: filelock in /home/shunta/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages (from chainer>=4.0->chainercv)\n",
            "Requirement already satisfied: protobuf>=3.0.0 in /home/shunta/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages (from chainer>=4.0->chainercv)\n",
            "Requirement already satisfied: setuptools in /home/shunta/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages (from protobuf>=3.0.0->chainer>=4.0->chainercv)x = transforms.random_crop(x, (28, 28))  # ランダムクロップ\n",
            "x = chainercv.transforms.random_flip(x)  # ランダム左右反転\n",
            "from functools import partial\n",
            "from chainercv import transformsreset_seed(0)epoch       main/loss   main/accuracy  val/main/loss  val/main/accuracy  elapsed_time  lr        \n",
            "1           2.96298     0.108887       2.28773        0.116992           18.6204       0.1         \n",
            "2           2.28375     0.123069       2.26054        0.145508           35.9189       0.1         \n",
            "3           2.16893     0.173255       1.97104        0.232617           54.1333       0.1         \n",
            "4           1.95599     0.234308       1.96269        0.239453           71.4199       0.1         \n",
            "5           1.84762     0.287482       1.74268        0.316016           89.6395       0.1         \n",
            "6           1.71918     0.344682       1.48986        0.427539           107.843       0.1         \n",
            "7           1.5616      0.420028       1.33444        0.494531           126.049       0.1         \n",
            "8           1.41343     0.48635        1.34962        0.502539           144.103       0.1         \n",
            "9           1.29375     0.542591       1.27011        0.565039           161.386       0.1         \n",
            "10          1.20855     0.577858       1.09895        0.621289           179.561       0.1         \n",
            "11          1.13542     0.603427       1.12695        0.600391           197.048       0.1         \n",
            "12          1.08715     0.626625       0.967723       0.666797           215.84        0.1         \n",
            "13          1.03912     0.645552       1.09458        0.655078           232.916       0.1         \n",
            "14          0.992342    0.661659       0.813403       0.712305           251.279       0.1         \n",
            "15          0.962952    0.676736       0.89199        0.704492           269.464       0.1         \n",
            "16          0.935602    0.687433       0.871531       0.711133           287.93        0.1         \n",
            "17          0.907586    0.694935       0.826585       0.737695           305.05        0.1         \n",
            "18          0.897032    0.699907       0.759569       0.751758           322.843       0.1         \n",
            "19          0.872711    0.70958        0.747807       0.752148           341.134       0.1         \n",
            "20          0.860642    0.710849       0.897129       0.714648           359.493       0.1         \n",
            "21          0.837644    0.720998       0.877011       0.71582            377.873       0.1         \n",
            "22          0.83491     0.719682       0.811209       0.740625           395.061       0.1         \n",
            "23          0.812516    0.730591       0.991357       0.69668            411.923       0.1         \n",
            "24          0.814586    0.728493       0.870199       0.740625           428.436       0.1         \n",
            "25          0.809576    0.730358       0.851287       0.744336           444.849       0.1         \n",
            "26          0.797749    0.73413        1.0621         0.686523           462.494       0.1         \n",
            "27          0.792886    0.737416       0.765583       0.745703           479.124       0.1         \n",
            "28          0.773611    0.74123        0.59921        0.805273           497.053       0.1         \n",
            "29          0.767626    0.742587       1.15419        0.685938           514.952       0.1         \n",
            "30          0.753782    0.749355       0.937311       0.715234           533.238       0.1         \n",
            "31          0.604251    0.797053       0.399113       0.869727           550.994       0.01        \n",
            "32          0.512318    0.827613       0.375138       0.877344           569.219       0.01        \n",
            "33          0.484475    0.834229       0.353442       0.883984           587.753       0.01        \n",
            "34          0.467319    0.841242       0.364244       0.884375           606.206       0.01        \n",
            "35          0.450393    0.846911       0.341441       0.888086           624.712       0.01        \n",
            "36          0.43828     0.848855       0.342883       0.8875             644.334       0.01        \n",
            "37          0.435329    0.851206       0.34381        0.888867           662.547       0.01        \n",
            "38          0.429289    0.852561       0.334433       0.889258           681.356       0.01        \n",
            "39          0.416126    0.857639       0.335736       0.891602           699.445       0.01        \n",
            "40          0.414865    0.857333       0.340546       0.888867           717.644       0.01        \n",
            "41          0.406426    0.85993        0.331028       0.890039           736.169       0.01        \n",
            "42          0.400299    0.864984       0.325277       0.894531           754.238       0.01        \n",
            "43          0.390969    0.863414       0.31114        0.89668            773.074       0.01        \n",
            "44          0.385966    0.867032       0.315048       0.898047           791.369       0.01        \n",
            "45          0.386317    0.868342       0.311667       0.901758           809.057       0.01        \n",
            "46          0.379431    0.869881       0.326095       0.895508           826.949       0.01        \n",
            "47          0.371893    0.871693       0.30523        0.900195           845.462       0.01        \n",
            "48          0.375175    0.869257       0.313753       0.899414           863.334       0.01        \n",
            "49          0.368864    0.872203       0.335674       0.892969           881.266       0.01        \n",
            "50          0.364174    0.874911       0.329148       0.895703           898.805       0.01        \n",
            "51          0.361013    0.876113       0.325556       0.894531           917.536       0.01        \n",
            "52          0.355882    0.876309       0.312672       0.898633           935.827       0.01        \n",
            "53          0.355655    0.877315       0.314614       0.904492           953.22        0.01        \n",
            "54          0.350558    0.878507       0.335558       0.89375            971.412       0.01        \n",
            "55          0.35336     0.879808       0.309912       0.897461           989.554       0.01        \n",
            "56          0.349074    0.878307       0.315572       0.897656           1007.9        0.01        \n",
            "57          0.344636    0.881792       0.315235       0.898633           1026.21       0.01        \n",
            "58          0.351751    0.878361       0.308908       0.90625            1044.43       0.01        \n",
            "59          0.335435    0.885143       0.325415       0.895898           1062.98       0.01        \n",
            "60          0.342543    0.881366       0.307564       0.904492           1081.12       0.01        \n",
            "61          0.296395    0.897061       0.26379        0.914844           1099.85       0.001       \n",
            "62          0.273945    0.905404       0.255291       0.918945           1118          0.001       \n",
            "63          0.256059    0.911111       0.259149       0.918359           1136.27       0.001       \n",
            "64          0.250382    0.913039       0.26012        0.919922           1153.98       0.001       \n",
            "65          0.251402    0.911066       0.252121       0.919922           1172.19       0.001       \n",
            "66          0.246617    0.914795       0.254615       0.917188           1190.44       0.001       \n",
            "67          0.242367    0.916622       0.260114       0.916602           1208.65       0.001       \n",
            "68          0.237993    0.916815       0.251317       0.918359           1227.71       0.001       \n",
            "69          0.233611    0.919627       0.255463       0.92168            1246.66       0.001       \n",
            "70          0.229861    0.920543       0.251819       0.921289           1264.57       0.001       \n",
            "71          0.231212    0.919916       0.256449       0.918359           1282.98       0.001       \n",
            "72          0.230945    0.920388       0.251938       0.922852           1300.6        0.001       \n",
            "73          0.231567    0.918879       0.25467        0.922656           1319.03       0.001       \n",
            "74          0.228168    0.919649       0.25099        0.922266           1336.64       0.001       \n",
            "75          0.223481    0.922452       0.252225       0.921289           1354.76       0.001       \n",
            "76          0.217657    0.923834       0.2558         0.920898           1373.26       0.001       \n",
            "77          0.217563    0.924383       0.247747       0.923828           1392.33       0.001       \n",
            "78          0.216533    0.923722       0.244587       0.923242           1410.24       0.001       \n",
            "79          0.217534    0.924383       0.248184       0.922852           1428.02       0.001       \n",
            "80          0.217936    0.924435       0.245536       0.923828           1446.1        0.001       \n",
            "81          0.213818    0.925448       0.250882       0.921094           1465.16       0.001       \n",
            "82          0.216617    0.924028       0.245679       0.925586           1483.26       0.001       \n",
            "83          0.211348    0.924991       0.252102       0.923242           1502.12       0.001       \n",
            "84          0.21004     0.92658        0.254647       0.919922           1519.85       0.001       \n",
            "85          0.212658    0.926482       0.252338       0.924219           1539.01       0.001       \n",
            "86          0.210038    0.926824       0.256114       0.921094           1556.43       0.001       \n",
            "87          0.209864    0.927885       0.251901       0.921875           1575.17       0.001       \n",
            "88          0.210507    0.926314       0.253217       0.924414           1593.11       0.001       \n",
            "89          0.201081    0.928911       0.254178       0.925195           1610.7        0.001       \n",
            "90          0.207075    0.92873        0.249093       0.922656           1628.39       0.001       \n",
            "91          0.199509    0.929798       0.248616       0.925              1645.72       0.0001      \n",
            "92          0.199114    0.930467       0.250459       0.922266           1664.97       0.0001      \n",
            "93          0.194366    0.932773       0.250289       0.925781           1682.84       0.0001      \n",
            "94          0.192413    0.933605       0.250086       0.921875           1700.81       0.0001      \n",
            "95          0.195286    0.931374       0.25102        0.925977           1719.84       0.0001      \n",
            "96          0.191748    0.932826       0.247356       0.925391           1738.38       0.0001      \n",
            "97          0.188767    0.935258       0.25027        0.924219           1756.14       0.0001      \n",
            "98          0.190619    0.93517        0.249067       0.925              1774.45       0.0001      \n",
            "99          0.190362    0.934206       0.246483       0.925586           1792.49       0.0001      \n",
            "100         0.193294    0.93295        0.247981       0.926172           1810          0.0001      \n",
            "Test accuracy: 0.92108387\n",
            "from tensorflow.examples.tutorials.mnist import input_data\n",
            "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
            "\\text{evidence}_i = \\sum_j W_{i,~ j} x_j + b_i\n",
            "y = \\text{softmax}(\\text{evidence})\n",
            "\\text{softmax}(x) = \\text{normalize}(\\exp(x))\n",
            "\\text{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n",
            "y = \\text{softmax}(Wx + b)\n",
            "import tensorflow as tf\n",
            "x = tf.placeholder(tf.float32, [None, 784])\n",
            "W = tf.Variable(tf.zeros([784,10]))\n",
            "b = tf.Variable(tf.zeros([10]))\n",
            "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
            "H_{y'}(y) = -\\sum_i y'_i \\log(y_i)\n",
            "y_ = tf.placeholder(tf.float32, [None, 10])\n",
            "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
            "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
            "sess = tf.InteractiveSession()\n",
            "tf.global_variables_initializer().run()\n",
            "for i in range(1000):\n",
            "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
            "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
            "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
            "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
            "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
            "# pandasはデータの取扱系ライブラリ\n",
            "import pandas as pd\n",
            "# とくにDataFrameはよく使うので、個別importしておく\n",
            "from pandas import DataFrame\n",
            "# numpyは行列計算系ライブラリ\n",
            "import numpy as np\n",
            "# matplotlibとseabornはグラフ系ライブラリ\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "# Jupyter Notebookを使う想定なのでブラウザ上にグラフ表示できるように設定\n",
            "%matplotlib inline\n",
            "df = pd.read_csv('CSVファイル名', header = None)\n",
            "# `header=None`を指定せず`pd.read_csv('CSVファイル名')`とした場合は1行目が自動的にカラム名として挿入される\n",
            "df = pd.read_excel('Excelファイル名', sheetname='シート名', header = None)\n",
            "# `header=None`を指定せず`pd.read_excel('Excelファイル名', sheetname='シート名')`とした場合は1行目が自動的にカラム名として挿入される\n",
            "df = pd.read_clipboard(header = None)\n",
            "# `header=None`を指定せず`pd.read_clipboard()`とした場合は1行目が自動的にカラム名として挿入される\n",
            "df.head()\n",
            "# ()の中が何もなければ最初の5行、数字いれればその行数分を上から見れる\n",
            "df.describe()\n",
            "df.shape\n",
            "df = df.rename(columns = {'変更前カラム名1': '変更後カラム名1', '変更前カラム名2': '変更後カラム名2'})\n",
            "df.info()\n",
            "df = df.dropna()\n",
            "# 1つでもNaNが含まれる行が削除される\n",
            "df = df.dropna(subset=['カラム名'])\n",
            "# 指定したカラム名にNaNが含まれる行が削除される\n",
            "# 平均値を入れたいときは\n",
            "df = df.fillna(df.mean())\n",
            "# 中央値を入れたいときは\n",
            "df = df.fillna(df.median())\n",
            "# 最頻値を入れたいときは\n",
            "df = df.fillna(df.mode().iloc[0])\n",
            "# 列方向に推測したデータを入れたいときは\n",
            "df = df.interpolate()\n",
            "# 行方向に推測したデータを入れたいときは\n",
            "df = df.interpolate(axis=1)\n",
            "# 前の値を入れたいときは\n",
            "df = df.fillna(method='ffill')\n",
            "# 後の値を入れたいときは\n",
            "df = df.fillna(method='bfill')\n",
            "df = df.fillna(入れたい値)\n",
            "df = df.fillna({'カラム名1':入れたい値,'カラム名2':入れたい値,'カラム名3':入れたい値})\n",
            "df['Male'] = df['Sex'].map({'male':1,'female':0})\n",
            "def adult_child_baby(age):\n",
            "    if age < 4:\n",
            "        return 'baby'\n",
            "    elif age < 19:\n",
            "        return 'child'\n",
            "    else:\n",
            "        return 'adult'def check_boy(human):\n",
            "    sex, age_range = human\n",
            "    if sex = 'male' and age_range = 'child':\n",
            "        return 1\n",
            "    else:\n",
            "        return 0pd.pivot_table(df, values='Age',index='Age_range', columns='Pclass', aggfunc='mean')\n",
            "# valueに値、indexに行項目、columnsに列項目、aggfuncに値の計算方法を指定\n",
            "df.pivot_table(values='Age',index='Age_range',columns='Survived',aggfunc=\"count\")\n",
            "df.groupby(['Age_range']).mean()\n",
            "df.groupby(['Survived','Age_range']).mean()\n",
            "df['Age_range'].value_counts()\n",
            "df['Age'].hist(bins=50)\n",
            "# binsで棒の本数を指定\n",
            "# まずSurvivedごとにAgeの標準偏差をしらべておく\n",
            "std = df.groupby('Survived')['Age'].std()\n",
            "# SurvivedごとのAgeの平均値を棒グラフ（bar）にして標準偏差の棒をつける\n",
            "df.groupby('Survived')['Age'].mean().plot(yerr=std, kind='bar', legend=False)\n",
            "# kind='bar'で棒グラフになる\n",
            "# legendはTrue, Falseで凡例はつけるかどうか\n",
            "# yerrでエラーバーをいれれる\n",
            "df.groupby('Sex')['Age'].mean().plot(kind='barh', legend=False, figsize=(5,3))\n",
            "# kind='barh'で横棒になる\n",
            "# figsizeでグラフの大きさ\n",
            "sns.countplot('Age_range',data=df)\n",
            "# 引数にカテゴリーを指定\n",
            "sns.countplot('Age_range',data=df,hue='Survived', order=['adult','child','baby'])\n",
            "# hueで細分化したいカテゴリを指定\n",
            "# orderで棒グラフの順番を指定できる\n",
            "sns.barplot(x='Age_range', y='Survived', hue='Sex', data = df)\n",
            "# エラーバーは信頼区間95%を表現\n",
            "sns.distplot(df['Age'].dropna(), bins=50, color='gray')\n",
            "# 曲線はカーネル密度推定\n",
            "# distplotは欠損データがあるとエラーになるのでdropna()を忘れないこと\n",
            "# colorで色が指定できたりもする\n",
            "sns.factorplot('Age_range','Survived',hue='Pclass',data=df, order=['adult','child'])\n",
            "sns.lmplot('Age','Survived', hue='Sex', data=df, hue_order=['female', 'male'], aspect=3)\n",
            "# 直線は回帰直線\n",
            "# 影は信頼区間95%を表現\n",
            "# aspectでグラフの縦横比率を指定できる\n",
            "df2 = sns.load_dataset('car_crashes')\n",
            "df2.head()\n",
            "plt.scatter(df2['speeding'], df2['alcohol'])\n",
            "# 第一引数にX軸にしたいもの、第二引数にY軸にしたいものを指定\n",
            "plt.scatter(df2['speeding'], df2['alcohol'])\n",
            "# X軸とY軸の最大値と最小値を指定\n",
            "plt.xlim([0, 10])\n",
            "plt.ylim([0, 12])\n",
            "# X軸とY軸の名前を指定\n",
            "plt.xlabel('speeding')\n",
            "plt.ylabel('alcohol')\n",
            "sns.scatterplot(x='speeding', y='alcohol', data=df2)\n",
            "sns.jointplot('speeding', 'alcohol', df2)\n",
            "sns.jointplot('speeding', 'alcohol', df2, kind=\"reg\")\n",
            "# kind=\"reg\"を指定すると回帰直線と信頼区間95%の影がでてくる\n",
            "sns.pairplot(df2)\n",
            "# pairplotの引数は欠損値があるとエラーになる\n",
            "sns.pairplot(df2, hue='abbrev')\n",
            "sns.heatmap(df2.corr(), annot=True)\n",
            "# annot=Trueで各マスに数値が入る\n",
            "sns.clustermap(df2.corr(), annot=True)\n",
            "import pandas_datareader.data as pdr\n",
            "import datetime\n",
            "end = datetime.date.today()\n",
            "start = end - datetime.timedelta(days=10)\n",
            "df3 = pdr.DataReader('GOOG', 'yahoo', start, end)\n",
            "df3.head()\n",
            "df3['High'].plot(legend=True, figsize=(10,4))\n",
            "df3[['High', 'Low']].plot(figsize=(10,4), legend=True, linestyle='--', marker='o')\n",
            "# markerで点スタイルを指定\n",
            "# linestyleで点と点を結ぶ線スタイルを指定\n",
            "sns.lineplot(x=df3.index, y='High', data=df3)\n",
            "\\vec{r} = (r_x, r_y, r_w, r_h)^T\n",
            "\\vec{g} = (g_x, g_y, g_w, g_h)^T\n",
            "\\vec{W} = argmin_{\\vec{w}}\\sum^{N}_{n=1}({\\vec{t}_n - \\vec{W}^Tf(\\vec{r}_n)})^2 + \\lambda\\|\\vec{W}\\|^2_{F}\n",
            "\\vec{t} = (t_x, t_y, t_w, t_h)^T\n",
            "t_x = (g_x - r_x) / r_w,\n",
            "t_y = (g_y - r_y) / r_h,\n",
            "t_w = log(g_w / r_w),\n",
            "t_h = log(g_h / r_h),\n",
            "\\vec{p} = (p^0, p^1,... p^N_c)^T\n",
            "\\vec{v} = (v_x, v_y, v_w, v_h)^T\n",
            "J(\\vec{p}, u, \\vec{v}, \\vec{t}) = J_{cls}(\\vec{p}, u) + \\lambda[u >= 1]J_{loc}(\\vec{v}, \\vec{t})\n",
            "J_{cls}(\\vec{p}, u) = -\\log{p^u} \n",
            "J_{loc} = \\sum_{i \\in { \\{x,y,w,h} \\}}smooth_{L1}(t_i - v_i)\n",
            "smooth_{L1}(x) = \\left\\{\n",
            "\\begin{array}{ll}\n",
            "0.5x^2 & if (|x| < 1) \\\\\n",
            "|x| - 0.5 &otherwise\n",
            "\\end{array}\n",
            "\\right.\n",
            "q = P_r(Obj) \\times IoU^{truth}_{pred}\n",
            "IoU^{truth}_{pred}\n",
            "\n",
            "P_r(C_i|Obj) \\times P_r(Obj) \\times IoU^{truth}_{pred}\n",
            "IoU = \\frac{area(R_p \\bigcap R_g)}{area(R_p \\bigcup R_g)}\n",
            "\n",
            "(c+4)kmn\n",
            "x^p_{ij} = {1, 0} \n",
            "\n",
            "L(x,c,l,g) = 1/N(L_{conf}(x, c) + \\alpha L_{loc}(x,l,g))\n",
            "L_{loc}(x,l,g) = \\sum^N_{i \\in Pos}\\sum_{m \\in {cx, cy, w, h}} x^k_{ij} {\\rm smooth_{L1}}(l^m_i-\\hat{g}^m_j)\n",
            "smooth_{L1}(x) = \\left\\{\n",
            "\\begin{array}{ll}\n",
            "0.5x^2 & if (|x| < 1) \\\\\n",
            "|x| - 0.5 &otherwise\n",
            "\\end{array}\n",
            "\\right.\n",
            "\n",
            "\\hat{g}^{cx}_j = (g^{cx}_j - d^{cx}_i) / d^{w}_i,\n",
            "\\hat{g}^{cy}_j = (g^{cy}_j - d^{cy}_i) / d^{h}_i,\n",
            "\\hat{g}^{w}_j = \\log(g^{w}_j / d^{w}_i),\n",
            "\\hat{g}^{h}_j = \\log(g^{h}_j / d^{h}_i),\n",
            "L_{conf}(x, c) = -\\sum^N_{i \\in Pos}x^p_{ij}\\log(\\hat{c}^p_i) -\\sum^N_{i \\in Neg}x^p_{ij}\\log(\\hat{c}^0_i)\n",
            "\\hat{c}^p_i = \\frac{\\exp(c^p_i)}{\\sum_p{\\exp(c^p_i)}}\n",
            "s_k = s_{min} + \\frac{s_{max} - s_{min}}{m-1}(k-1)\n",
            "\n",
            "a_r = {1, 2, 3, 1/2, 1/3}\n",
            "\n",
            "w^a_k = s_k \\sqrt{a_r}\n",
            "\n",
            "h^a_k = s_k / \\sqrt{a_r}\n",
            "\n",
            "s_k' = \\sqrt{s_ks_k+1}\n",
            "        mbox_loc = concatenate([conv4_3_norm_mbox_loc_flat,\n",
            "                                fc7_mbox_loc_flat,\n",
            "                                conv6_2_mbox_loc_flat,\n",
            "                                conv7_2_mbox_loc_flat,\n",
            "                                conv8_2_mbox_loc_flat,\n",
            "                                pool6_mbox_loc_flat],\n",
            "                               axis=1, name='mbox_loc')\n",
            "        mbox_conf = concatenate([conv4_3_norm_mbox_conf_flat,\n",
            "                                 fc7_mbox_conf_flat,\n",
            "                                 conv6_2_mbox_conf_flat,\n",
            "                                 conv7_2_mbox_conf_flat,\n",
            "                                 conv8_2_mbox_conf_flat,\n",
            "                                 pool6_mbox_conf_flat],\n",
            "                                axis=1, name='mbox_conf')\n",
            "num_boxes = mbox_loc._keras_shape[-1] // 4\n",
            "        mbox_loc = Reshape((num_boxes, 4),\n",
            "                           name='mbox_loc_final')(mbox_loc)\n",
            "        mbox_conf = Reshape((num_boxes, num_classes),\n",
            "                            name='mbox_conf_logits')(mbox_conf)\n",
            "    predictions = concatenate([mbox_loc,\n",
            "                               mbox_conf,\n",
            "                               mbox_priorbox],\n",
            "                              axis=2,\n",
            "                              name='predictions')\n",
            "\n",
            "    def decode_boxes(self, mbox_loc, mbox_priorbox, variances):\n",
            "        prior_width = mbox_priorbox[:, 2] - mbox_priorbox[:, 0]\n",
            "        prior_height = mbox_priorbox[:, 3] - mbox_priorbox[:, 1]\n",
            "        prior_center_x = 0.5 * (mbox_priorbox[:, 2] + mbox_priorbox[:, 0])\n",
            "        prior_center_y = 0.5 * (mbox_priorbox[:, 3] + mbox_priorbox[:, 1])\n",
            "    def detection_out(self, predictions, background_label_id=0, keep_top_k=200,\n",
            "                      confidence_threshold=0.01):\n",
            "class PriorBox(Layer):priors[i] = [xmin, ymin, xmax, ymax, varxc, varyc, varw, varh].\n",
            "        inter_upleft = np.maximum(self.priors[:, :2], box[:2])\n",
            "        inter_botright = np.minimum(self.priors[:, 2:4], box[2:])\n",
            "        inter_wh = inter_botright - inter_upleft\n",
            "        inter_wh = np.maximum(inter_wh, 0)\n",
            "        inter = inter_wh[:, 0] * inter_wh[:, 1]\n",
            "        # compute union\n",
            "        area_pred = (box[2] - box[0]) * (box[3] - box[1])\n",
            "        area_gt = (self.priors[:, 2] - self.priors[:, 0])\n",
            "        area_gt *= (self.priors[:, 3] - self.priors[:, 1])\n",
            "        union = area_pred + area_gt - inter\n",
            "        # compute iou\n",
            "        iou = inter / union\n",
            "        return iou\n",
            "        iou = self.iou(box)\n",
            "        encoded_box = np.zeros((self.num_priors, 4 + return_iou))\n",
            "        assign_mask = iou > self.overlap_threshold\n",
            "        if not assign_mask.any():\n",
            "            assign_mask[iou.argmax()] = True\n",
            "        if return_iou:\n",
            "            encoded_box[:, -1][assign_mask] = iou[assign_mask]\n",
            "        assigned_priors = self.priors[assign_mask]\n",
            "        box_center = 0.5 * (box[:2] + box[2:])\n",
            "        box_wh = box[2:] - box[:2]\n",
            "        assigned_priors_center = 0.5 * (assigned_priors[:, :2] +\n",
            "                                        assigned_priors[:, 2:4])\n",
            "        assigned_priors_wh = (assigned_priors[:, 2:4] -\n",
            "                              assigned_priors[:, :2])\n",
            "        # we encode variance\n",
            "        encoded_box[:, :2][assign_mask] = box_center - assigned_priors_center\n",
            "        encoded_box[:, :2][assign_mask] /= assigned_priors_wh\n",
            "        encoded_box[:, :2][assign_mask] /= assigned_priors[:, -4:-2]\n",
            "        encoded_box[:, 2:4][assign_mask] = np.log(box_wh /\n",
            "                                                  assigned_priors_wh)\n",
            "        encoded_box[:, 2:4][assign_mask] /= assigned_priors[:, -2:]\n",
            "        return encoded_box.ravel()\n",
            "        # \n",
            "        assignment = np.zeros((self.num_priors, 4 + self.num_classes + 8))\n",
            "        assignment[:, 4] = 1.0\n",
            "        if len(boxes) == 0:\n",
            "            return assignment\n",
            "        encoded_boxes = np.apply_along_axis(self.encode_box, 1, boxes[:, :4])\n",
            "        encoded_boxes = encoded_boxes.reshape(-1, self.num_priors, 5)\n",
            "        best_iou = encoded_boxes[:, :, -1].max(axis=0)\n",
            "        best_iou_idx = encoded_boxes[:, :, -1].argmax(axis=0)\n",
            "        best_iou_mask = best_iou > 0\n",
            "        best_iou_idx = best_iou_idx[best_iou_mask]\n",
            "        assign_num = len(best_iou_idx)\n",
            "        encoded_boxes = encoded_boxes[:, best_iou_mask, :]\n",
            "        # エンコードした座標の割り当て\n",
            "        assignment[:, :4][best_iou_mask] = encoded_boxes[best_iou_idx,\n",
            "                                                         np.arange(assign_num),\n",
            "                                                         :4]\n",
            "        assignment[:, 4][best_iou_mask] = 0\n",
            "        # クラスの割り当て\n",
            "        assignment[:, 5:-8][best_iou_mask] = boxes[best_iou_idx, 4:]\n",
            "        # 学習用ポジティブサンプルの割り当て\n",
            "        assignment[:, -8][best_iou_mask] = 1\n",
            "        return assignment\n",
            "class MultiboxLoss(object):\n",
            "    def _l1_smooth_loss(self, y_true, y_pred):\n",
            "        abs_loss = tf.abs(y_true - y_pred)\n",
            "        sq_loss = 0.5 * (y_true - y_pred)**2\n",
            "        l1_loss = tf.where(tf.less(abs_loss, 1.0), sq_loss, abs_loss - 0.5)\n",
            "        return tf.reduce_sum(l1_loss, -1)\n",
            "\n",
            "    def _softmax_loss(self, y_true, y_pred):\n",
            "        y_pred = tf.maximum(tf.minimum(y_pred, 1 - 1e-15), 1e-15)\n",
            "        softmax_loss = -tf.reduce_sum(y_true * tf.log(y_pred),\n",
            "                                      axis=-1)\n",
            "        return softmax_loss\n",
            "\n",
            "    def compute_loss(self, y_true, y_pred):\n",
            "        batch_size = tf.shape(y_true)[0]\n",
            "        num_boxes = tf.to_float(tf.shape(y_true)[1])<annotation>\n",
            "        <folder>VOC2007</folder>\n",
            "        <filename>000032.jpg</filename>\n",
            "        <source>\n",
            "                <database>The VOC2007 Database</database>\n",
            "                <annotation>PASCAL VOC2007</annotation>\n",
            "                <image>flickr</image>\n",
            "                <flickrid>311023000</flickrid>\n",
            "        </source>\n",
            "        <owner>\n",
            "                <flickrid>-hi-no-to-ri-mo-rt-al-</flickrid>\n",
            "                <name>?</name>\n",
            "        </owner>\n",
            "        <size>\n",
            "                <width>500</width>\n",
            "                <height>281</height>\n",
            "                <depth>3</depth>\n",
            "        </size>\n",
            "        <segmented>1</segmented>\n",
            "        <object>\n",
            "                <name>aeroplane</name>\n",
            "                <pose>Frontal</pose>\n",
            "                <truncated>0</truncated>\n",
            "                <difficult>0</difficult>\n",
            "                <bndbox>\n",
            "                        <xmin>104</xmin>\n",
            "                        <ymin>78</ymin>\n",
            "                        <xmax>375</xmax>\n",
            "                        <ymax>183</ymax>\n",
            "                </bndbox>\n",
            "        </object>\n",
            "        <object>\n",
            "                <name>aeroplane</name>\n",
            "                <pose>Left</pose>\n",
            "                <truncated>0</truncated>\n",
            "                <difficult>0</difficult>\n",
            "                <bndbox>\n",
            "                        <xmin>133</xmin>\n",
            "                        <ymin>88</ymin>\n",
            "                        <xmax>197</xmax>\n",
            "                        <ymax>123</ymax>\n",
            "                </bndbox>\n",
            "        </object>\n",
            "        <object>\n",
            "                <name>person</name>\n",
            "                <pose>Rear</pose>\n",
            "                <truncated>0</truncated>\n",
            "                <difficult>0</difficult>\n",
            "                <bndbox>\n",
            "                        <xmin>195</xmin>\n",
            "                        <ymin>180</ymin>\n",
            "                        <xmax>213</xmax>\n",
            "                        <ymax>229</ymax>\n",
            "                </bndbox>\n",
            "        </object>\n",
            "        <object>\n",
            "                <name>person</name>\n",
            "                <pose>Rear</pose>\n",
            "                <truncated>0</truncated>\n",
            "                <difficult>0</difficult>\n",
            "                <bndbox>\n",
            "                        <xmin>26</xmin>\n",
            "                        <ymin>189</ymin>\n",
            "                        <xmax>44</xmax>\n",
            "                        <ymax>238</ymax>\n",
            "                </bndbox>\n",
            "        </object>\n",
            "</annotation>\n",
            "[xmin, ymin, xmax, ymax, binary_class_label[クラス数に依存],  prior_box_xmin, prior_box_ymin, prior_box_xmax, prior_box_ymax, prior_box_variance_xmin, prior_box_variance_ymin, prior_box_variance_xmax, prior_box_variance_ymax,]\n",
            "[xmin, ymin, xmax, ymax, binary_class_label[クラス数に依存],  prior_box_xmin, prior_box_ymin, prior_box_xmax, prior_box_ymax, prior_box_variance_xmin, prior_box_variance_ymin, prior_box_variance_xmax, prior_box_variance_ymax,]cd  {download folder}/SIPpip install PyQt5==5.7.1\n",
            "brew install libxml2\n",
            "deploy.prototxt\n",
            "*.caffemodel\n",
            "input: \"data\"\n",
            "input_shape {\n",
            "  dim: *\n",
            "  dim: *\n",
            "  dim: *\n",
            "  dim: *\n",
            "}\n",
            "layer {\n",
            "  name: \"input_1\"\n",
            "  type: \"Input\"\n",
            "  top: \"data\"\n",
            "  input_param {\n",
            "    # These dimensions are purely for sake of example;\n",
            "    # see infer.py for how to reshape the net to the given input size.\n",
            "    shape { dim: * dim: * dim: * dim: * }\n",
            "  }\n",
            "}\n",
            "\\hat{y}_i=\\phi(\\textbf{x}_\\textbf{i})=\\sum_{k = 1}^K f_k(\\textbf{x}_\\textbf{i}),\\ f_k \\in \\mathcal{F}\n",
            "where\\ \\mathcal{F} = \\{f(\\textbf{x}) = w_{q(\\textbf{x})} \\}\n",
            "q:\\mathbb{R}^m \\rightarrow T,w \\in \\mathbb{R}^T\n",
            "L(\\phi)=\\sum_{i} l( \\hat{y}_i, y_i)+ \\sum_{k}\\Omega(f_k)\n",
            "where\\ \\Omega(f)=\\gamma T+\\frac{1}{2} \\lambda \\parallel w \\parallel^2 \n",
            "L^{(t)} =\\sum_{i=1}^n (l( y_i, \\hat{y}_i^{ (t-1) } ) + f_t ( \\textbf{x}_i ) ) + \\Omega(f_t)\n",
            "\\tilde{L}^{(t)} =\\sum_{i=1}^n [g_i f_t ( \\textbf{x}_i ) + \\frac{1}{2} h_i f_t^2 ( \\textbf{x}_i ) ]+ \\Omega(f_t)\n",
            "where\\ g_i=\\partial_{\\hat{y}_i^{ (t-1) } } l(y_i,\\hat{y}_i^{(t-1)}),\\ h_i=\\partial_{\\hat{y}_i^{ (t-1) } }^2 l(y_i,\\hat{y}_i^{(t-1)}) \n",
            "\\tilde{L}^{(t)} (q) =-\\frac{1}{2} \\sum_{j=1}^T \\frac{ (\\sum_{i \\in I_j} g_i )^2 }{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T\n",
            "where\\ I_j := \\{ i|q( \\textbf{x}_i ) = j \\}\n",
            "L_{split} = \\frac{1}{2} \\left[ \\frac {( \\sum_{i \\in I_L} g_i )^2 } {\\sum_{i \\in I_L} h_i + \\lambda } + \\frac {( \\sum_{i \\in I_R} g_i )^2 } {\\sum_{i \\in I_R} h_i + \\lambda } - \\frac {( \\sum_{i \\in I} g_i )^2 } {\\sum_{i \\in I} h_i + \\lambda } \\right] - \\gamma \n",
            "% virtualenv --system-site-packages ~/env/tensorflow\n",
            "% source ~/env/tensorflow/bin/activate\n",
            "(tensorflow) %\n",
            "(tensorflow) % export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.1-py3-none-any.whl\n",
            "(tensorflow) % pip3 install --upgrade $TF_BINARY_URL\n",
            "(tensorflow) % python\n",
            "...\n",
            ">>> import tensorflow as tf\n",
            ">>> hello = tf.constant('Hello, TensorFlow!')\n",
            ">>> sess = tf.Session()\n",
            ">>> print(sess.run(hello))\n",
            "Hello, TensorFlow!\n",
            ">>> a = tf.constant(10)\n",
            ">>> b = tf.constant(32)\n",
            ">>> print(sess.run(a + b))\n",
            "42\n",
            "import tensorflow as tf\n",
            "import numpy as npimport tensorflow as tf\n",
            "import numpy as np\n",
            "# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n",
            "x_data = np.random.rand(100).astype(np.float32)\n",
            "y_data = x_data * 0.1 + 0.3\n",
            "# Try to find values for W and b that compute y_data = W * x_data + b\n",
            "# (We know that W should be 0.1 and b 0.3, but TensorFlow will\n",
            "# figure that out for us.)\n",
            "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
            "b = tf.Variable(tf.zeros([1]))\n",
            "y = W * x_data + b\n",
            "# Minimize the mean squared errors.\n",
            "loss = tf.reduce_mean(tf.square(y - y_data))\n",
            "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
            "train = optimizer.minimize(loss)\n",
            "# Before starting, initialize the variables.  We will 'run' this first.\n",
            "init = tf.global_variables_initializer()\n",
            "# Launch the graph.\n",
            "sess = tf.Session()\n",
            "sess.run(init)0 [ 0.17769754] [ 0.34861392]\n",
            "20 [ 0.1106104] [ 0.29447961]\n",
            "40 [ 0.10272982] [ 0.29857972]\n",
            "60 [ 0.10070232] [ 0.29963461]\n",
            "80 [ 0.10018069] [ 0.29990602]\n",
            "100 [ 0.1000465] [ 0.29997581]\n",
            "120 [ 0.10001197] [ 0.29999378]\n",
            "140 [ 0.10000309] [ 0.2999984]\n",
            "160 [ 0.1000008] [ 0.29999959]\n",
            "180 [ 0.10000021] [ 0.29999989]\n",
            "200 [ 0.1000001] [ 0.29999995]\n",
            "package mainpackage mainpackage mainpackage maindef step_function(x):\n",
            "  if x>0:\n",
            "    return 1\n",
            "  else:\n",
            "    return 0import numpy as np\n",
            "import matplotlib.pylab as plth(x) = \\frac{1}{1+e^{-x}}\\\\\n",
            "e^{-x}はnumpyではnp.exp(-x)と書ける。\n",
            "import numpy as npimport numpy as np\n",
            "import matplotlib.pylab as pltimport numpy as np\n",
            "import matplotlib.pylab as pltimport numpy as np\n",
            "import matplotlib.pylab as pltimport numpy as np\n",
            "import matplotlib.pylab as pltcrawler\n",
            "  webからももクロの画像を集めてくるRubyスクリプト/out\n",
            "  /train\n",
            "    /reni\n",
            "      ..150枚ほどの画像..\n",
            "    /kanako\n",
            "      ..150枚ほどの画像..\n",
            "    /shiori\n",
            "      ..150枚ほどの画像..\n",
            "    /arin\n",
            "      ..150枚ほどの画像..\n",
            "    /momoka\n",
            "      ..150枚ほどの画像..\n",
            "  test\n",
            "    /reni\n",
            "      ..150枚ほどの画像..\n",
            "    /kanako\n",
            "      ..150枚ほどの画像..\n",
            "    /shiori\n",
            "      ..150枚ほどの画像..\n",
            "    /arin\n",
            "      ..150枚ほどの画像..\n",
            "    /momoka\n",
            "      ..150枚ほどの画像..\n",
            "cd python-package; python setup.py install\n",
            "error: Error: setup script specifies an absolute path:修正箇所： 関数で`LIB_PATH`を生成しているところをマスクし，所定のパスをベタ書き．import numpy as np\n",
            "import lightgbm as lgb\n",
            "from sklearn.datasets import load_iris\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import accuracy_scoreimport numpy as np\n",
            "import lightgbm as lgb\n",
            "from sklearn.datasets import load_boston\n",
            "from sklearn.metrics import mean_squared_error\n",
            "from sklearn.model_selection import KFold, train_test_splitgbm = lgb.LGBMClassifier(objective='multiclass',\n",
            "                        num_leaves = 31,\n",
            "                        learning_rate=0.1,\n",
            "                        min_child_samples=10,\n",
            "                        n_estimators=100)\n",
            "gbm.fit(X_train, y_train,\n",
            "        eval_set=[(X_test, y_test)],\n",
            "        eval_metric='multi_logloss',\n",
            "        early_stopping_rounds=10)\n",
            "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
            "def lgb_analysis(X_train, X_test, y_train, y_test, n_folds=5):\n",
            "    '''\n",
            "      Base analysis process by LightGBM\n",
            "    '''\n",
            "    kf = KFold(n_splits=n_folds, random_state=1)\n",
            "    y_preds_train = []\n",
            "    y_preds_test = []\n",
            "    for k, (train, test) in enumerate(kf.split(X_train, y_train)):\n",
            "        gbm = lgb.LGBMClassifier(objective='multiclass',\n",
            "                        num_leaves = 23,\n",
            "                        learning_rate=0.1,\n",
            "                        n_estimators=100)\n",
            "        gbm.fit(X_train[train], y_train[train],\n",
            "            eval_set=[(X_train[test], y_train[test])],\n",
            "            eval_metric='multi_logloss',\n",
            "            verbose=False,\n",
            "            early_stopping_rounds=10)\n",
            "        y_pred_train = gbm.predict_proba(X_train[test], \n",
            "                            num_iteration=gbm.best_iteration)\n",
            "        y_pred_test = gbm.predict_proba(X_test, \n",
            "                            num_iteration=gbm.best_iteration)\n",
            "        y_pred_k = np.argmax(y_pred_test, axis=1)\n",
            "        accu = accuracy_score(y_test, y_pred_k)\n",
            "        print('fold[{:>3d}]: accuracy = {:>.4f}'.format(k, accu))\n",
            "        y_preds_train.append(y_pred_train)\n",
            "        y_preds_test.append(y_pred_test)def xgb_analysis(X_train, X_test, y_train, y_test, n_folds=5):\n",
            "    '''\n",
            "      Base analysis process by XGBoost\n",
            "    '''\n",
            "    kf = KFold(n_splits=n_folds, random_state=1)\n",
            "    y_preds_train = []\n",
            "    y_preds_test = []\n",
            "    for k, (train, test) in enumerate(kf.split(X_train, y_train)):\n",
            "        xgbclf = xgb.XGBClassifier(objective='multi:softmax',\n",
            "                        max_depth=5,\n",
            "                        learning_rate=0.1,\n",
            "                        n_estimators=100)\n",
            "        xgbclf.fit(X_train[train], y_train[train],\n",
            "            eval_set=[(X_train[test], y_train[test])],\n",
            "            eval_metric='mlogloss',\n",
            "            verbose=False,\n",
            "            early_stopping_rounds=10)\n",
            "        y_pred_train = xgbclf.predict_proba(X_train[test])\n",
            "        y_pred_test = xgbclf.predict_proba(X_test)\n",
            "        y_pred_k = np.argmax(y_pred_test, axis=1)\n",
            "        accu = accuracy_score(y_test, y_pred_k)\n",
            "        print('fold[{:>3d}]: accuracy = {:>.4f}'.format(k, accu))\n",
            "        y_preds_train.append(y_pred_train)\n",
            "        y_preds_test.append(y_pred_test)LightGBM process:\n",
            "[LightGBM] [Warning] Ignoring Column_0 , only has one value\n",
            "[LightGBM] [Warning] Ignoring Column_32 , only has one value\n",
            "[LightGBM] [Warning] Ignoring Column_39 , only has one value\n",
            "fold[  0]: accuracy = 0.9360\n",
            "[LightGBM] [Warning] Ignoring Column_0 , only has one value\n",
            "[LightGBM] [Warning] Ignoring Column_32 , only has one value\n",
            "[LightGBM] [Warning] Ignoring Column_39 , only has one value\n",
            "fold[  1]: accuracy = 0.9394\n",
            "[LightGBM] [Warning] Ignoring Column_0 , only has one value\n",
            "[LightGBM] [Warning] Ignoring Column_32 , only has one value\n",
            "[LightGBM] [Warning] Ignoring Column_39 , only has one value\n",
            "fold[  2]: accuracy = 0.9394\n",
            "[LightGBM] [Warning] Ignoring Column_0 , only has one value\n",
            "[LightGBM] [Warning] Ignoring Column_32 , only has one value\n",
            "[LightGBM] [Warning] Ignoring Column_39 , only has one value\n",
            "[LightGBM] [Warning] Ignoring Column_56 , only has one value\n",
            "fold[  3]: accuracy = 0.9276\n",
            "[LightGBM] [Warning] Ignoring Column_0 , only has one value\n",
            "[LightGBM] [Warning] Ignoring Column_24 , only has one value\n",
            "[LightGBM] [Warning] Ignoring Column_32 , only has one value\n",
            "[LightGBM] [Warning] Ignoring Column_39 , only has one value\n",
            "fold[  4]: accuracy = 0.9310\n",
            "XGBoost process:\n",
            "fold[  0]: accuracy = 0.9377\n",
            "fold[  1]: accuracy = 0.9411\n",
            "fold[  2]: accuracy = 0.9444\n",
            "fold[  3]: accuracy = 0.9343\n",
            "fold[  4]: accuracy = 0.9310cd R-package\n",
            "R CMD INSTALL --build  .\n",
            "devtools::install_github(\"Microsoft/LightGBM\", subdir = \"R-package\")\n",
            "library(lightgbm)\n",
            "data(iris)\n",
            "実行コード\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "train = pd.read_csv(\"train.csv\", dtype={\"Age\": np.float64}, )\n",
            "test  = pd.read_csv(\"test.csv\", dtype={\"Age\": np.float64}, )\n",
            "train.head(10)\n",
            "\n",
            "\n",
            "実行コード\n",
            "train_corr = train.corr()\n",
            "train_corr\n",
            "\n",
            "\n",
            "実行コード\n",
            "def correct_data(titanic_data):\n",
            "実行コード\n",
            "train_corr = train.corr()\n",
            "train_corr\n",
            "\n",
            "\n",
            "実行コード\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.svm import SVC, LinearSVC\n",
            "from sklearn.neighbors import KNeighborsClassifier\n",
            "from sklearn.tree import DecisionTreeClassifier\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.neural_network import MLPClassifier\n",
            "実行コード\n",
            "results = []\n",
            "names = []\n",
            "for name,model in models:\n",
            "    result = cross_val_score(model, train_data[predictors], train_data[\"Survived\"],  cv=3)\n",
            "    names.append(name)\n",
            "    results.append(result)\n",
            "実行コード\n",
            "for i in range(len(names)):\n",
            "    print(names[i],results[i].mean())LogisticRegression 0.785634118967\n",
            "SVC 0.687991021324\n",
            "LinearSVC 0.58810325477\n",
            "KNeighbors 0.701459034792\n",
            "DecisionTree 0.766554433221\n",
            "RandomForest 0.796857463524\n",
            "MLPClassifier 0.785634118967\n",
            "\n",
            "実行コード\n",
            "alg = RandomForestClassifier()\n",
            "alg.fit(train_data[predictors], train_data[\"Survived\"])\n",
            "実行コード\n",
            "parameters = {\n",
            "        'n_estimators'      : [5, 10, 20, 30, 50, 100, 300],\n",
            "        'max_depth'         : [3, 5, 10, 15, 20, 25, 30, 40, 50, 100]\n",
            "        'random_state'      : [0],\n",
            "}\n",
            "gsc = GridSearchCV(RandomForestClassifier(), parameters,cv=3)\n",
            "gsc.fit(train_data[predictors], train_data[\"Survived\"])\n",
            "\n",
            "\n",
            "実行コード\n",
            "def correct_data(train_data, test_data):from keras.datasets import mnistfrom keras.utils.np_utils import to_categoricalfrom keras.models import Sequential\n",
            "from keras.layers import Densemodel.fit(x_train, y_train,\n",
            "            batch_size=100,\n",
            "            epochs=12,\n",
            "            verbose=1)\n",
            "score = model.evaluate(x_test, y_test)\n",
            "print(score[0])\n",
            "print(score[1])\n",
            "import keras\n",
            "from keras.datasets import mnist\n",
            "from keras.models import Sequential\n",
            "from keras.layers import Dense, Dropout\n",
            "from keras.optimizers import RMSprop\n",
            "from __future__ import print_function\n",
            "import keras\n",
            "from keras.datasets import mnist\n",
            "from keras.models import Sequential\n",
            "from keras.layers import Dense, Dropout, Flatten\n",
            "from keras.layers import Conv2D, MaxPooling2D\n",
            "from keras import backend as Kimport keras\n",
            "from keras.datasets import mnist\n",
            "from keras.models import Sequential\n",
            "from keras.layers import Dense, Activation\n",
            "from keras.layers import SimpleRNN\n",
            "from keras import initializers\n",
            "from keras.optimizers import RMSpropimport pandas as pd\n",
            "import matplotlib.pyplot as pltdf[\"Age\"].fillna(df.Age.median(), inplace=True)\n",
            "split_data = []\n",
            "for survived in [0,1]:\n",
            "    split_data.append(df[df.Survived==survived])temp = [i[\"Age\"].dropna() for i in split_data]\n",
            "plt.hist(temp, histtype=\"barstacked\", bins=16)\n",
            "df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n",
            "df2 = df.drop([\"Name\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\"], axis=1)\n",
            "df2.dtypestrain_data = df2.values\n",
            "xs = train_data[:, 2:] # Pclass以降の変数\n",
            "y  = train_data[:, 1]  # 正解データ\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "forest = RandomForestClassifier(n_estimators = 100)test_data = test_df2.values\n",
            "xs_test = test_data[:, 1:]\n",
            "output = forest.predict(xs_test)import csv\n",
            "with open(\"predict_result_data.csv\", \"w\") as f:\n",
            "    writer = csv.writer(f, lineterminator='\\n')\n",
            "    writer.writerow([\"PassengerId\", \"Survived\"])\n",
            "    for pid, survived in zip(test_data[:,0].astype(int), output.astype(int)):\n",
            "        writer.writerow([pid, survived])\n",
            "Chainerのインストールからサンプルプログラムの入手方法、関数の説明など。かなり詳しい。\n",
            "example/mnistを使って回帰問題をする場合のプログラムの変更点と陥った問題点に関する説明。\n",
            "example/mnistを使って回帰問題をする場合のプログラムの変更点。\n",
            "mnist classificationをConvolutional Neural Networkを使って実装する。\n",
            "example/mnistを一部変えて学習させた時の結果などが見れる。\n",
            "example/mnistの詳しい解説。chainerで実装されている関数の説明。\n",
            "example/ptb/train_ptb.pyの関数などをコメント付きで詳しく説明している。\n",
            "example/ptb/train_ptb.pyを使って学習したモデルを使って文章を生成するコードを記載。\n",
            "chainerのインストールからGPUドライバの設定、サンプルコードの解説をしている。chainerのversionは少し古いかも。\n",
            "train_ptb.pyを使って学習したモデルを使って文生成するコードを記載。  \n",
            "ImageNetデータセットの容易からConvolutional Neural Networkの解説まで記載。\n",
            "flickr style datasetを使った画像分類。\n",
            "単語ごとにone hotなベクトルを作って文書をポジネガ分類\n",
            "身長(cm)、体重(kg)、胸囲(cm)を使って、肥満状態かどうかを判別\n",
            "論理演算子XORやANDの学習\n",
            "論理演算子XORの学習\n",
            "論理演算子XORの学習\n",
            "文字レベルの言語モデルをChainerで実装したコード\n",
            "青空文庫から太宰治の小説データを取得し、言語モデルを学習。学習したモデルを使ってテキスト生成を行っている。\n",
            "Encoder-Decoderを使った翻訳モデルの実装。\n",
            "アニメのセリフデータを使ってRNNを学習。\n",
            "word2vecで学習した単語の分散表現ベクトルを使って、文書をポジネガ分類。\n",
            "アニメ顔データの取得方法から、データの前処理及びモデルの説明を記載。\n",
            "CIFAR-10データセットをつかって画像を10クラスに分類。\n",
            "CIFAR-10データセットをつかって画像を10クラスに分類。コードの説明が分かりやすい。\n",
            "Intel(R) Core(TM) i7-2600 CPU @ 3.40GHz\n",
            "Memory: 32GB\n",
            "Geforce GTX 1080 （Founders Edition）\n",
            "Ubuntu 16.04\n",
            "Python 3.6.3 :: Anaconda\n",
            "Keras （backend: Tensorflow）\n",
            "def loss_func(y_true, y_pred):\n",
            "    return mean_squared_error(y_true[:, -480:, :], y_pred[:, -480:, :])\n",
            "\n",
            "koume_prophet.py\n",
            "import tweepy\n",
            "import re\n",
            "import pandas as pd\n",
            "import datetime as dt\n",
            "from fbprophet import Prophet\n",
            "import seaborn as sns\n",
            "sns.set_style(style=\"ticks\")\n",
            "\n",
            "\n",
            "koume_prophet.py\n",
            "# キーの設定\n",
            "consumer_key = \"XXXXXXXXXXXXXXXXXXXXXXXX\"\n",
            "consumer_secret = \"XXXXXXXXXXXXXXXXXXXXXXXX\"\n",
            "access_token = \"XXXXXXXXXXXXXXXXXXXXXXXX\"\n",
            "access_secret = \"XXXXXXXXXXXXXXXXXXXXXXXX\"\n",
            "\n",
            "\n",
            "koume_prophet.py\n",
            "## 取得対象の設定\n",
            "# user_idの設定（”@〜”のユーザーIDをサイト(https://idtwi.com/)で数字に変換した後、こちらに代入）\n",
            "USERid = \"391900115\"\n",
            "# 取得したいツイート数の設定\n",
            "numberOfTweets = 5000\n",
            "\n",
            "\n",
            "koume_prophet.py\n",
            "#ユーザーのtweetを取得\n",
            "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
            "auth.set_access_token(access_token, access_secret)\n",
            "api = tweepy.API(auth, wait_on_rate_limit = True)\n",
            "\n",
            "\n",
            "koume_prophet.py\n",
            "aTimeLine = tweepy.Cursor(api.user_timeline,id=USERid).items(numberOfTweets)\n",
            "\n",
            "\n",
            "koume_prophet.py\n",
            "t_txt = [] # Tweetのテキスト情報を格納(今回は不要)\n",
            "t_num = [] # Tweetの日付、いいね数、RT数などを格納\n",
            "cnt = 1\n",
            "for tweet in aTimeLine:\n",
            "    ## 数値・日付情報取得\n",
            "    fav = tweet.favorite_count\n",
            "    rt = tweet.retweet_count\n",
            "    date = str(tweet.created_at)[:10]\n",
            "    ## テキスト情報取得・クレンジング\n",
            "    T = tweet.text.replace('\\n','')\n",
            "    if \"#まいにちチクショー\" in T and \"RT @\" not in T:\n",
            "        T = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+', \"\", T)\n",
            "        T = T.replace('コウメ太夫のチクポーレディオ＃', '')\n",
            "        T = T.strip()\n",
            "        T = T.replace('#まいにちチクショー', '')\n",
            "        T = T.replace('\\u3000', '')\n",
            "        T = T.replace('チクショー！！', '')\n",
            "        t_txt.append([cnt, T])\n",
            "        t_num.append([cnt, date, fav, rt])\n",
            "        cnt += 1\n",
            "\n",
            "\n",
            "koume_prophet.py\n",
            "### データ作成\n",
            "## 取得データを集計\n",
            "data = pd.DataFrame(t_num, columns = [\"index\", \"date\", \"fav\", \"rt\"])\n",
            "data = data.set_index(\"index\")\n",
            "data = data.groupby(\"date\")\n",
            "data_agg = data[[\"fav\",\"rt\"]].sum() \n",
            "## 今日までの日付リスト生成\n",
            "date = dt.date(2016, 3, 1)\n",
            "today = dt.date.today()\n",
            "duration = (today-date).days\n",
            "date_list = []\n",
            "for i in range(duration):\n",
            "    date_list.append(date)\n",
            "    date += dt.timedelta(days=1)\n",
            "date_list = [str(n) for n in date_list]\n",
            "date_list_df = pd.DataFrame(date_list, columns = [\"date\"])\n",
            "## 結合\n",
            "dataset = pd.merge(date_list_df, data_agg, on = \"date\", how = \"left\")\n",
            "dataset =dataset.fillna(0)\n",
            "dataset = dataset.set_index(\"date\")\n",
            "\n",
            "\n",
            "koume_prophet.py\n",
            "# データ準備\n",
            "train_data = dataset[dataset.index.get_loc(\"2016-03-01\"):dataset.index.get_loc(\"2019-02-28\")]\n",
            "test_data = dataset[dataset.index.get_loc(\"2019-03-01\"):dataset.index.get_loc(\"2019-06-17\")]\n",
            "test_data = test_data.reset_index(drop = True)\n",
            "train_data = train_data.reset_index()\n",
            "\n",
            "\n",
            "koume_prophet.py\n",
            "train_data = train_data.rename(columns = {\"date\":\"ds\", \"fav\":\"y\"})\n",
            "\n",
            "\n",
            "koume_prophet.py\n",
            "m = Prophet(yearly_seasonality = True, weekly_seasonality = True)\n",
            "m.fit(train_data)\n",
            "\n",
            "\n",
            "koume_prophet.py\n",
            "future = m.make_future_dataframe(periods = 120)\n",
            "forecast = m.predict(future)\n",
            "m.plot(forecast)\n",
            "\n",
            "\n",
            "koume_prophet.py\n",
            "m.plot_components(forecast)\n",
            "\n",
            "\n",
            "koume_prophet.py\n",
            "forecast = forecast.set_index(\"ds\")\n",
            "result = forecast[forecast.index.get_loc(\"2019-03-01\"):forecast.index.get_loc(\"2019-06-17\")+1]\n",
            "result = result.iloc[:, -1]\n",
            "result = result.reset_index(drop = True)\n",
            "test = test_data.join(result)\n",
            "test = test[test['fav'] != 0]\n",
            "test[\"MAPE\"] = abs((test[\"fav\"]-test[\"yhat\"])/test[\"fav\"])*100\n",
            "test[\"MAPE\"].mean()\n",
            "\n",
            "# import module\n",
            "import featuretools as ft\n",
            "import pandas as pd# generate EntitySet\n",
            "es = ft.EntitySet(id='demodat')# generate relationship\n",
            "r_cust_session = ft.Relationship(es['cust']['customer_id'], es['session']['customer_id'])\n",
            "r_session_trans = ft.Relationship(es['session']['session_id'], es['trans']['session_id'])\n",
            "Entityset(demodat)\n",
            "Entityset: demodat\n",
            "  Entities:\n",
            "    cust [Rows: 5, Columns: 3]\n",
            "    session [Rows: 35, Columns: 4]\n",
            "    trans [Rows: 500, Columns: 5]\n",
            "  Relationships:\n",
            "    session.customer_id -> cust.customer_id\n",
            "    trans.session_id -> session.session_id\n",
            "\n",
            "es['trans'].variables# define aggregate functions\n",
            "list_agg = ['sum','min','max','count']#depth=0\n",
            "# define aggregate functions\n",
            "list_agg = ['sum','median','count','std']\n",
            "# define transfer functions\n",
            "list_trans = ['year','month','day']\n",
            "# run dfs\n",
            "df_feature_depth0, _ = ft.dfs(\n",
            "                                     entityset=es,\n",
            "                                     target_entity='cust',\n",
            "                                     agg_primitives=list_agg,\n",
            "                                     trans_primitives =list_trans,\n",
            "                                     max_depth=0)\n",
            "# count features\n",
            "print(len(df_feature_depth0.columns))# depth=1\n",
            "# define aggregate functions\n",
            "list_agg = ['sum','median','count','std']\n",
            "# define transfer functions\n",
            "list_trans = ['year','month','day']\n",
            "# run dfs\n",
            "df_feature_depth1, _ = ft.dfs(\n",
            "                                     entityset=es,\n",
            "                                     target_entity='cust',\n",
            "                                     agg_primitives=list_agg,\n",
            "                                     trans_primitives =list_trans,\n",
            "                                     max_depth=1)\n",
            "# count features\n",
            "print(len(df_feature_depth1.columns))\n",
            "print(list(df_feature_depth1.columns))# depth=2\n",
            "# define aggregate functions\n",
            "list_agg = ['sum','median','count','std']\n",
            "# define transfer functions\n",
            "list_trans = ['year','month','day']\n",
            "# run dfs\n",
            "df_feature_depth2, _ = ft.dfs(\n",
            "                                     entityset=es,\n",
            "                                     target_entity='cust',\n",
            "                                     agg_primitives=list_agg,\n",
            "                                     trans_primitives =list_trans,\n",
            "                                     max_depth=2)\n",
            "# count features\n",
            "print(len(df_feature_depth2.columns))\n",
            "print(list(df_feature_depth2.columns))pip install seaborn\n",
            "conda install seaborn\n",
            "import numpy as np\n",
            "import matplotlib as mpl\n",
            "import matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import seaborn as snstitanic = sns.load_dataset(\"titanic\")\n",
            "tips = sns.load_dataset(\"tips\")\n",
            "iris = sns.load_dataset(\"iris\")\n",
            "x = np.random.normal(size=100)\n",
            "sns.distplot(x)\n",
            "mean, cov = [0, 1], [(1, .5), (.5, 1)]\n",
            "data = np.random.multivariate_normal(mean, cov, 200)\n",
            "df = pd.DataFrame(data, columns=[\"x\", \"y\"])\n",
            "df.head()\n",
            "    x   y\n",
            "0   1.512856    2.527512\n",
            "1   0.311464    2.143966\n",
            "2   0.774056    1.870894\n",
            "3   -0.995346   -0.371048\n",
            "4   0.756758    2.866903\n",
            "sns.jointplot(x=\"x\", y=\"y\", data=df)\n",
            "sns.stripplot(x=\"day\", y=\"total_bill\", data=tips)\n",
            "sns.swarmplot(x=\"day\", y=\"total_bill\", data=tips)\n",
            "sns.swarmplot(x=\"day\", y=\"total_bill\", hue=\"sex\", data=tips)\n",
            "sns.boxplot(x=\"day\", y=\"total_bill\", data=tips)\n",
            "sns.barplot(x=\"sex\", y=\"survived\", hue=\"class\", data=titanic)\n",
            "sns.pointplot(x=\"sex\", y=\"survived\", hue=\"class\", data=titanic)\n",
            "sns.regplot(x=\"total_bill\", y=\"tip\", data=tips)\n",
            "sns.lmplot(x=\"total_bill\", y=\"tip\", data=tips)\n",
            "sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", data=tips)\n",
            "sns.pairplot(iris, hue=\"species\", size=2.5)\n",
            "# -*- coding: utf-8 -*-\n",
            "import numpy as np\n",
            "from scipy import stats\n",
            "from sklearn.cluster import KMeans\n",
            "import matplotlib.pyplot as plt\n",
            "from IPython.display import display, HTML # Jupyter notebook用\n",
            "%matplotlib inline\n",
            "class XMeans:\n",
            "    \"\"\"\n",
            "    x-means法を行うクラス\n",
            "    \"\"\"if __name__ == \"__main__\":\n",
            "    import matplotlib.pyplot as plt[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2]\n",
            "[[ 1.01854145  2.00982242]\n",
            " [ 1.00199794  1.02110352]\n",
            " [ 2.00022392  2.00435037]\n",
            " [ 2.04408807  1.0518478 ]]\n",
            "[ 42.91288569  44.48049658  37.32131967  29.6422041 ]\n",
            "[20 20 20 20]\n",
            "from sklearn.datasets import make_blobs\n",
            "X, y = make_blobs(n_samples=500,\n",
            "                  n_features=2,\n",
            "                  centers=5,\n",
            "                  cluster_std=0.8,\n",
            "                  center_box=(-10.0, 10.0),\n",
            "                  shuffle=True,\n",
            "                  random_state=1)  # For reproducibilityif __name__ == \"__main__\":\n",
            "    import matplotlib.pyplot as pltfrom sklearn.datasets import make_blobs\n",
            "X, y = make_blobs(n_samples=500,\n",
            "                  n_features=2,\n",
            "                  centers=8,\n",
            "                  cluster_std=1.5,\n",
            "                  center_box=(-10.0, 10.0),\n",
            "                  shuffle=True,\n",
            "                  random_state=1)  # For reproducibilityif __name__ == \"__main__\":\n",
            "    import matplotlib.pyplot as pltdistortions = []km = KMeans(n_clusters=5,       # クラスターの個数\n",
            "            init='k-means++',   # k-means++法によりクラスタ中心を選択\n",
            "            n_init=10,          # 異なるセントロイドの初期値を用いたk-meansの実行回数 default: '10' 実行したうちもっとSSE値が小さいモデルを最終モデルとして選択\n",
            "            max_iter=300,       # k-meansアルゴリズムの内部の最大イテレーション回数  default: '300'\n",
            "            random_state=0)     # セントロイドの初期化に用いる乱数発生器の状態\n",
            "y_km = km.fit_predict(X)\n",
            "from __future__ import print_functionAutomatically created module for IPython interactive environment\n",
            "For n_clusters = 3 The average silhouette_score is : 0.500273979793\n",
            "For n_clusters = 4 The average silhouette_score is : 0.473805434223\n",
            "For n_clusters = 5 The average silhouette_score is : 0.451524016461\n",
            "For n_clusters = 6 The average silhouette_score is : 0.428239776719\n",
            "For n_clusters = 7 The average silhouette_score is : 0.427688325647\n",
            "For n_clusters = 8 The average silhouette_score is : 0.409792863353\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot  as plt\n",
            "from scipy import optimize\n",
            "make_data.py\n",
            "\n",
            "def make_data(N, draw_plot=True, is_confused=False, confuse_bin=50):\n",
            "    '''N個のデータセットを生成する関数\n",
            "    データをわざと複雑にするための機能 is_confusedを実装する\n",
            "    '''\n",
            "    np.random.seed(1) # シードを固定して、乱数が毎回同じ出力になるようにする\n",
            "make_data.py\n",
            "df = make_data(1000)\n",
            "df.head(5)\n",
            "make_data.py\n",
            "df_data = make_data(1000, is_confused=True, confuse_bin=10)\n",
            "\n",
            "\n",
            "make_data.py\n",
            "def sigmoid(z):\n",
            "    return 1.0 / (1 + np.exp(-z))\n",
            "estimate_param.py\n",
            "def define_likelihood(weight_vector, *args):\n",
            "    '''dfのデータセット分をなめていき、対数尤度の和を定義する関数\n",
            "    この関数をOptimizerに喰わせてパラメータの最尤推定を行う    \n",
            "    '''\n",
            "    likelihood = 0\n",
            "    df_data = args[0]\n",
            "estimate_param.py\n",
            "def estimate_weight(df_data, initial_param):\n",
            "    '''学習用のデータとパラメータの初期値を受け取って、\n",
            "    最尤推定の結果の最適パラメータを返す関数\n",
            "    '''        \n",
            "    parameter = optimize.minimize(define_likelihood,\n",
            "                                  initial_param, #適当に重みの組み合わせの初期値を与える\n",
            "                                  args=(df_data),\n",
            "                                  method='Nelder-Mead')\n",
            "optimize.py\n",
            "weight_vector = np.random.rand(3)\n",
            "weight_vector\n",
            "optimize.py\n",
            "#データを作成するして、プロットする\n",
            "df_data = make_data(1000)\n",
            "draw_split_line.py\n",
            "def draw_split_line(weight_vector):\n",
            "    '''分離線を描画する関数\n",
            "    '''\n",
            "    a,b,c = weight_vector\n",
            "    x = np.array(range(-10,10,1))\n",
            "    y = (a * x + c)/-b\n",
            "    plt.plot(x,y, alpha=0.3)    \n",
            "\n",
            "\n",
            "optimize.py\n",
            "weight_vector = estimate_weight(df_data, weight_vector) #最尤推定の実行\n",
            "weight_vector\n",
            "predict.py\n",
            "def validate_prediction(df_data, weight_vector):df_pred = validate_prediction(df_data, weight_vector)\n",
            "df_pred.head(10)\n",
            "show_p.py\n",
            "def draw_prob(df_data):\n",
            "run_predict.py\n",
            "plt.figure(figsize=(16, 4))\n",
            "plt.subplot(1,2,1) #2つの図を並べて表示する準備\n",
            "run_predict.py\n",
            "plt.figure(figsize=(16, 4))\n",
            "plt.subplot(1,2,1)pip install scikit-learn\n",
            "\n",
            "svm.py\n",
            "from sklearn.svm import SVC\n",
            "from sklearn.model_selection import train_test_split\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "\n",
            "\n",
            "svm.py\n",
            "# class: 0\n",
            "df_a = pd.DataFrame({'x1': np.random.randn(100),\n",
            "                     'x2': np.random.randn(100),\n",
            "                     'y' : 0})\n",
            "# class: 1\n",
            "df_b = pd.DataFrame({'x1': np.random.randn(100) + 5,\n",
            "                     'x2': np.random.randn(100) + 3,\n",
            "                     'y' : 1})\n",
            "df = df_a.append(df_b)\n",
            "svm.py\n",
            "# 1. モデルインスタンス生成\n",
            "clf = SVC()\n",
            "class_report.py\n",
            "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
            "data_split.py\n",
            "from sklearn.model_selection import train_test_split\n",
            "cross_val.py\n",
            "from sklearn.model_selection import cross_val_score>> array([ 0.96551724,  0.93103448,  0.96428571,  0.89285714,  0.92857143])\n",
            "\n",
            "grid_cv.py\n",
            "from sklearn.model_selection import GridSearchCV\n",
            "from sklearn.ensemble import RandomForestClassifier>> test_score : 0.9444444444444444\n",
            ">> best_params : {'max_depth': 5, 'n_estimators': 10}\n",
            "\n",
            "pipe_line.py\n",
            "from sklearn.pipeline import Pipeline\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.decomposition import PCA>> test_score : 0.9722222222222222\n",
            ">> best_params : {'pca__n_components': 2, 'rf__max_depth': 10, 'rf__n_estimators': 10}\n",
            "\n",
            "make_pipeline.py\n",
            "from sklearn.pipeline import make_pipelinefrom sklearn.feature_selection import SelectKBest, f_regressionfrom sklearn.feature_selection import SelectPercentile\n",
            "class_report.py\n",
            "from sklearn.metrics import classification_report\n",
            "confusion_matrix.py\n",
            "from sklearn.metrics import confusion_matrixMSE = \\frac{1}{n} \\ \\sum_{i=1}^{n} (f_{i}-y_{i})^2 \\\\\n",
            "\n",
            "mse.py\n",
            "from sklearn.metrics import mean_squared_error>> 0.62519679744672363\n",
            "\n",
            "tf-idf.py\n",
            "from sklearn.feature_extraction.text import TfidfVectorizer[[ 0.          0.70710678  0.          0.70710678  0.        ]\n",
            " [ 0.60534851  0.          0.          0.          0.79596054]\n",
            " [ 0.60534851  0.          0.79596054  0.          0.        ]]\n",
            "from sklearn.datasets import load_wineimport tensorflow as tf# Variables\n",
            "x = tf.placeholder(\"float\", [None, 784])\n",
            "y_ = tf.placeholder(\"float\", [None, 10])# Create the model\n",
            "def model(X, w_h, b_h, w_o, b_o):\n",
            "    h = tf.sigmoid(tf.matmul(X, w_h) + b_h)\n",
            "    pyx = tf.nn.softmax(tf.matmul(h, w_o) + b_o)\\textbf{u} ^{(h)} = \\textbf{w} ^{(h)} \\textbf{z} ^{(i)} + \\textbf{b}^{(h)}\n",
            "\\textbf{z} ^{(h)} = f^{(h)}(\\textbf{u}^{(h)})\n",
            "f^{(h)}  \\ : \\ Sigmoid()\\ ...\\ \\texttt{activation function}\n",
            "\\textbf{u} ^{(o)} = \\textbf{w} ^{(o)} \\textbf{z} ^{(h)} + \\textbf{b}^{(o)}\n",
            "\\textbf{z} ^{(o)} = f^{(o)} (\\textbf{u} ^{(o)})\n",
            "f^{(o)} \\ :\\ Softmax()\\ ...\\ \\texttt{activation function} \n",
            "# Regularization terms (weight decay)\n",
            "L2_sqr = tf.nn.l2_loss(w_h) + tf.nn.l2_loss(w_o)\n",
            "lambda_2 = 0.01# the loss and accuracy\n",
            "loss = cross_entropy + lambda_2 * L2_sqr\n",
            "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
            "correct_prediction = tf.equal(tf.argmax(y_hypo,1), tf.argmax(y_,1))\n",
            "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))# Train\n",
            "init = tf.initialize_all_variables()# （with tf.Session() as sess: の内部になります）from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_functionTraining...\n",
            "  step, accurary =      0:  0.130\n",
            "  step, accurary =   2000:  0.900\n",
            "  step, accurary =   4000:  0.910\n",
            "  step, accurary =   6000:  0.930\n",
            "  step, accurary =   8000:  0.920\n",
            "  step, accurary =  10000:  0.960\n",
            "  step, accurary =  12000:  0.950\n",
            "  step, accurary =  14000:  0.950\n",
            "  step, accurary =  16000:  0.960\n",
            "  step, accurary =  18000:  0.960\n",
            "  step, accurary =  20000:  0.960\n",
            "accuracy =  0.9546\n",
            "get_dummy_dataset.py\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn\n",
            "%matplotlib inline\n",
            "display_dummy_dataset.py\n",
            "plt.figure(figsize =(10,10))\n",
            "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='jet')\n",
            "\n",
            "\n",
            "do_decision_tree.py\n",
            "from sklearn.tree import DecisionTreeClassifier             # 決定木用\n",
            "clf = DecisionTreeClassifier(max_depth=2, random_state = 0) # インスタンス作成 max_depth:木の深さ\n",
            "visualize_tree(clf, X, y)    # 描画実行\n",
            "\n",
            "\n",
            "do_random_forest.py\n",
            "from sklearn.ensemble import RandomForestClassifier # ランダムフォレスト用\n",
            "clf = RandomForestClassifier(n_estimators=100, random_state=0) # インスタンス作成　n_estimators:作る決定木の数の指定\n",
            "visualize_tree(clf, X, y, boundaries=False)\n",
            "\n",
            "\n",
            "visualize_tree.py\n",
            "# 決定木を描画してみる\n",
            "def visualize_tree(classifier, X, y, boundaries=True,xlim=None, ylim=None):\n",
            "    \"\"\"決定木の可視化関数。\n",
            "    INPUTS: 分類モデル, X, y, optional x/y limits.\n",
            "    OUTPUTS: Meshgridを使った決定木の可視化\n",
            "    \"\"\"\n",
            "    # fitを使ったモデルの構築\n",
            "    classifier.fit(X, y)\n",
            "get_dummy_malti_sin_dataset.py\n",
            "from sklearn.ensemble import RandomForestRegressor\n",
            "do_random_forest_regression.py\n",
            "from sklearn.ensemble import RandomForestRegressor # ランダムフォレスト回帰用## SVM RBF Kernel\n",
            "SVC(C=7.7426368268112693, cache_size=200, class_weight=None, coef0=0.0,\n",
            "  degree=3, gamma=7.7426368268112782e-05, kernel='rbf', max_iter=-1,\n",
            "  probability=False, random_state=None, shrinking=True, tol=0.001,\n",
            "  verbose=False)\n",
            "WhatMnist.py\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "# Chainer\n",
            "import chainer\n",
            "import chainer.functions as F\n",
            "import chainer.links as L\n",
            "from chainer import optimizers\n",
            "from chainer import serializers, Variable\n",
            "# 可視化(Jupyter想定)\n",
            "%matplotlib inline\n",
            "import matplotlib.pyplot as plt\n",
            "import matplotlib.cm as cm\n",
            "\n",
            "\n",
            "WhatMnist.py\n",
            "# http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
            "# タプルで(train, valid, test)\n",
            "# train -> (data, label)\n",
            "# valid -> (data, label) 親切すぎてValidationデータまで分かれてる\n",
            "# test -> (data, label)\n",
            "# Pandasを使ってデータを読み込む\n",
            "mnist = pd.read_pickle('mnist.pkl')\n",
            "\n",
            "\n",
            "WhatMnist.py\n",
            "if(0):\n",
            "    # Pandas + csvの場合\n",
            "    mnist = pd.read_csv('mnist.csv') \n",
            "    # Numpy + csvの場合\n",
            "    mnist = np.loadtxt('mnist.csv')\n",
            "    # ラベル列の分離(最初の行がラベルと仮定)\n",
            "    mnist_data, mnist_label = np.split(mnist, [1], axis=1)\n",
            "    # 学習行とテスト行のsplit\n",
            "    x_train,x_test = np.split(mnist_data, [50000])\n",
            "    y_train,y_test = np.split(mnist_label, [50000])\n",
            "\n",
            "\n",
            "WhatMnist.py\n",
            "print('## 次元と量')\n",
            "print(\"train.data:{0}, train.label:{1}\".format(mnist[0][0].shape, mnist[0][1].shape))\n",
            "print(\"valid.data:{0}, valid.label:{1}\".format(mnist[1][0].shape, mnist[1][1].shape))\n",
            "print(\"test.data:{0}, test.label:{1}\".format(mnist[2][0].shape, mnist[2][1].shape))\n",
            "WhatMnist.py\n",
            "x_train = np.array(mnist[0][0], dtype=np.float32)\n",
            "y_train = np.array(mnist[0][1], dtype=np.int32)\n",
            "x_test = np.array(mnist[2][0], dtype=np.float32)\n",
            "y_test = np.array(mnist[2][1], dtype=np.int32)\n",
            "print('x_train:' + str(x_train.shape))\n",
            "print('y_train:' + str(y_train.shape))\n",
            "print('x_test:' + str(x_test.shape))\n",
            "print('y_test:' + str(y_test.shape))\n",
            "\n",
            "\n",
            "WhatMnist.py\n",
            "# 予測器クラス\n",
            "class MLP(chainer.Chain):\n",
            "    def __init__(self):\n",
            "        super(MLP, self).__init__(\n",
            "            l1=L.Linear(784, 100),\n",
            "            l2=L.Linear(100, 100),\n",
            "            l3=L.Linear(100, 10),\n",
            "        )\n",
            "WhatMnist.py\n",
            "n = 10\n",
            "x = Variable(x_test[n:n+1])\n",
            "v = model.predictor(x)\n",
            "plt.imshow(x_test[n:n+1].reshape((28,28)), cmap = cm.Greys_r)\n",
            "print(np.argmax(v.data))\n",
            "\n",
            "curl -fsSL https://mackerel.io/file/script/amznlinux/setup-all-yum.sh | MACKEREL_APIKEY='xxxxxxxxxxxxxxxxx' sh\n",
            "yum -y install mackerel-check-plugins\n",
            "\n",
            "/etc/mackerel-agent/mackerel-agent.conf\n",
            "[plugin.checks.test_log]\n",
            "command = \"check-log --file /home/ec2-user/test.log --pattern FATAL --return\"\n",
            "\n",
            "[root@ip-10-0-0-24 ec2-user]# service mackerel-agent restart\n",
            "Stopping mackerel-agent:                                   [  OK  ]\n",
            "Starting mackerel-agent:                                   [  OK  ]\n",
            "[root@ip-10-0-0-24 ec2-user]#\n",
            "echo \"FATAL:message\" >> /home/ec2-user/test.log\n",
            "\n",
            "/etc/mackerel-agent/mackerel-agent.conf\n",
            "[plugin.checks.check_demo_app]\n",
            "command = \"check-procs --pattern java8\"\n",
            "\n",
            "DD_API_KEY=xxxxxxxxxxxxxxxxxx bash -c \"$(curl -L https://raw.githubusercontent.com/DataDog/dd-agent/master/packaging/datadog-agent/source/install_agent.sh)\"\n",
            "cp /etc/dd-agent/conf.d/process.yaml.example /etc/dd-agent/conf.d/process.yaml\n",
            "\n",
            "/etc/dd-agent/conf.d/process.yaml\n",
            "init_config:[root@ip-10-0-0-24 ec2-user]# service datadog-agent restart\n",
            "Stopping Datadog Agent (using killproc on supervisord):    [  OK  ]\n",
            "Starting Datadog Agent (using supervisord):                [  OK  ]\n",
            "[root@ip-10-0-0-24 ec2-user]#\n",
            "[root@ip-10-0-0-24 conf.d]# service datadog-agent info\n",
            "====================\n",
            "Collector (v 5.10.1)\n",
            "====================\n",
            "・・・・\n",
            "  Checks\n",
            "  ======127.0.0.1 - - [31/Jan/2017:00:57:59 +0000] ERROR hogehogehoge\n",
            "127.0.0.1 - - [31/Jan/2017:00:58:00 +0000] SUCCESS hogehogehoge\n",
            "\n",
            "/opt/datadog-agent/agent/checks/libs/test_log_parser.py\n",
            "from time import strftime\n",
            "import re\n",
            "import time\n",
            "/etc/dd-agent/datadog.conf\n",
            "dogstreams: /var/log/test.log:/opt/datadog-agent/agent/checks/libs/test_log_parser.py:test_log_parser\n",
            "\n",
            "[root@ip-10-0-0-24 libs]# service datadog-agent restart\n",
            "Stopping Datadog Agent (using killproc on supervisord):    [  OK  ]\n",
            "Starting Datadog Agent (using supervisord):                [  OK  ]\n",
            "\n",
            "/var/log/datadog/collector.log\n",
            "2017-01-31 01:46:32 UTC | INFO | dd.collector | checks.collector(datadog.py:143) | Instantiating function-based dogstream\n",
            "2017-01-31 01:46:32 UTC | INFO | dd.collector | checks.collector(datadog.py:150) | dogstream: parsing /var/log/test.log with <function test_log_parser at 0x7f7bb5bcc410> (requested /opt/datadog-agent/agent/checks/libs/test_log_parser.py:test_log_parser)\n",
            "\n",
            "echo \"127.0.0.1 - - [31/Jan/2017:00:58:05 +0000] SUCCESS hogehogehoge5\" >> /var/log/test.log\n",
            "echo \"127.0.0.1 - - [31/Jan/2017:00:57:59 +0000] ERROR hogehogehoge\" >> /var/log/test.log\n",
            "\\| {\\bf x} \\|_p = (\\ |x_1|^p + |x_2|^p + \\cdots + |x_n|^p )^{1/p}\n",
            "\\| {\\bf x} \\|_1 = |x_1| + |x_2|\n",
            "\\| {\\bf x} \\|_2 = \\sqrt{|x_1|^2 + |x_2|^2}\n",
            "def LP(x, y, lp=1):\n",
            "    x = np.abs(x) \n",
            "    y = np.abs(y)\n",
            "    return (x**lp + y**lp)**(1./lp)import numpy\n",
            "import theano\n",
            "import theano.tensor as T\n",
            "x = T.lscalar(\"x\")\n",
            "m = T.dmatrix()\n",
            "x = T.lscalar(\"x\")\n",
            "y = x*2\n",
            "z = T.exp(x)\n",
            "f = theano.function([x], x*2)\n",
            "y = x*2\n",
            "f = theano.function([x], y)\n",
            ">>> f(3)\n",
            "array(6)\n",
            ">>> x = T.dscalar()\n",
            ">>> y = T.dscalar()\n",
            ">>> c = T.dscalar()\n",
            ">>> ff = theano.function([c], x*2+y, givens=[(x, c*10), (y,5)])\n",
            ">>> ff(2)\n",
            "array(45)\n",
            "x, y = T.dscalars(\"x\", \"y\") # ※まとめて宣言する書き方\n",
            "z = (x+2*y)**2\n",
            "gx = T.grad(z, x)\n",
            "gy = T.grad(z, y)\n",
            "fgy = theano.function([x,y], gy)\n",
            ">>> fgy(1,2)\n",
            "array(20.0)\n",
            "変数=theano.shared(object)\n",
            ">>> x = T.dscalar(\"x\")\n",
            ">>> b = theano.shared(numpy.array([1,2,3,4,5]))\n",
            ">>> f = theano.function([x], b * x)\n",
            ">>> f(2)\n",
            "array([  2.,   4.,   6.,   8.,  10.])\n",
            ">>> b.get_value()\n",
            "array([1,2,3,4,5])\n",
            ">>> b.set_value([10,11,12])\n",
            ">>> f(2)\n",
            "array([ 20.,  22.,  24.])\n",
            "c = theano.shared(0)\n",
            "f = theano.function([], c, updates= {c: c+1})\n",
            ">>> f()\n",
            "array(0)\n",
            ">>> f()\n",
            "array(1)\n",
            ">>> f()\n",
            "array(2)\n",
            "x = T.dvector(\"x\") # input\n",
            "c = theano.shared(0.) # これを更新していく。初期値はとりあえず０．\n",
            "y = T.sum((x-c)**2)  # y=最小化したい値\n",
            "gc = T.grad(y, c) # y を c について偏微分\n",
            "d2 = theano.function([x], y, updates={c: c - 0.05*gc}) # 実行する度に c を更新して、現時点のyを返す\n",
            ">>> d2([1,2,3,4,5])\n",
            "array(55.0)\n",
            ">>> c.get_value()\n",
            "1.5\n",
            ">>> d2([1,2,3,4,5])\n",
            "array(21.25)\n",
            ">>> c.get_value()\n",
            "2.25\n",
            ">>> d2([1,2,3,4,5])\n",
            "array(12.8125)\n",
            ">>> c.get_value()\n",
            "2.625\n",
            "from tensorflow.examples.tutorials.mnist import input_data\n",
            "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
            "import tensorflow as tf\n",
            "sess = tf.InteractiveSession()\n",
            "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
            "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
            "W = tf.Variable(tf.zeros([784,10]))\n",
            "b = tf.Variable(tf.zeros([10]))\n",
            "sess.run(tf.global_variables_initializer())\n",
            "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
            "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
            "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
            "for i in range(1000):\n",
            "  batch = mnist.train.next_batch(50)\n",
            "  train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
            "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
            "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
            "print accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
            "def weight_variable(shape):\n",
            "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
            "  return tf.Variable(initial)def conv2d(x, W):\n",
            "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')W_conv1 = weight_variable([5, 5, 1, 32])\n",
            "b_conv1 = bias_variable([32])\n",
            "x_image = tf.reshape(x, [-1,28,28,1])\n",
            "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
            "h_pool1 = max_pool_2x2(h_conv1)\n",
            "W_conv2 = weight_variable([5, 5, 32, 64])\n",
            "b_conv2 = bias_variable([64])W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
            "b_fc1 = bias_variable([1024])keep_prob = tf.placeholder(tf.float32)\n",
            "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
            "W_fc2 = weight_variable([1024, 10])\n",
            "b_fc2 = bias_variable([10])cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
            "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
            "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
            "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
            "sess.run(tf.initialize_all_variables())\n",
            "for i in range(20000):\n",
            "  batch = mnist.train.next_batch(50)\n",
            "  if i%100 == 0:\n",
            "    train_accuracy = accuracy.eval(feed_dict={\n",
            "        x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
            "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
            "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})import numpy as np\n",
            "import pandas as pdRangeIndex: 52560 entries, 0 to 52559\n",
            "Data columns (total 10 columns):\n",
            "time          52560 non-null datetime64[ns]\n",
            "is_train      52560 non-null bool\n",
            "discrete_2    52560 non-null int64\n",
            "cat_1         52560 non-null object\n",
            "cat_2         52560 non-null object\n",
            "num_1         52560 non-null float64\n",
            "num_2         52560 non-null float64\n",
            "num_3         52560 non-null float64\n",
            "discrete_1    52560 non-null int64\n",
            "cat_2_ja      52560 non-null object\n",
            "dtypes: bool(1), datetime64[ns](1), float64(3), int64(2), object(3)\n",
            "memory usage: 3.7+ MB\n",
            "df_large.head()\n",
            "plotnine.ggplot(..., plotnine.aes(...)) + plotnine.geom_line(...) + plotnine.theme(...)\n",
            "# 初期化\n",
            "from plotnine import *\n",
            "import plotnineggplot(\n",
            "    pd.melt(df_large.select_dtypes(include=[float, int, bool]), id_vars=['is_train']), \n",
            "    aes(x='value', y='..density..', group='is_train', fill='is_train')\n",
            ") + geom_histogram(\n",
            "    position='identity', alpha=.6, bins=10,\n",
            ") + facet_wrap(\n",
            "    'variable', scales='free', nrow=1\n",
            ") + scale_fill_manual(\n",
            "    values=cud_rgb_hex\n",
            ") + thm_wide\n",
            "# カテゴリカル変数\n",
            "ggplot(\n",
            "    pd.melt(df_large[df_large.select_dtypes(include=['object']).columns.tolist() + ['is_train']],\n",
            "            id_vars=['is_train']), \n",
            "    aes(x='value', y='..count..', group='is_train', fill='is_train')\n",
            ") + geom_bar(\n",
            "    position='identity', alpha=.5\n",
            ") + facet_wrap('variable', scales='free') + scale_fill_manual(\n",
            "    values=cud_rgb_hex\n",
            ") + thm_wide\n",
            "g_box = ggplot(\n",
            "    pd.melt(df_large.select_dtypes(include=[float, int, bool]), id_vars=['is_train']), \n",
            "    aes(x='is_train', y='value', group='is_train', fill='is_train')\n",
            ") + scale_fill_manual(\n",
            "    values=cud_rgb_hex\n",
            ")\n",
            "g_box + geom_boxplot() + facet_wrap(\n",
            "    'variable', scales='free'\n",
            ") + labs(x=' ', title='Box Plot') + thm_wide\n",
            "tmp = pd.melt(\n",
            "    df_large[['is_train', 'num_1', 'num_2', 'num_3', 'discrete_1', 'discrete_2']].reset_index(),\n",
            "    id_vars=['index', 'is_train']\n",
            ")\n",
            "tmp = pd.merge(tmp, tmp, on=['index', 'is_train'])\n",
            "g = ggplot(\n",
            "    tmp, aes(x='value_x', y='value_y', color='is_train')\n",
            ") + geom_point(size=0.5) + facet_grid(\n",
            "    ['variable_x', 'variable_y'], scales='free'\n",
            ") + scale_color_manual(\n",
            "    values=cud_rgb_hex\n",
            ")+ thm_sq\n",
            "# x=0, y=0 の補助線を引く\n",
            "g + geom_vline(xintercept=0, linetype=':') + geom_hline(yintercept=0, linetype=':')\n",
            "ggplot(df_large, aes(x='num_1', y='num_2', color='num_3', size='num_3')) + geom_point() + facet_wrap('is_train') + thm_sq\n",
            "# 別々の系列として作成\n",
            "g = ggplot(df_large, aes(x='time')) + scale_color_manual(values=cud_rgb_hex)\n",
            "cols = ['num_1', 'num_2', 'num_3']\n",
            "for col in cols:\n",
            "    g = g + geom_line(aes(y='np.cumsum(np.abs({}))'.format(col),\n",
            "                          color='\"{}\"'.format(col), linetype='\"{}\"'.format(col)), size=2)\n",
            "print(g + scale_y_log10() + thm_wide)# 初期化\n",
            "import altair as alt\n",
            "alt.renderers.enable('notebook')  # jupyter notebook の場合のみ初期化に必要\n",
            "# ユニバーサルカラースキーマ作成\n",
            "scale_color_cud = alt.Scale(range=cud_rgb_hex)\n",
            "# 上限エラー無効化\n",
            "alt.data_transformers.enable('default', max_rows=None)\n",
            "hists = []\n",
            "chart = alt.Chart(df_large.iloc[:500]).mark_area(opacity=0.3, interpolate='step')# 数値列名のリスト\n",
            "cols = [x for x in df_large.select_dtypes(include=[int, float]).columns]def make_altair_grouped_boxplot(data, column, group):\n",
            "    box = alt.Chart(data).encode(x='{}:O'.format(group))\n",
            "    whiskers = box.encode(\n",
            "        x='{}:O'.format(group)\n",
            "    ).transform_window(\n",
            "        #  外れ値の選別のため transform_aggregate は使えない\n",
            "        lower_box='q1({})'.format(column),\n",
            "        upper_box='q3({})'.format(column),\n",
            "        med='median({})'.format(column),\n",
            "        frame=[None, None], groupby=[group]\n",
            "    ).transform_calculate(\n",
            "        IQR='datum.upper_box - datum.lower_box'\n",
            "    ).transform_calculate(\n",
            "        lower_whisker='datum.lower_box - 1.5 * datum.IQR',\n",
            "        upper_whisker='datum.upper_box + 1.5 * datum.IQR'\n",
            "    )\n",
            "    # 外れ値の選別\n",
            "    outliers = whiskers.encode(\n",
            "        alt.X('{}:O'.format(group)),\n",
            "        alt.Y(column, title=None), # y軸名を上に移動させるため\n",
            "        alt.Color(group, scale=scale_color_cud)\n",
            "    ).mark_circle().transform_filter(\n",
            "        '(datum.{0} < datum.lower_whisker) | (datum.upper_whisker < datum.{0})'.format(column)\n",
            "    )\n",
            "    # 1件のみ残す\n",
            "    whiskers = whiskers.transform_window(\n",
            "        rank='rank()', frame=[None, None], groupby=[group]\n",
            "    ).transform_filter('datum.rank ==1')cols = ['num_1', 'num_2', 'num_3', 'discrete_1', 'discrete_2']\n",
            "alt.hconcat(*[make_altair_grouped_boxplot(df_large.iloc[:500], col, 'is_train') for col in cols]).properties(title='箱ひげ図 (Tukey)')\n",
            "alt.Chart(df_large.iloc[:500]).mark_circle().encode(\n",
            "    alt.Color('num_3', scale=alt.Scale(type='log')),\n",
            "    alt.Size('num_3', scale=alt.Scale(type='log')),\n",
            "    x='num_1', y='num_2'\n",
            ").facet(column='is_train').properties(title='疑似三次元プロット').interactive()\n",
            "c = alt.Chart(df_large.iloc[:500]).transform_calculate(\n",
            "    **{'abs_{}'.format(i): 'abs(datum.num_{})'.format(i) for i in range(1, 4)}\n",
            ").transform_window(\n",
            "    # 累積値に変換\n",
            "    **{'cumsum_{}'.format(i): 'sum(abs_{})'.format(i) for i in range(1, 4)},\n",
            "    frame=[None, 0]\n",
            ").transform_calculate(\n",
            "    # 色分け用のダミー列\n",
            "    **{'col_{}'.format(i): '\"num_{}\"'.format(i) for i in range(1, 4)}\n",
            ").encode(\n",
            "    x='time:T',\n",
            ")\n",
            "alt.layer(\n",
            "    c.mark_line(strokeDash=[20,5]).encode(alt.Y('cumsum_1:Q', title='num_1', scale=alt.Scale(type='log')),\n",
            "                         alt.Color('col_1:O', scale=scale_color_cud)),\n",
            "    c.mark_line(strokeDash=[5, 5]).encode(alt.Y('cumsum_2:Q', title='num_2'), color='col_2:O'),\n",
            "    c.mark_line(strokeDash=[10, 3, 3]).encode(alt.Y('cumsum_3:Q', title='num_3'), color='col_3:O')\n",
            ").properties(width=800, height=200, title='時系列グラフ').interactive()\n",
            "alt.vconcat(\n",
            "    c.mark_line(strokeDash=[20, 5], color=cud_rgb_hex[0]).encode(alt.Y('cumsum_1:Q', scale=alt.Scale(type='log'))).properties(width=800, height=100).interactive(),\n",
            "    c.mark_line(strokeDash=[5, 5], color=cud_rgb_hex[1]).encode(alt.Y('cumsum_2:Q', scale=alt.Scale(type='log'))).properties(width=800, height=100).interactive(),\n",
            "    c.mark_line(strokeDash=[10, 3, 3], color=cud_rgb_hex[2]).encode(alt.Y('cumsum_3:Q', scale=alt.Scale(type='log'))).properties(width=800, height=100).interactive()\n",
            ").properties(title='時系列グラフ')\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as snscols = ['num_1', 'num_2', 'num_3', 'discrete_1', 'discrete_2', 'cat_1', 'cat_2_ja']\n",
            "fig, axes = plt.subplots(ncols=len(cols), nrows=1, figsize=(16, 5))\n",
            "for col, ax in zip(cols, axes):\n",
            "    if df_large[col].dtype in [int, float]:\n",
            "        sns.distplot(df_large.query('is_train')[col], bins=10, kde=False, norm_hist=True, label='True', ax=ax)\n",
            "        sns.distplot(df_large.query('~is_train')[col], bins=10, kde=False, norm_hist=True, label='False', ax=ax)\n",
            "    elif df_large[col].dtype == 'O':\n",
            "        sns.countplot(x=col, data=df_large.query('is_train'),label='True', ax=ax)\n",
            "        sns.countplot(x=col, data=df_large.query('~is_train'), label='False', ax=ax)\n",
            "plt.suptitle('複数変数のヒストグラム')\n",
            "fig.legend()\n",
            "plt.show()\n",
            "sns.pairplot(df_large.assign(is_train=df_large['is_train'].astype(str)),\n",
            "             hue='is_train')\n",
            "plt.suptitle('散布図行列')\n",
            "plt.show()\n",
            "cols = ['num_1', 'num_2', 'num_3', 'discrete_1']\n",
            "fig, axes = plt.subplots(ncols=len(cols), nrows=1, figsize=(16, 5))\n",
            "for col, ax in zip(cols, axes):\n",
            "    sns.boxplot(data=df_large, x='is_train', y=col, ax=ax)\n",
            "    ax.set_title(col)\n",
            "    ax.set_ylabel('')\n",
            "plt.suptitle('複数変数の箱ひげ図')\n",
            "plt.show()\n",
            "plt.figure(figsize=(16, 5))\n",
            "sns.relplot(data=df_large, x='num_1', y='num_2', hue='num_3', size='num_3', col='is_train')\n",
            "plt.suptitle('疑似三次元プロット')\n",
            "plt.show()\n",
            "# overray\n",
            "cols = ['num_1', 'num_2', 'num_3']\n",
            "fig, ax = plt.subplots(figsize=(16, 5))\n",
            "for col in cols:\n",
            "    sns.lineplot(x=df_large['time'], y=np.cumsum(df_large[col].abs()), label=col, ax=ax)\n",
            "plt.suptitle('時系列グラフ')\n",
            "ax.set_ylabel('')\n",
            "ax.set(yscale='log')\n",
            "plt.show()import pixiedustI_H(t) = - \\sum_{i = 1}^{c} p(i|t)log_2 p(i|t) \\\\\n",
            "ただし，p(i|t) = \\frac{n_i}{N} \\\\\n",
            "I_H(t) = - \\sum_{i = 1}^{1} \\frac{n_i}{N} log_2 \\frac{n_i}{N} = \\frac{N}{N} log_2 \\frac{N}{N} = 0\n",
            "I_H(t) = - \\sum_{i = 1}^{N} \\frac{1}{N} log_2 \\frac{1}{N} = log_2 N\n",
            "\\Delta I_H(t) = I_H(t_B) - \\sum_{i=1}^{b} w_i I_{Hi} (t_{Ai})\n",
            "I_G(t) = 1- \\sum_{i = 1}^{c} p(i|t)^2 \\\\\n",
            "ただし，p(i|t) = \\frac{n_i}{N} \\\\\n",
            "I_G(t) = 1- \\sum_{i = 1}^{1} (\\frac{N}{N})^2 = 0 \n",
            "I_G(t) = 1- \\sum_{i = 1}^{N} (\\frac{1}{N})^2 = 1 - \\frac{1}{N}\n",
            "\\Delta I_G(t) = I_G(t_B) - w_L I_G(t_L) - w_R I_G(t_R)\n",
            "\n",
            "decision_tree.py\n",
            "from sklearn.datasets import load_iris\n",
            "from sklearn import treeimport argparse\n",
            "import logging\n",
            "from typing import List, Tuple, Counter as _Counter\n",
            "from collections import CounterPlayer(Naive) win rate 17.20000%\n",
            "## Battle No.0\n",
            "### Turn No.1\n",
            "      Attack guardian:1 ->   archer:0 damage=55 \n",
            "      Attack    thief:0 ->    devil:1 damage=18 \n",
            "      Attack   archer:0 ->    devil:1 damage=36 \n",
            "      Attack    devil:1 ->    thief:0 damage=15 \n",
            "      Attack   knight:0 -> guardian:1 damage=23 \n",
            "      Attack     ogre:1 ->    thief:0 damage=21 \n",
            "### Turn No.2\n",
            "      Attack    thief:0 -> guardian:1 damage=14 \n",
            "      Attack guardian:1 ->    thief:0 damage=55 defeated\n",
            "      Attack   archer:0 ->     ogre:1 damage=27 \n",
            "      Attack   knight:0 ->     ogre:1 damage=17 \n",
            "      Attack    devil:1 ->   archer:0 damage=19 \n",
            "      Attack     ogre:1 ->   archer:0 damage=22 defeated\n",
            "### Turn No.3\n",
            "      Attack guardian:1 ->   knight:0 damage=47 \n",
            "      Attack    devil:1 ->   knight:0 damage=11 \n",
            "      Attack   knight:0 -> guardian:1 damage=19 \n",
            "      Attack     ogre:1 ->   knight:0 damage=15 \n",
            "### Turn No.4\n",
            "      Attack guardian:1 ->   knight:0 damage=44 defeated\n",
            "@dataclass\n",
            "class NaiveGambit(Gambit):\n",
            "    \"\"\"\n",
            "    ランダムに行動する\n",
            "    \"\"\"    # バトルシミュレーション2 （プレイヤーの行動を、チートして決める）\n",
            "    gambits[Side.PLAYER] = CunningGambit()\n",
            "    win, logs = simulate_battle(teams, gambits, args.n_battle)\n",
            "    print(\"Player(Cunning) win rate %7.5f%%\" % (100.0 * win[0] / sum(win.values()),))\n",
            "Player(Cunning) win rate 90.90000%\n",
            "    # 学習を始める\n",
            "    logger.info(\"# Training\")@dataclass\n",
            "class MLbasedGambit(Gambit):\n",
            "    \"\"\"\n",
            "    機械学習にもとづいて最適な行動を推測\n",
            "    \"\"\"    # バトルシミュレーション3 （プレイヤーの行動を、機械学習モデルで決める）\n",
            "    gambits[Side.PLAYER] = MLbasedGambit()\n",
            "    win, logs = simulate_battle(teams, gambits, args.n_battle)\n",
            "    print(\"Player(MLbased) win rate %7.5f%%\" % (100.0 * win[0] / sum(win.values()),))\n",
            "Player(Cunning) win rate 90.80000%\n",
            "    # プレイヤーのステータスをいじる\n",
            "    teams[0] = Team(0, [\n",
            "        Unit(Status(Side.PLAYER, 0, \"knight2\", 104, 76, 65, 32)),\n",
            "        Unit(Status(Side.PLAYER, 1, \"archer2\", 76, 52, 40, 50)),\n",
            "        Unit(Status(Side.PLAYER, 2, \"thief2\", 89, 52, 54, 68)),\n",
            "    ])Player(Naive) win rate 15.40000%\n",
            "Player(Cunning) win rate 92.30000%\n",
            "Player(MLbased) win rate 92.30000%\n",
            "input = [\n",
            "  [1., 0., 0.],\n",
            "  [0., 1., 0.],\n",
            "  [0., 0., 1.]\n",
            "]\n",
            "winning_hands = [\n",
            "  [0., 1., 0.],\n",
            "  [0., 0., 1.],\n",
            "  [1., 0., 0.]\n",
            "]\n",
            "W = \n",
            "\\begin{bmatrix}\n",
            "0 & 1 & 0 \\\\\n",
            "0 & 0 & 1 \\\\\n",
            "1 & 0 & 0 \n",
            "\\end{bmatrix}\n",
            "import tensorflow as tfimport tensorflow as tf$ python main.py\n",
            "3.27587\n",
            "1.85469\n",
            "1.17299\n",
            "0.821553\n",
            "0.619587\n",
            "0.492207\n",
            "0.40589\n",
            "0.3441\n",
            "0.297949\n",
            "0.2623\n",
            "# シンボリック変数定義 \"T\" は theano.tensor の置き換え\n",
            "X = T.scalar()\n",
            "Y = T.scalar()\n",
            "def model(X, w):\n",
            "    return X * w\n",
            "# 共有変数の宣言\n",
            "w = theano.shared(np.asarray(0., dtype=theano.config.floatX))\n",
            "y = model(X, w)\n",
            "# グラフ定義\n",
            "cost = T.mean(T.sqr(y - Y))\n",
            "gradient = T.grad(cost=cost, wrt=w)\n",
            "updates = [[w, w - gradient * 0.01]]\n",
            "# 関数\n",
            "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
            "# ループ計算による学習\n",
            "for i in range(100):\n",
            "    for x, y in zip(trX, trY):\n",
            "        train(x, y)import numpy as np\n",
            "import tensorflow as tfw = tf.Variable([0.])\n",
            "b = tf.Variable([0.])y_hypo = lin_model(x, w, b)\n",
            "cost = tf.reduce_mean(tf.square(y_hypo - y))train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cost)# Initializing\n",
            "init = tf.initialize_all_variables()# Train\n",
            "with tf.Session() as sess:\n",
            "    sess.run(init)    0:(w,b)=(    0.0135,     0.0599)\n",
            "  100:(w,b)=(    0.9872,     2.6047)\n",
            "  200:(w,b)=(    1.4793,     2.9421)\n",
            "  300:(w,b)=(    1.7280,     2.9869)\n",
            "  400:(w,b)=(    1.8538,     2.9928)\n",
            "  500:(w,b)=(    1.9173,     2.9936)\n",
            "  600:(w,b)=(    1.9494,     2.9937)\n",
            "  700:(w,b)=(    1.9657,     2.9937)\n",
            "  800:(w,b)=(    1.9739,     2.9937)\n",
            "  900:(w,b)=(    1.9780,     2.9937)\n",
            " 1000:(w,b)=(    1.9801,     2.9937)import numpy as np\n",
            "import tensorflow as tf def lin_model(X, w, b):\n",
            "    return X * w + b<annotation>\n",
            "    <folder>VOC2007</folder>\n",
            "    <filename>000001.jpg</filename>\n",
            "    <source>\n",
            "        <database>The VOC2007 Database</database>\n",
            "        <annotation>PASCAL VOC2007</annotation>\n",
            "        <image>flickr</image>\n",
            "        <flickrid>341012865</flickrid>\n",
            "    </source>\n",
            "    <owner>\n",
            "        <flickrid>Fried Camels</flickrid>\n",
            "        <name>Jinky the Fruit Bat</name>\n",
            "    </owner>\n",
            "    <size>\n",
            "        <width>353</width>\n",
            "        <height>500</height>\n",
            "        <depth>3</depth>\n",
            "    </size>\n",
            "    <segmented>0</segmented>\n",
            "    <object>\n",
            "        <name>dog</name>\n",
            "        <pose>Left</pose>\n",
            "        <truncated>1</truncated>\n",
            "        <difficult>0</difficult>\n",
            "        <bndbox>\n",
            "            <xmin>48</xmin>\n",
            "            <ymin>240</ymin>\n",
            "            <xmax>195</xmax>\n",
            "            <ymax>371</ymax>\n",
            "        </bndbox>\n",
            "    </object>\n",
            "    <object>\n",
            "        <name>person</name>\n",
            "        <pose>Left</pose>\n",
            "        <truncated>1</truncated>\n",
            "        <difficult>0</difficult>\n",
            "        <bndbox>\n",
            "            <xmin>8</xmin>\n",
            "            <ymin>12</ymin>\n",
            "            <xmax>352</xmax>\n",
            "            <ymax>498</ymax>\n",
            "        </bndbox>\n",
            "    </object>\n",
            "</annotation>\n",
            "y_p(t)=\\mathbf{w}^T\\mathbf{x}(t)+b\n",
            "E(\\mathbf{w}, b)=\\sum_{t=0}^{M}\\left [ {y_{train}(t)-y_p(t)} \\right ]^2=\\sum_{t=0}^{M}\\left [ {y_{train}(t)-\\left \\{\\mathbf{w}^T\\mathbf{x}(t)+b  \\right \\}} \\right ]^2\n",
            "# import関連\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib\n",
            "import matplotlib.pylab as plt\n",
            "import seaborn as sns# dataフォルダの場所を各自指定してください\n",
            "data_dir = \"./\"\n",
            "data = pd.read_csv(data_dir + \"USDJPY_1997_2017.csv\") # FXデータの読み込み（データは同じリポジトリのdataフォルダに入っています）\n",
            "data.head() # データの概要を見てみます\n",
            "# pandasのDataFrameのままでは、扱いにくい+実行速度が遅いので、numpyに変換して処理します\n",
            "data2 = np.array(data)\n",
            "# 説明変数となる行列Xを作成します\n",
            "day_ago = 25 # 何日前までのデータを使用するのかを設定\n",
            "num_sihyou = 1 # 終値# Xの確認です\n",
            "data_show = pd.DataFrame(X)\n",
            "data_show\n",
            "# 被説明変数となる Y = pre_day後の終値-当日終値 を作成します\n",
            "Y = np.zeros(len(data2))# 【重要】X, Yを正規化します\n",
            "original_X = np.copy(X) # コピーするときは、そのままイコールではダメ\n",
            "tmp_mean = np.zeros(len(X))# XとYを学習データとテストデータ(2017年～)に分ける\n",
            "X_train = X[200:5193,:] # 次のプログラムで200日平均を使うので、それ以降を学習データに使用します\n",
            "Y_train = Y[200:5193] # 学習データを使用して、線形回帰モデルを作成します\n",
            "from sklearn import linear_model # scikit-learnライブラリの関数を使用します\n",
            "linear_reg_model = linear_model.LinearRegression()# 2017年のデータで予想し、グラフで予測具合を見る# 2017年の予測結果の合計を計算ーーーーーーーーー\n",
            "# 前々日終値に比べて前日終値が高い場合は、買いとする\n",
            "sum_2017 = 0# 5日移動平均を追加します\n",
            "data2 = np.c_[data2, np.zeros((len(data2),1))] # 列の追加\n",
            "ave_day = 5\n",
            "for i in range(ave_day, len(data2)):\n",
            "    tmp =data2[i-ave_day+1:i+1,4].astype(np.float) # pythonは0番目からindexが始まります\n",
            "    data2[i,5] = np.mean(tmp)\n",
            "x = [10, 20, 30, 40, 50]\n",
            "i=4\n",
            "print(x[0:i])\n",
            "print(x[i])\n",
            "# 25日移動平均を追加します\n",
            "data2 = np.c_[data2, np.zeros((len(data2),1))]\n",
            "ave_day = 25\n",
            "for i in range(ave_day, len(data2)):\n",
            "    tmp =data2[i-ave_day+1:i+1,4].astype(np.float)\n",
            "    data2[i,6] = np.mean(tmp)# 一目均衡表を追加します (9,26, 52) \n",
            "para1 =9\n",
            "para2 = 26\n",
            "para3 = 52# 25日ボリンジャーバンド（±1, 2シグマ）を追加します\n",
            "parab = 25\n",
            "data2 = np.c_[data2, np.zeros((len(data2),4))] # 列の追加\n",
            "for i in range(parab, len(data2)):\n",
            "    tmp = data2[i-parab+1:i+1,4].astype(np.float)\n",
            "    data2[i,13] = np.mean(tmp) + 1.0* np.std(tmp) \n",
            "    data2[i,14] = np.mean(tmp) - 1.0* np.std(tmp) \n",
            "    data2[i,15] = np.mean(tmp) + 2.0* np.std(tmp) \n",
            "    data2[i,16] = np.mean(tmp) - 2.0* np.std(tmp) \n",
            "# 説明変数となる行列Xを作成します\n",
            "day_ago = 25 # 何日前までのデータを使用するのかを設定\n",
            "num_sihyou = 1 + 4 + 4 +4 # 終値1本、MVave4本、itimoku4本、ボリンジャー4本# 2017年のデータで予想し、グラフで予測具合を見る# 2017年の予測結果の合計を計算ーーーーーーーーー\n",
            "# 前々日終値に比べて前日終値が高い場合は、買いとする\n",
            "sum_2017 = 0加法性: f(x + y) = f(x) + f(y)\n",
            "斉次性: f(a * x) = a * f(x)\n",
            "\n",
            "\\vec{x} = ({x_1, x_2, x_3}),\\:\\vec{w} = ({w_1, w_2, w_3})\n",
            "\\vec{w}・\\vec{x} = (w_1x_1+w_2x_2+w_3x_3)x = np.random.randn(3)\n",
            "w = np.random.randn(3)\n",
            "np.dot(w,x)\n",
            "\n",
            "X=\\left(\\begin{matrix}\n",
            "x_{11} & x_{12} & x_{13} \\\\\n",
            "x_{21} & x_{22} & x_{32}\n",
            "\\end{matrix}\\right),\\:\n",
            "W=\\left(\\begin{matrix}\n",
            "w_{11} & w_{12} \\\\\n",
            "w_{21} & w_{22} \\\\\n",
            "w_{31} & w_{32}\n",
            "\\end{matrix}\\right)\n",
            "WX = \\left(\\begin{matrix}\n",
            "w_{11}x_{11}+w_{21}x_{12}+w_{31}x_{13} & w_{12}x_{11}+w_{22}x_{12}+w_{32}x_{13} \\\\\n",
            "w_{11}x_{21}+w_{21}x_{22}+w_{31}x_{23} & w_{12}x_{21}+w_{22}x_{22}+w_{32}x_{23}\n",
            "\\end{matrix}\\right)X = np.random.randn(2,3)\n",
            "W = np.random.randn(3,2)\n",
            "np.dot(X,W)\n",
            "X = np.random.randn(3,2)\n",
            "W = np.random.randn(3,2)\n",
            "np.dot(X,W) # エラー\n",
            "X = np.random.randn(1,4)\n",
            "W = np.random.randn(4,2)\n",
            "np.dot(X, W) # 計算可能\n",
            "f(x)=x+4\\:\\:\\:---->\\:\\:f'(x)=1\\\\\n",
            "f(x)=1/x\\:\\:\\:---->\\:\\:f'(x)=-1/x^2\\\\\n",
            "f(x)=4x\\:\\:\\:---->\\:\\:f'(x)=4\\\\\n",
            "f(x)=\\exp(x)\\:\\:---->\\:\\:f'(x)=\\exp(x)\n",
            "\n",
            "one2one.py\n",
            "x = np.random.randn(1)\n",
            "w = np.random.randn(1)\n",
            "w*x\n",
            "\n",
            "\n",
            "many2one.py\n",
            "x = np.random.randn(1,3)\n",
            "w = np.random.randn(3,1)\n",
            "np.dot(x,w)\n",
            "\n",
            "\n",
            "one2many.py\n",
            "x = np.random.randn(1,1)\n",
            "w = np.random.randn(1,3)\n",
            "np.dot(x,w)\n",
            "\n",
            "X=\\begin{matrix} x_1 & x_2 & x_3 \\end{matrix},\n",
            "W=\\begin{matrix} w_{11} & w_{12} & w_{13} \\\\\n",
            "w_{21} & w_{22} & w_{23} \\\\\n",
            "w_{31} & w_{32} & w_{33} \\end{matrix},\n",
            "O=\\begin{matrix} o_1 & o_2 & o_3\\end{matrix}\n",
            "\n",
            "many2many.py\n",
            "X=np.random.randn(1,3)\n",
            "W=np.random.randn(3,3)\n",
            "np.dot(X,W)\n",
            "\n",
            "\n",
            "multi_layer.py\n",
            "X=np.random.randn(1,3)\n",
            "layer1_W=np.random.randn(3,2)\n",
            "layer2_W=np.random.randn(2,1)\n",
            "layer1_O=np.dot(X,layer1_W)\n",
            "layer2_O=np.dot(layer1_O, layer2_W)\n",
            "\n",
            "w_i^{new}=w_i-lr\\frac{\\partial{f(x)}}{\\partial{w_i}}\n",
            "o = w_1x_1+w_2x_2+w_3x_3,\\:g(x)=\\frac{1}{1+\\exp(-x)},\\:o'=g(o)=\\frac{1}{1+\\exp(-o)}=\\frac{1}{1+\\exp(-(w_1x_1+w_2x_2+w_3x_3))}\n",
            "\n",
            "o_{1} = w_{11}x_1+w_{21}x_2+w_{31}x_3,\\:o_{2} = w_{12}x_1+w_{22}x_2+w_{32}x_3,\\\\o'=\\frac{1}{1+\\exp(-(w'_1\\frac{1}{1+\\exp(-(w_{11}x_1+w_{21}x_2+w_{31}x_3)}+w'_2\\frac{1}{1+\\exp(-(w_{12}x_1+w_{22}x_2+w_{32}x_3)})}\n",
            "\n",
            "\\frac{\\partial{f(x)}}{\\partial{w}}=\\frac{\\partial{f(x)}}{\\partial{o}}\\frac{\\partial{o}}{\\partial{w}}\n",
            "o=w+z,\\:f(x)=(w+z)y=oy,\\\\\n",
            "\\frac{\\partial{o}}{\\partial{w}}=1,\\:\\frac{\\partial{f(x)}}{\\partial{o}}=y,\\:\\frac{\\partial{f(x)}}{\\partial{w}}=\\frac{\\partial{f(x)}}{\\partial{o}}\\frac{\\partial{o}}{\\partial{w}}=y\n",
            "f(x)=\\frac{1}{1+\\exp(-x)},\\\\\n",
            "f'(x)=f(x)(1-f(x))=\\frac{1}{1+\\exp(-x)}(1-\\frac{1}{1+\\exp(-x)})\n",
            "\\frac{\\partial{D}}{\\partial{C}}=(\\frac{1}{C})'=\\frac{-1}{C^2}=\\frac{-1}{(1.37)^2}=-0.53\\\\\n",
            "\\frac{\\partial{D}}{\\partial{B}}=\\frac{\\partial{D}}{\\partial{C}}\\frac{\\partial{C}}{\\partial{B}}=(-0.53)(1+B)'=(-0.53)(1)=-0.53\\\\\n",
            "\\frac{\\partial{D}}{\\partial{A}}=\\frac{\\partial{D}}{\\partial{B}}\\frac{\\partial{B}}{\\partial{A}}=(-0.53)(\\exp(A))'=(-0.53)(\\exp(A))=(-0.53)(0.37)=0.2\\\\\n",
            "\\frac{\\partial{D}}{\\partial{x}}=\\frac{\\partial{D}}{\\partial{A}}\\frac{\\partial{A}}{\\partial{x}}=(-0.2)(-x)'=(-0.2)(-1)=0.2\n",
            "\n",
            "nn.py\n",
            "class nn():\n",
            "  def __init__(self, n_i, n_o, lr):\n",
            "    self.weight = np.random.randn(n_o, n_i)\n",
            "    self.input = None\n",
            "    self.grad = np.zeros((n_i, n_o))\n",
            "    self.lr = lr\n",
            "test.py\n",
            "from nn import nn\n",
            "fc = nn(10, 2, 0.1)\n",
            "x = np.random.randn(10, 1)\n",
            "fc.forward(x)\n",
            "grad = np.random.randn(1,2) # 前から来る微分値\n",
            "fc.backward(grad)\n",
            "fc.update()\n",
            "\n",
            "\n",
            "sigmoid.py\n",
            "class sigmoid():\n",
            "  def __init__(self):\n",
            "    self.output = None\n",
            "multi_layer_perceptron.py\n",
            "from nn import nn\n",
            "from sigmoid import sigmoid\n",
            "fc1 = nn(10, 5, 0.1)\n",
            "sig1 = sigmoid()\n",
            "fc2 = nn(5, 2, 0.1)\n",
            "sig2 = sigmoid()\n",
            "x = np.random.randn(10, 1)\n",
            "sig2.forward(fc2.forward(sig1.forward(fc1.forward(x))))\n",
            "grad = np.random.randn(1,2)\n",
            "fc1.backward(sig1.backward(fc2.backward(sig2.backward(grad))))\n",
            "fc1.update()\n",
            "fc2.update()\n",
            "\n",
            "f(x)=\\frac{1}{n}\\sum_{i=1}^n(y_i-x_i)^2\\\\\n",
            "\\frac{\\partial{f(x)}}{\\partial{x_i}} = \\frac{2}{n}(y_i-x_i)\n",
            "\n",
            "MSE.py\n",
            "class MSE():\n",
            "  def forward(self, x, y):\n",
            "    return np.square(y.reshape(-1)-x.reshape(-1)).mean()\n",
            "train.py\n",
            "from nn import nn\n",
            "from sigmoid import sigmoid\n",
            "from MSE import MSE\n",
            "fc1 = nn(10, 5, 0.1)\n",
            "sig1 = sigmoid()\n",
            "fc2 = nn(5, 2, 0.1)\n",
            "sig2 = sigmoid()\n",
            "mse = MSE()\n",
            "x = np.random.randn(10) # 学習データ生成\n",
            "t = np.random.randn(2) # 教師データ生成\n",
            "for i in range(100):\n",
            "  out = sig2.forward(fc2.forward(sig1.forward(fc1.forward(x))))\n",
            "  loss = mse.forward(out, t)\n",
            "  print(loss)\n",
            "  grad = mse.backward(out, t)\n",
            "  fc1.backward(sig1.backward(fc2.backward(sig2.backward(grad))))\n",
            "  fc1.update()\n",
            "  fc2.update()\n",
            "\n",
            "%matplotlib inline\n",
            "import requests\n",
            "import json, sys\n",
            "from collections import defaultdict\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "plt.style.use('ggplot')# ------------------- 記事毎ストック数取得 -----------------------#\n",
            "def get_stockers(_id):\n",
            "    global headers\n",
            "    url = 'https://qiita.com/api/v2/items/{}/stockers'.format(_id)\n",
            "    cnt = 0\n",
            "    _sum = 0\n",
            "    while True:\n",
            "        cnt += 1\n",
            "        payload = {'page': cnt, 'per_page': 20}\n",
            "        res = requests.get(url, params=payload, headers=headers)\n",
            "        data = res.json()\n",
            "        for d in data:\n",
            "            users[d['id']] += 1\n",
            "        num = len(data)\n",
            "        if num == 0:\n",
            "            break\n",
            "        _sum += num# ------------------- 記事情報取得 -----------------------#\n",
            "url = 'https://qiita.com/api/v2/authenticated_user/items'# ------------------- データの整形 -----------------------#\n",
            "for i, d in enumerate(data_list):\n",
            "    sys.stdout.write(\"{}, \".format(i))sum_of_stocks = np.sum([r['stock'] for r in res]).astype(np.float32)# Tag集計\n",
            "tag_cnt = defaultdict(int)\n",
            "for r in res:\n",
            "    for t in r['tags']:\n",
            "        tag_cnt[t['name']] += 1# User集計\n",
            "id_list = []\n",
            "cnt_list = []\n",
            "for _id, cnt in users.items():\n",
            "    id_list.append((_id, cnt))# Under Samplingの関数（X: num:ターゲット件数 label:少数派のラベル）from imblearn.over_sampling import SMOTEfrom sklearn import metricsmodel = VGG16(weights='imagenet', include_top=False)\n",
            "anchor_box_aspect_ratios = [\n",
            "    (1. / math.sqrt(2), 2. / math.sqrt(2)),  # 1:2\n",
            "    (1., 1.),  # 1:1\n",
            "    (2. / math.sqrt(2), 1. / math.sqrt(2))  # 2:1\n",
            "]\n",
            "import pandas as pd\n",
            "from sklearn.model_selection import train_test_splitimport pandas as pd\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as snsprint(\"歪度 :\" , data[\"bookmark\"].skew())\n",
            "print(\"尖度 :\" , data[\"bookmark\"].kurtosis())\n",
            "print(data[\"bookmark\"].describe())print(data[\"bookmark\"].describe(percentiles=[0.1, 0.2,0.3, 0.4,0.5, 0.6,0.7,0.71, 0.8,0.9]))import pandas as pd\n",
            "import numpy as npxgbc = XGBClassifier()\n",
            "result = cross_validate(xgbc, x, y, cv=5, return_estimator=True)train[\"biggenre\"] = train[\"biggenre\"].replace({\n",
            "                                        1:\"恋愛\",\n",
            "                                        2:\"ファンタジー\",\n",
            "                                        3:\"文芸\",\n",
            "                                        4:\"SF\",\n",
            "                                        99:\"その他\",\n",
            "                                        98:\"ノンジャンル\"\n",
            "                                    })\n",
            "sns.countplot(x=\"biggenre\", hue=\"ten_or_more_bookmarks\", data=train)\n",
            "sns.countplot(x=\"genre\", hue=\"ten_or_more_bookmarks\", data=train)\n",
            "sns.countplot(x=\"isr15\", hue=\"ten_or_more_bookmarks\", data=train)\n",
            "x_axis_names = [\n",
            "    \"story_character_count\",\n",
            "    \"title_character_count\",\n",
            "    \"vocabulary_rate\",\n",
            "    \"blank_line_rate\"\n",
            "]test = pd.read_csv(\"test.csv\", index_col=0)\n",
            "test  = test.loc[:,[\"biggenre\",\"genre\",\"isr15\",\"title_character_count\",\"story_character_count\",\"vocabulary_rate\",\"blank_line_rate\",\"bookmark\"]]brew install python\n",
            "sudo easy_install pip\n",
            "sudo pip install --upgrade virtualenv\n",
            "virtualenv --system-site-packages ./tensorflow\n",
            "cd tensorflow\n",
            "source bin/activate\n",
            "sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl\n",
            "\n",
            "mnist_for_ml_beginners.py\n",
            "# -*- coding: utf-8 -*-$ python mnist_for_ml_beginners.py\n",
            "開始時刻: 1449994007.63\n",
            "--- MNISTデータの読み込み開始 ---\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "--- MNISTデータの読み込み完了 ---\n",
            "can't determine number of CPU cores: assuming 4\n",
            "I tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 4\n",
            "can't determine number of CPU cores: assuming 4\n",
            "I tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 4\n",
            "--- 訓練開始 ---\n",
            "--- 訓練終了 ---\n",
            "精度\n",
            "0.9212\n",
            "終了時刻: 1449994010.09\n",
            "かかった時間: 2.45791196823\n",
            "deactivate\n",
            "\n",
            "有価証券報告書の情報取得(1st_step)\n",
            "import requests\n",
            "import xml.etree.ElementTree as ET\n",
            "import json\n",
            "import os\n",
            "import io\n",
            "import re\n",
            "import multiprocessing\n",
            "from collections import defaultdict\n",
            "from datetime import datetime as dt\n",
            "XBRLファイルのダウンロード(2nd_step)\n",
            "import requests\n",
            "import json\n",
            "import os\n",
            "import io\n",
            "import shutil\n",
            "import multiprocessing\n",
            "from collections import defaultdict\n",
            "from zipfile import ZipFile(X, y) -> 変換器 -> 変換器 -> ... -> 推定器 -> 予測値\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.linear_model import ElasticNet\n",
            "from sklearn.metrics import mean_squared_error\n",
            "from sklearn.datasets import make_regression  # 疑似データ作成用\n",
            "from sklearn.model_selection import train_test_split  # 疑似データ作成用from sklearn.model_selection import GridSearchCV変換器 -> 変換器 -> ... -> 学習器\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.linear_model import ElasticNet\n",
            "from sklearn.model_selection import GridSearchCV\n",
            "from sklearn.metrics import mean_squared_error\n",
            "from sklearn.decomposition import PCA\n",
            "from sklearn.pipeline import Pipeliney = np.log(y)\n",
            "y=y**2\n",
            "y = f(y)\n",
            "...#! /usr/bin/env python3\n",
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "Created on Sun Jun 17 01:19:29 2018\n",
            "sklearn supplemental classesfrom sklearn.preprocessing import FunctionTransformer# The script MUST contain a function named azureml_main\n",
            "# which is the entry point for this module.\n",
            "#\n",
            "# The entry point function can contain up to two input arguments:\n",
            "#   Param<dataframe1>: a pandas.DataFrame\n",
            "#   Param<dataframe2>: a pandas.DataFrame\n",
            "def azureml_main(dataframe1 = None, dataframe2 = None):from chainer.datasets import mnistDownloading from http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz...\n",
            "Downloading from http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz...\n",
            "Downloading from http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz...\n",
            "Downloading from http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz...\n",
            "label: 5\n",
            "from chainer import iteratorsimport chainer.links as L\n",
            "import chainer.functions as F\n",
            "import numpy\n",
            "numpy.random.seed(0)\n",
            "import chainer\n",
            "if chainer.cuda.available:\n",
            "    chainer.cuda.cupy.random.seed(0)\n",
            "import chainer\n",
            "import chainer.links as L\n",
            "import chainer.functions as Fprint('1つ目の全結合相のバイアスパラメータの形は、', model.l1.b.shape)\n",
            "print('初期化直後のその値は、', model.l1.b.data)\n",
            "1つ目の全結合相のバイアスパラメータの形は、 (100,)\n",
            "初期化直後のその値は、 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            "AttributeError: 'Linear' object has no attribute 'W'\n",
            "from chainer import optimizersimport numpy as np\n",
            "from chainer.dataset import concat_examples\n",
            "from chainer.cuda import to_cpuepoch:01 train_loss:0.7828 val_loss:0.8276 val_accuracy:0.8167\n",
            "epoch:02 train_loss:0.3672 val_loss:0.4564 val_accuracy:0.8826\n",
            "epoch:03 train_loss:0.3069 val_loss:0.3702 val_accuracy:0.8976\n",
            "epoch:04 train_loss:0.3333 val_loss:0.3307 val_accuracy:0.9078\n",
            "epoch:05 train_loss:0.3308 val_loss:0.3079 val_accuracy:0.9129\n",
            "epoch:06 train_loss:0.3210 val_loss:0.2909 val_accuracy:0.9162\n",
            "epoch:07 train_loss:0.2977 val_loss:0.2781 val_accuracy:0.9213\n",
            "epoch:08 train_loss:0.2760 val_loss:0.2693 val_accuracy:0.9232\n",
            "epoch:09 train_loss:0.1762 val_loss:0.2566 val_accuracy:0.9263\n",
            "epoch:10 train_loss:0.2444 val_loss:0.2479 val_accuracy:0.9284\n",
            "from chainer import serializers-rw-rw-r-- 1 ubuntu ubuntu 333853 Mar 29 16:51 my_mnist.model\n",
            "# まず同じモデルのオブジェクトを作る\n",
            "infer_model = MLP()label: 7\n",
            "from chainer.cuda import to_gpu(784,) -> (1, 784)\n",
            "predicted label: 7\n",
            "from chainer.datasets import mnistfrom chainer import iteratorsimport chainer\n",
            "import chainer.links as L\n",
            "import chainer.functions as Ffrom chainer import optimizers\n",
            "from chainer import training# TrainerにUpdaterを渡す\n",
            "trainer = training.Trainer(updater, (max_epoch, 'epoch'),\n",
            "                           out='mnist_result')\n",
            "from chainer.training import extensionstrainer.run()\n",
            "epoch       main/loss   main/accuracy  validation/main/loss  validation/main/accuracy  elapsed_time\n",
            "1           1.6035      0.61194        0.797731              0.833564                  2.98546       \n",
            "2           0.595589    0.856793       0.452023              0.88123                   5.74528       \n",
            "3           0.4241      0.885944       0.368583              0.897943                  8.34872       \n",
            "4           0.367762    0.897152       0.33103               0.905756                  11.4449       \n",
            "5           0.336136    0.904967       0.309321              0.912282                  14.2671       \n",
            "6           0.314134    0.910464       0.291451              0.914557                  17.0762       \n",
            "7           0.297581    0.914879       0.276472              0.920985                  19.8298       \n",
            "8           0.283512    0.918753       0.265166              0.923655                  23.2033       \n",
            "9           0.271917    0.922125       0.254976              0.926523                  26.1452       \n",
            "10          0.260754    0.925123       0.247672              0.927413                  29.3136       \n",
            "from IPython.display import Image\n",
            "Image(filename='mnist_result/loss.png')\n",
            "Image(filename='mnist_result/accuracy.png')\n",
            "%%bash\n",
            "dot -Tpng mnist_result/cg.dot -o mnist_result/cg.png\n",
            "Image(filename='mnist_result/cg.png')\n",
            "import numpy as np\n",
            "from chainer import serializers\n",
            "from chainer.cuda import to_gpu\n",
            "from chainer.cuda import to_cpulabel: 7\n",
            "predicted_label: 7\n",
            "import chainer\n",
            "import chainer.functions as F\n",
            "import chainer.links as Lfrom chainer.datasets import cifar\n",
            "from chainer import iterators\n",
            "from chainer import optimizers\n",
            "from chainer import training\n",
            "from chainer.training import extensionsepoch       main/loss   main/accuracy  validation/main/loss  validation/main/accuracy  elapsed_time\n",
            "1           1.53309     0.444293       1.29774               0.52707                   5.2449        \n",
            "2           1.21681     0.56264        1.18395               0.573746                  10.6833       \n",
            "3           1.06828     0.617358       1.10173               0.609773                  16.0644       \n",
            "4           0.941792    0.662132       1.0695                0.622611                  21.2535       \n",
            "5           0.832165    0.703345       1.0665                0.624104                  26.4523       \n",
            "6           0.729036    0.740257       1.0577                0.64371                   31.6299       \n",
            "7           0.630143    0.774208       1.07577               0.63953                   36.798        \n",
            "8           0.520787    0.815541       1.15054               0.639431                  42.1951       \n",
            "9           0.429535    0.849085       1.23832               0.6459                    47.3631       \n",
            "10          0.334665    0.882842       1.3528                0.633061                  52.5524       \n",
            "11          0.266092    0.90549        1.44239               0.635251                  57.7396       \n",
            "12          0.198057    0.932638       1.6249                0.6249                    62.9918       \n",
            "13          0.161151    0.944613       1.76964               0.637241                  68.2177       \n",
            "14          0.138705    0.952145       1.98031               0.619725                  73.4226       \n",
            "15          0.122419    0.957807       2.03002               0.623806                  78.6411       \n",
            "16          0.109989    0.962148       2.08948               0.62281                   84.3362       \n",
            "17          0.105851    0.963675       2.31344               0.617237                  89.5656       \n",
            "18          0.0984753   0.966289       2.39499               0.624801                  95.1304       \n",
            "19          0.0836834   0.970971       2.38215               0.626791                  100.36        \n",
            "20          0.0913404   0.96925        2.46774               0.61873                   105.684       \n",
            "Image(filename='MyModel_cifar10_result/loss.png')\n",
            "Image(filename='MyModel_cifar10_result/accuracy.png')\n",
            "%matplotlib inline\n",
            "import matplotlib.pyplot as pltpredicted_label: dog\n",
            "answer: airplane\n",
            "predicted_label: truck\n",
            "answer: truck\n",
            "predicted_label: bird\n",
            "answer: dog\n",
            "predicted_label: horse\n",
            "answer: horse\n",
            "predicted_label: truck\n",
            "answer: truck\n",
            "class ConvBlock(chainer.Chain):class DeepCNN(chainer.ChainList):model = train(DeepCNN(10), max_epoch=100)\n",
            "epoch       main/loss   main/accuracy  validation/main/loss  validation/main/accuracy  elapsed_time\n",
            "1           2.05147     0.242887       1.71868               0.340764                  14.8099       \n",
            "2           1.5242      0.423816       1.398                 0.48537                   29.12         \n",
            "3           1.24906     0.549096       1.12884               0.6042                    43.4423       \n",
            "4           0.998223    0.652649       0.937086              0.688495                  58.291        \n",
            "5           0.833486    0.720009       0.796678              0.73756                   73.4144       \n",
            ".\n",
            ".\n",
            ".\n",
            "95          0.0454193   0.987616       0.815549              0.863555                  1411.86       \n",
            "96          0.0376641   0.990057       0.878458              0.873109                  1426.85       \n",
            "97          0.0403836   0.98953        0.849209              0.86465                   1441.19       \n",
            "98          0.0369386   0.989677       0.919462              0.873905                  1456.04       \n",
            "99          0.0361681   0.990677       0.88796               0.86873                   1470.46       \n",
            "100         0.0383634   0.988676       0.92344               0.869128                  1484.91     \n",
            "Image(filename='DeepCNN_cifar10_result/loss.png')\n",
            "Image(filename='DeepCNN_cifar10_result/accuracy.png')\n",
            "import numpy as np\n",
            "from chainer import dataset\n",
            "from chainer.datasets import cifarmodel = train(DeepCNN(10), max_epoch=100, train_dataset=CIFAR10Augmented(), test_dataset=CIFAR10Augmented(False))\n",
            "epoch       main/loss   main/accuracy  validation/main/loss  validation/main/accuracy  elapsed_time\n",
            "1           2.023       0.248981       1.75221               0.322353                  18.4387       \n",
            "2           1.51639     0.43716        1.36708               0.512639                  36.482        \n",
            "3           1.25354     0.554177       1.17713               0.586087                  54.6892       \n",
            "4           1.05922     0.637804       0.971438              0.665904                  72.9602       \n",
            "5           0.895339    0.701886       0.918005              0.706409                  91.4061       \n",
            ".\n",
            ".\n",
            ".  \n",
            "95          0.0877855   0.973171       0.726305              0.89162                   1757.87       \n",
            "96          0.0780378   0.976012       0.943201              0.890725                  1776.41       \n",
            "97          0.086231    0.973765       0.57783               0.890227                  1794.99       \n",
            "98          0.0869593   0.973512       1.65576               0.878981                  1813.52       \n",
            "99          0.0870466   0.972931       0.718033              0.891421                  1831.99       \n",
            "100         0.079011    0.975332       0.754114              0.892815                  1850.46     \n",
            "Image(filename='DeepCNN_cifar10augmented_result/loss.png')\n",
            "Image(filename='DeepCNN_cifar10augmented_result/accuracy.png')\n",
            "def update_state(xk, sk, wx, wRec):    for k in range(0, X.shape[1]):\n",
            "        # S[k] = S[k-1] * wRec + X[k] * wx\n",
            "        S[:,k+1] = update_state(X[:,k], S[:,k], wx, wRec)n = T.iscalar('n')\n",
            "result, updates = theano.scan(fn=lambda prior, nonseq: prior * 2,\n",
            "                              sequences=None,\n",
            "                              outputs_info=a, # 一つ前のLoopにおける値を参照 --> prior\n",
            "                              non_sequences=a, # シーケンスでない値 --> nonseq\n",
            "                              n_steps=n)>>> array([10, 20, 40], dtype=int32)\n",
            "v = T.matrix('v')\n",
            "s0 = T.vector('s0')\n",
            "result, updates = theano.scan(fn=lambda seq, prior: seq + prior * 2,\n",
            "                                             sequences=v,\n",
            "                                             outputs_info=s0,\n",
            "                                             non_sequences=None)\n",
            "myfun2 = theano.function(inputs=[v, s0], outputs=result, updates=updates)>>> array([[ 2.,  1.],\n",
            "       [ 4.,  3.],\n",
            "       [ 9.,  7.]], dtype=float32)\n",
            "class simpleRNN(object):\n",
            "    #   members:  slen  : state length\n",
            "    #             w_x   : weight of input-->hidden layer\n",
            "    #             w_rec : weight of recurrnce \n",
            "    def __init__(self, slen, nx, nrec):\n",
            "        self.len = slen\n",
            "        self.w_x = theano.shared(\n",
            "            np.asarray(np.random.uniform(-.1, .1, (nx)),\n",
            "            dtype=theano.config.floatX)\n",
            "        )\n",
            "        self.w_rec = theano.shared(\n",
            "            np.asarray(np.random.uniform(-.1, .1, (nrec)),\n",
            "            dtype=theano.config.floatX)\n",
            "        )    np.random.seed(seed=1)  Data Set Shape\n",
            "                  feature1   feature2   feature3  ...\n",
            "     sample1:        -          -          -\n",
            "     sample2:        -          -          -\n",
            "     sample3:        -          -          -\n",
            "       .\n",
            "       .\n",
            "（以下のようにグループ化することで，theano.scan() の動作と整合性をとる．）\n",
            "  Data Set Shape (updated)\n",
            "               [  time1[sample1,  time2[sample1,  time3[sample1 ...    ]\n",
            "                        sample2,        sample2,        sample2,\n",
            "                        sample3,        sample3,        sample3,\n",
            "                         ...    ]         ...   ]         ...    ]\n",
            "    # Tensor Declaration\n",
            "    x_t = T.matrix('x_t')\n",
            "    x = T.matrix('x')\n",
            "    y_ = T.vector('y_')\n",
            "    s0 = T.vector('s0')\n",
            "    y_hypo = T.vector('y_hypo')# Train Net Model\n",
            "    params = [net.w_x, net.w_rec]\n",
            "    optimizer = GradientDescentOptimizer(params, learning_rate=1.e-5)\n",
            "    train_op = optimizer.minimize(loss)Initial weights: wx =   0.0900, wRec =   0.0113\n",
            "epoch[  100] : cost =529.6915\n",
            "epoch[  200] : cost =504.5684\n",
            "epoch[  300] : cost =475.3019\n",
            "epoch[  400] : cost =435.9507\n",
            "epoch[  500] : cost =362.6525\n",
            "epoch[  600] : cost =  0.2677\n",
            "epoch[  700] : cost =  0.1585\n",
            "epoch[  800] : cost =  0.1484\n",
            "epoch[  900] : cost =  0.1389\n",
            "epoch[ 1000] : cost =  0.1300\n",
            "epoch[ 1100] : cost =  0.1216\n",
            "epoch[ 1200] : cost =  0.1138\n",
            "epoch[ 1300] : cost =  0.1064\n",
            "epoch[ 1400] : cost =  0.0995\n",
            "epoch[ 1500] : cost =  0.0930\n",
            "epoch[ 1600] : cost =  0.0870\n",
            "epoch[ 1700] : cost =  0.0813\n",
            "epoch[ 1800] : cost =  0.0760\n",
            "epoch[ 1900] : cost =  0.0710\n",
            "epoch[ 2000] : cost =  0.0663\n",
            "Final weights : wx =   1.0597, wRec =   0.9863Initial weights: wx =   0.0900, wRec =   0.0113\n",
            "epoch[  100] : cost =  5.7880\n",
            "epoch[  200] : cost =  0.3313\n",
            "epoch[  300] : cost =  0.0181\n",
            "epoch[  400] : cost =  0.0072\n",
            "epoch[  500] : cost =  0.0068\n",
            "epoch[  600] : cost =  0.0068\n",
            "epoch[  700] : cost =  0.0068\n",
            "epoch[  800] : cost =  0.0068\n",
            "epoch[  900] : cost =  0.0068\n",
            "epoch[ 1000] : cost =  0.0068\n",
            "epoch[ 1100] : cost =  0.0068\n",
            "epoch[ 1200] : cost =  0.0068\n",
            "epoch[ 1300] : cost =  0.0068\n",
            "epoch[ 1400] : cost =  0.0068\n",
            "epoch[ 1500] : cost =  0.0068\n",
            "epoch[ 1600] : cost =  0.0068\n",
            "epoch[ 1700] : cost =  0.0068\n",
            "epoch[ 1800] : cost =  0.0068\n",
            "epoch[ 1900] : cost =  0.0068\n",
            "epoch[ 2000] : cost =  0.0068\n",
            "Final weights : wx =   0.9995, wRec =   0.9993{\n",
            "  デ: 2, ン: 1, ネ: 1,\n",
            "  デデ: 1, デン: 1, ンネ: 1\n",
            "}\n",
            "# -*- coding: utf-8 -*-# -*- coding: utf-8 -*-\n",
            "for window in windows\n",
            "    patch = get_patch(image, window)\n",
            "    results = CNNdetector(patch)\n",
            "\n",
            "ROIs = region_proposal(image)\n",
            "for ROI in ROIs\n",
            "    patch = get_patch(image, ROI)\n",
            "    results = CNNdetector(patch)\n",
            "\n",
            "ROIs = region_proposal_by_selective_search(image)\n",
            "Fmaps = CNN(image)\n",
            "for ROI in ROIs\n",
            "    patch = ROI_pooling(Fmaps, ROI)\n",
            "    results = FCdetector(patch)\n",
            "\n",
            "Fmaps = CNN(image)\n",
            "ROIs = regionproposalCNN(Fmaps)\n",
            "for ROI in ROIs\n",
            "    patch = ROIpooling(Fmaps, ROI)\n",
            "    results = FCdetector(patch)\n",
            "\n",
            "get_affair_dataset.py\n",
            "from sklearn.linear_model import LogisticRegression # ロジスティック回帰用\n",
            "from sklearn.cross_validation import train_test_split # クロスバリデーションのsplit用\n",
            "import statsmodels.api as sm\n",
            "describe_affair.py\n",
            "df.head()\n",
            "\n",
            "\n",
            "easy_display1.py\n",
            "# 年齢と不倫有無\n",
            "sns.countplot('age', data = df.sort('age'), hue = 'Had_Affair', palette='coolwarm')\n",
            "\n",
            "\n",
            "easy_display2.py\n",
            "# 結婚年数と不倫有無\n",
            "sns.countplot('yrs_married', data = df.sort('yrs_married'), hue = 'Had_Affair', palette='coolwarm')\n",
            "\n",
            "\n",
            "easy_display3.py\n",
            "# 子供の人数と不倫有無\n",
            "sns.countplot('children', data = df.sort('children'), hue = 'Had_Affair', palette='coolwarm')\n",
            "\n",
            "\n",
            "change_dummy_value.py\n",
            "# numpyのget_dummiesでdummy変数に変換。\n",
            "occ_dummies = pd.get_dummies(df.occupation)\n",
            "hus_occ_dummies = pd.get_dummies(df.occupation_husb)\n",
            "get_x.py\n",
            "# Xに、元のデータフレームから職業、旦那の職業、結婚有無を削除したものをセット。\n",
            "X = df.drop(['occupation', 'occupation_husb', 'Had_Affair'], axis =1) \n",
            "# 職業をダミー変数化したデータフレームを用意\n",
            "dummys = pd.concat([occ_dummies, hus_occ_dummies], axis =1)\n",
            "# 職業等を削除したデータフレームに、職業ダミー変数データフレームを結合\n",
            "X = pd.concat([X, dummys], axis=1)\n",
            "drop_nonavailable_value.py\n",
            "X = X.drop('hocc1', axis = 1)\n",
            "X = X.drop('hocc1', axis = 1)\n",
            "# affairsは、目的変数を作成するのに使っているので、これも説明変数から除く。\n",
            "X = X.drop('affairs', axis =1 )\n",
            "do_logistic_regression.py\n",
            "# 目的変数セット\n",
            "Y = df.Had_Affair\n",
            "Y = np.ravel(Y)    # np.ravelでYを1次元配列にする\n",
            "confirm_coefficient.py\n",
            "# インスタンスの.coef_[0]に、係数が入っている\n",
            "coeff_df = DataFrame([X.columns, log_model.coef_[0]]).T\n",
            "coeff_df\n",
            "\n",
            "\n",
            "do_logistic_regression_train_test.py\n",
            "# trainとtest用データ用意\n",
            "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)# 必要なライブラリの import\n",
            "from sklearn.datasets import load_iris\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.linear_model import LogisticRegressionTest set score: 0.868421052631579\n",
            "\n",
            "from sklearn.model_selection import cross_val_score\n",
            "logreg = LogisticRegression()\n",
            "# 交差検証\n",
            "scores = cross_val_score(logreg, iris.data, iris.target)\n",
            "# 各分割におけるスコア\n",
            "print('Cross-Validation scores: {}'.format(scores))\n",
            "# スコアの平均値\n",
            "import numpy as np\n",
            "print('Average score: {}'.format(np.mean(scores)))\n",
            "Cross-Validation scores: [ 0.96078431  0.92156863  0.95833333]\n",
            "Average score: 0.9468954248366014\n",
            "\n",
            "# 単純な方法\n",
            "kfold = KFold(n_splits=3)\n",
            "print('Cross-validation scores: \\n{}'.format(cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\n",
            "# 層化 k 分割交差検証\n",
            "stratifiedkfold = StratifiedKFold(n_splits=3)\n",
            "print('Cross-validation scores: \\n{}'.format(cross_val_score(logreg, iris.data, iris.target, cv=stratifiedkfold)))\n",
            "# 単純な方法\n",
            "Cross-validation scores:\n",
            "[ 0.  0.  0.]\n",
            "# 層化 k 分割交差検証\n",
            "Cross-validation scores:\n",
            "[ 0.96078431  0.92156863  0.95833333]\n",
            "print(iris.target)\n",
            "# 出力結果\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n",
            "from sklearn.svm import SVCBest score: 0.9736842105263158\n",
            "Best parameters: {'C': 100, 'gamma': 0.001}\n",
            "X_trainval, X_test, y_trainval, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
            "X_train, X_valid, y_train, y_valid = train_test_split(X_trainval, y_trainval, random_state=1)\n",
            "print('Size of trainings set: {}, validation set: {}, test set: {}'.format(X_train.shape, X_valid.shape, X_test.shape))\n",
            "Size of trainings set: (84, 4), validation set: (28, 4), test set: (38, 4)\n",
            "best_score = 0\n",
            "best_parameters = {}Best score on validation set: 0.9642857142857143\n",
            "Best parameters: {'C': 10, 'gamma': 0.001}\n",
            "Test set score with best parameters: 0.9210526315789473\n",
            "\n",
            "best_score = 0\n",
            "best_parameters  = {}Best score on validation set: 0.9726896292113683\n",
            "Best parameters: {'C': 100, 'gamma': 0.01}\n",
            "Test set score with best parameters: 0.9736842105263158\n",
            "from sklearn.model_selection import GridSearchCVprint('Test set score: {}'.format(grid_search.score(X_test, y_test)))\n",
            "print('Best parameters: {}'.format(grid_search.best_params_))\n",
            "print('Best cross-validation: {}'.format(grid_search.best_score_))\n",
            "Test set score: 0.9736842105263158\n",
            "Best parameters: {'C': 100, 'gamma': 0.01}\n",
            "Best cross-validation: 0.9732142857142857\n",
            "#include <tiny_dnn/tiny_dnn.h>\n",
            "network<sequential> net;auto net = make_mlp<relu>({100, 300, 10});\n",
            "network<sequential> net;W = tf.Variable(tf.zeros([784, 10]))\n",
            "b = tf.Variable(tf.zeros([10]))\n",
            "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
            "y = Dense(10, input_shape=(784,))(x)\n",
            "\n",
            "# This is a sample mean-reversion algorithm on Quantopian for you to test and adapt.    # Use the top 1% of stocks defined by average daily trading volume.\n",
            "    set_universe(universe.DollarVolumeUniverse(99, 100))\n",
            "    # Set execution cost assumptions. For live trading with Interactive Brokers \n",
            "    # we will assume a $1.00 minimum per trade fee, with a per share cost of $0.0075. \n",
            "    set_commission(commission.PerShare(cost=0.0075, min_trade_cost=1.00))\n",
            "    # Set market impact assumptions. We limit the simulation to \n",
            "    # trade up to 2.5% of the traded volume for any one minute,\n",
            "    # and  our price impact constant is 0.1. \n",
            "    set_slippage(slippage.VolumeShareSlippage(volume_limit=0.025, price_impact=0.10))\n",
            "    # Define the other variables\n",
            "    context.long_leverage = 0.5\n",
            "    context.short_leverage = -0.5\n",
            "    context.lower_percentile = 20\n",
            "    context.upper_percentile = 80\n",
            "    context.returns_lookback = 5\n",
            "    # Rebalance every Monday (or the first trading day if it's a holiday).\n",
            "    # At 11AM ET, which is 1 hour and 30 minutes after market open.\n",
            "    schedule_function(rebalance, \n",
            "                      date_rules.week_start(days_offset=0),\n",
            "                      time_rules.market_open(hours = 1, minutes = 30))  \n",
            "# The handle_data function is run every bar.    \n",
            "def handle_data(context,data):    \n",
            "    # Record and plot the leverage of our portfolio over time. \n",
            "    record(leverage = context.account.leverage)# This rebalancing is called according to our schedule_function settings.     \n",
            "def rebalance(context,data):\n",
            "    # Get the last N days of prices for every stock in our universe.\n",
            "    prices = history(context.returns_lookback, '1d', 'price')\n",
            "    # Calculate the past 5 days' returns for each security.\n",
            "    returns = (prices.iloc[-1] - prices.iloc[0]) / prices.iloc[0]\n",
            "    # Remove stocks with missing prices.\n",
            "    # Remove any stocks we ordered last time that still have open orders.\n",
            "    # Get the cutoff return percentiles for the long and short portfolios.\n",
            "    returns = returns.dropna()\n",
            "    open_orders = get_open_orders()\n",
            "    if open_orders:\n",
            "        eligible_secs = [sec for sec in data if sec not in open_orders]\n",
            "        returns = returns[eligible_secs]\n",
            "    # Lower percentile is the threshhold for the bottom 20%, upper percentile is for the top 20%.\n",
            "    lower, upper = np.percentile(returns, [context.lower_percentile,\n",
            "                                           context.upper_percentile])    # Set the allocations to even weights in each portfolio.\n",
            "    long_weight = context.long_leverage / len(long_secs)\n",
            "    short_weight = context.short_leverage / len(short_secs)\n",
            "    for security in data:# データ加工・処理・分析モジュール\n",
            "import numpy as np\n",
            "import numpy.random as random\n",
            "import scipy as sp\n",
            "import pandas as pd\n",
            "from pandas import Series, DataFrame\n",
            "# 学習用データとテストデータに分けるためのモジュール（正解率を出すため）\n",
            "from sklearn.model_selection import train_test_split\n",
            "# アヤメの花(学習するデータ)\n",
            "from sklearn.datasets import load_iris# 説明変数X(特徴量4つ×150)と目的変数Y(アヤメの種類×150)に分ける\n",
            "X = df.drop('target', axis=1)\n",
            "Y = df['target']# k-近傍法（k-NN）\n",
            "from sklearn.neighbors import KNeighborsClassifier# 上記データ\n",
            "data = [[5.2, 3.0, 1.5, 0.6]]# 決定木\n",
            "from sklearn.tree import DecisionTreeClassifier# 予測したいデータ\n",
            "data = [[5.2, 3.0, 1.5, 0.6]]# サポートベクターマシン（SVM）\n",
            "from sklearn.svm import LinearSVC# 予測したいデータ\n",
            "data = [[5.2, 3.0, 1.5, 0.6]]# ロジスティック回帰\n",
            "from sklearn.linear_model import LogisticRegression# 予測したいデータ\n",
            "data = [[5.2, 3.0, 1.5, 0.6]]import pandas as pd\n",
            "import re,itertools,json\n",
            "def visualize_instance_html_japanese(self, exp, label, div_name, exp_object_name,\n",
            "                                text=True, opacity=True):\n",
            "    if not text:\n",
            "        return u''\n",
            "    text = (self.indexed_string.raw_string()\n",
            "            .encode('utf-8', 'xmlcharrefreplace').decode())\n",
            "    text = re.sub(r'[<>&]', '|', text)\n",
            "    exp = [(self.indexed_string.word(x[0]),\n",
            "            self.indexed_string.string_position(x[0]),\n",
            "            x[1]) for x in exp]\n",
            "    all_ocurrences = list(itertools.chain.from_iterable(\n",
            "        [itertools.product([x[0]], x[1], [x[2]]) for x in exp]))\n",
            "    all_ocurrences = [(x[0], int(x[1]), x[2]) for x in all_ocurrences]\n",
            "    ret = '''\n",
            "        %s.show_raw_text(%s, %d, %s, %s, %s);\n",
            "        ''' % (exp_object_name, json.dumps(all_ocurrences), label,\n",
            "               json.dumps(text), div_name, json.dumps(opacity))\n",
            "    return retimport MeCab\n",
            "def tokenize(text):\n",
            "    wakati = MeCab.Tagger(\"-O wakati\")\n",
            "    wakati.parse(\"\")\n",
            "    return wakati.parse(text)$ pip install chainer\n",
            ">>> import chainer\n",
            ">>> x_data = np.array([5], dtype=np.float32)\n",
            ">>> x_data\n",
            "array([ 5.], dtype=float32)\n",
            ">>> x = chainer.Variable(x_data)\n",
            ">>> x\n",
            "<variable at 0x10b796fd0>\n",
            ">>> x.data\n",
            "array([ 5.], dtype=float32)\n",
            ">>> y = x ** 2 - 2 * x + 1\n",
            ">>> y\n",
            "<variable at 0x10b693dd0>\n",
            ">>> y.data\n",
            "array([ 16.], dtype=float32)\n",
            ">>> y.backward()\n",
            ">>> x.grad\n",
            "array([ 8.], dtype=float32)\n",
            "y'(x) = 2x - 2\\\\\n",
            "\\rightarrow \\ y'(5) = 8\n",
            ">>> x = chainer.Variable(np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32))\n",
            ">>> y = x**2 - 2*x + 1\n",
            ">>> y.grad = np.ones((2, 3), dtype=np.float32)\n",
            ">>> y.backward()\n",
            ">>> x.grad\n",
            "array([[  0.,   2.,   4.],\n",
            "       [  6.,   8.,  10.]], dtype=float32)\n",
            ">>> import chainer.links as L\n",
            ">>> f = L.Linear(3, 2)\n",
            ">>> f\n",
            "<chainer.links.connection.linear.Linear object at 0x10b7b4290>\n",
            "f(x) = Wx + b\\\\\n",
            "f \\in \\mathcal{R}^{2 \\times 1},\n",
            "x \\in \\mathcal{R}^{3 \\times 1},\\\\\n",
            "W \\in \\mathcal{R}^{2 \\times 3}, b \\in \\mathcal{R}^{2 \\times 1}\n",
            ">>> f.W.data\n",
            "array([[-0.02878495,  0.75096768, -0.10530342],\n",
            "       [-0.26099312,  0.44820449, -0.06585278]], dtype=float32)\n",
            ">>> f.b.data\n",
            "array([ 0.,  0.], dtype=float32)\n",
            ">>> f = L.Linear(3, 2)\n",
            ">>> x = chainer.Variable(np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32))\n",
            ">>> y = f(x)\n",
            ">>> y.data\n",
            "array([[ 1.15724015,  0.43785751],\n",
            "       [ 3.0078783 ,  0.80193317]], dtype=float32)\n",
            ">>> f.W.data\n",
            "array([[-0.02878495,  0.75096768, -0.10530342],\n",
            "       [-0.26099312,  0.44820449, -0.06585278]], dtype=float32)>>> f.zerograds()\n",
            ">>> f.W.grad\n",
            "array([[ 0.,  0.,  0.],\n",
            "       [ 0.,  0.,  0.]], dtype=float32)>>> y.grad = np.ones((2, 2), dtype=np.float32)\n",
            ">>> y.backward()\n",
            ">>> f.W.grad\n",
            "array([[ 5.,  7.,  9.],\n",
            "       [ 5.,  7.,  9.]], dtype=float32)\n",
            ">>> f.b.grad\n",
            "array([ 2.,  2.], dtype=float32)\n",
            ">>> l1 = L.Linear(4, 3)\n",
            ">>> l2 = L.Linear(3, 2)\n",
            ">>> l1.W.data\n",
            "array([[-0.2187428 ,  0.51174778,  0.30037731, -1.08665013],\n",
            "       [ 0.65367842,  0.23128517,  0.25591806, -1.0708735 ],\n",
            "       [-0.85425782,  0.25255874,  0.23436508,  0.3276397 ]], dtype=float32)>>> x = chainer.Variable(np.array([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=np.float32))\n",
            "MyChain.py\n",
            "# -*- coding: utf-8 -*-\n",
            "from chainer import Chain\n",
            "import chainer.links as L>>> model = MyChain()\n",
            ">>> optimizer = optimizers.SGD()  # 最適化手法をSGDに指定\n",
            ">>> optimizer.setup(model)\n",
            ">>> optimizer\n",
            "<chainer.optimizers.sgd.SGD object at 0x10b7b40d0>\n",
            "\n",
            "損失関数\n",
            "def forward(x, y, model):\n",
            "    loss = ... # 各自で損失関数を定義\n",
            "    return loss\n",
            "\n",
            "optimizer.update(forward, x, y, model)\n",
            "\n",
            "LSTM.py\n",
            "n_input = 28\n",
            "n_steps = 28\n",
            "n_hidden = 128\n",
            "batch_size = 50\n",
            "x = tf.placeholder(\"float\", [None, n_steps, n_input]) # Noneはバッチサイズが入る。つまり [50, 28, 28]\n",
            "抜粋-rnn_cell_impl.py\n",
            "class BasicLSTMCell(RNNCell):\n",
            ".\n",
            ".\n",
            ".\n",
            " def __call__(self, inputs, state, scope=None):\n",
            "    \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
            "    #num_unit = n_hidden = 128\n",
            "    #inputs = x [50, 28]*28 tensors\n",
            "    #activation is tanh\n",
            "    with vs.variable_scope(scope or \"basic_lstm_cell\"):\n",
            "      # Parameters of gates are concatenated into one multiply for efficiency.\n",
            "      if self._state_is_tuple:\n",
            "        c, h = state\n",
            "      else:\n",
            "        c, h = array_ops.split(1, 2, state)\n",
            "      concat = _linear([inputs, h], 4 * self._num_units, True, scope=scope) #[batch*output_size]\n",
            "sigmoid_test.py\n",
            "test = tf.sigmoid(tf.constant([1.43326855, -10.14613152, 2.10967159, 6.07900429, -3.25419664, -1.93730605, -8.57098293, 10.21759605, 1.16319525, 2.90590048]))\n",
            "ステップ毎の内部状態.py\n",
            "Iter 1280, Minibatch Loss= 1.806859, Training Accuracy= 0.35938\n",
            "y_ [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
            "answers of n_steps 6 6 6 6 6 0 0 6 6 6 7 2 2 2 0 0 0 0 0 0 0 0 0 0 0 7 4 4\n",
            "#右に行くほど最後のステップ\n",
            ".\n",
            ".\n",
            ".\n",
            "Iter 8960, Minibatch Loss= 0.827538, Training Accuracy= 0.75000\n",
            "y_[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
            "answers of n_steps 6 6 6 6 6 6 7 7 7 7 7 7 7 7 7 7 7 7 7 7 8 8 8 8 8 6 6 6\n",
            ".\n",
            ".\n",
            ".\n",
            "Iter 99840, Minibatch Loss= 0.125606, Training Accuracy= 0.96094\n",
            "y_[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
            "answers of n_steps 6 6 6 6 6 6 6 6 6 6 9 8 9 9 9 9 0 0 0 0 0 0 4 4 4 4 4 4\n",
            "\n",
            "#パッケージインポート\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "%matplotlib inlinetrain_data.info()\n",
            "train_data_sub1 = train_data.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
            "train_data_sub2 = train_data_sub1.dropna()\n",
            "train_data_sub2.info()\n",
            "train_data_sub2.describe()\n",
            "#Pclass別の生存率\n",
            "pclass_groupby = pd.concat([train_data_sub2.groupby('Pclass')['Survived'].sum() / train_data_sub2.groupby('Pclass')['Survived'].count(), \n",
            "                            train_data_sub2.groupby('Pclass')['Survived'].count()], axis=1)\n",
            "pclass_groupby.columns = ['Survived_rate', 'num_of_passenger']\n",
            "pclass_groupby\n",
            "#性別の生存率\n",
            "sex_groupby = pd.concat([train_data_sub2.groupby('Sex')['Survived'].sum() / train_data_sub2.groupby('Sex')['Survived'].count(), \n",
            "                            train_data_sub2.groupby('Sex')['Survived'].count()], axis=1)\n",
            "sex_groupby.columns = ['Survived_rate', 'num_of_passenger']\n",
            "sex_groupby\n",
            "#生存, 非生存の年齢層分布\n",
            "plt.figure(figsize=(15,6))\n",
            "plt.hist(train_data_sub2.Age[train_data_sub2.Survived == 0], normed=True, bins=10, alpha=0.8, color='red', label='not survived')\n",
            "plt.hist(train_data_sub2.Age[train_data_sub2.Survived == 1], normed=True, bins=10, alpha=0.7, color='blue', label='survived')\n",
            "plt.grid(True)\n",
            "plt.legend(loc='best', fontsize=15)\n",
            "plt.xlabel('Age')\n",
            "train_data_sub2.groupby('Survived', as_index=False)['Age'].mean()\n",
            "#生存, 非生存の運賃分布\n",
            "plt.figure(figsize=(15,6))\n",
            "plt.hist(train_data_sub2.Fare[train_data_sub2.Survived == 0], normed=True, bins=10, alpha=0.8, color='red', label='not survived')\n",
            "plt.hist(train_data_sub2.Fare[train_data_sub2.Survived == 1], normed=True, bins=20, alpha=0.7, color='blue', label='survived')\n",
            "plt.grid(True)\n",
            "plt.legend(loc='best', fontsize=15)\n",
            "plt.xlabel('Fare')\n",
            "#ダミー変数取得\n",
            "dummy_train = pd.get_dummies(train_data_sub2[['Sex', 'Embarked']])\n",
            "dummy_train.head()\n",
            "#元のデータに結合\n",
            "train_data_sub3 = pd.concat([train_data_sub2.drop(['Sex', 'Embarked'], axis=1), dummy_train], axis=1)\n",
            "train_data_sub3.head()\n",
            "#テストデータも同様に処理\n",
            "test_data_sub1 = test_data.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
            "dummy_test = pd.get_dummies(test_data_sub1[['Sex', 'Embarked']])\n",
            "test_data_sub2 = pd.concat([test_data_sub1.drop(['Sex', 'Embarked'], axis=1), dummy_test], axis=1)\n",
            "#モデル作成\n",
            "X_train = train_data_sub3.drop('Survived', axis=1)\n",
            "y_train = train_data_sub3.Survived#モデル評価\n",
            "bench_mark = train_data_sub3.Survived.sum() / train_data_sub3.Survived.count()bench_mark :  0.4044943820224719\n",
            "training score :  0.7991573033707865\n",
            "feature_coef = pd.concat([df(X_train.columns), df(log_reg.coef_[0, :])], axis=1)\n",
            "feature_coef.columns = ['feature name', 'coefficient']\n",
            "feature_coef['abs_coefficient'] = abs(feature_coef.coefficient)\n",
            "feature_coef.sort_values(by='abs_coefficient', ascending=False).drop('abs_coefficient', axis=1)\n",
            "test_data_sub2.Age = test_data_sub2.fillna(test_data_sub2.Age.mean())\n",
            "test_data_sub2.Fare = test_data_sub2.fillna(test_data_sub2.Fare.mean())\n",
            "survived_predict = log_reg.predict(test_data_sub2)\n",
            "survived_predict[:5]\n",
            "array([0, 1, 0, 0, 1])\n",
            "submittion_file = pd.concat([df(test_data.PassengerId), df(survived_predict)], axis=1)\n",
            "submittion_file.columns = ['PassengerId', 'Survived']\n",
            "submittion_file.to_csv('submittion.csv', index=False)\n",
            "$ CC=gcc-7 CXX=g++-7 python setup.py install\n",
            "import wordbatch\n",
            "from wordbatch.extractors import WordBag, WordHashword_comment = wb.fit_transform(str_array)\n",
            "X = word_comment.tocsr() # csrにするとき\n",
            "from nltk.corpus import stopwords\n",
            "import re\n",
            "clf = FTRL(alpha=0.01, beta=0.01, L1=0.0, L2=0.0, D=2**28, iters=3, \n",
            "            inv_link=\"sigmoid\", e_clip=1.0, threads=4, verbose=0) clf = FM_FTRL(alpha=0.01, beta=0.01, L1=0.00001, L2=0.1, D=1000, alpha_fm=0.01, L2_fm=0.0, init_fm=0.01,\n",
            "                    D_fm=200, e_noise=0.0001, iters=15, inv_link=\"identity\", threads=4)\n",
            "\n",
            "solve_X_y.m\n",
            "% solve_X_y.m\n",
            "X = rand(10, 4)\n",
            "# X =\n",
            "#\n",
            "#    0.033536   0.816107   0.996677   0.958327\n",
            "#    0.683542   0.116498   0.614316   0.884338\n",
            "#    0.734337   0.769245   0.696212   0.245270\n",
            "#    0.216938   0.013297   0.885327   0.906086\n",
            "#    0.630620   0.733668   0.820551   0.784664\n",
            "#    0.138834   0.838178   0.216751   0.638286\n",
            "#    0.100739   0.893597   0.891867   0.239482\n",
            "#    0.362333   0.404999   0.018274   0.922847\n",
            "#    0.102606   0.442110   0.744582   0.452299\n",
            "#    0.590709   0.274452   0.459526   0.656588\n",
            "rank_deficient.m\n",
            "X = [1 1 2 3;\n",
            "     1 2 3 5;\n",
            "     1 3 5 8;\n",
            "     1 5 8 13;\n",
            "     1 8 13 21;\n",
            "     1 13 21 34]\n",
            "solve_X_y.jl\n",
            "X = rand(10, 4)\n",
            "# 10x4 Array{Float64,2}:\n",
            "#  0.71148    0.968352  0.0952939  0.796324 \n",
            "#  0.915128   0.128326  0.630086   0.0635579\n",
            "#  0.351199   0.131409  0.934867   0.501701 \n",
            "#  0.165645   0.874088  0.173725   0.976326 \n",
            "#  0.765261   0.790716  0.760362   0.204496 \n",
            "#  0.544099   0.156464  0.041718   0.507071 \n",
            "#  0.764964   0.852837  0.230312   0.134783 \n",
            "#  0.0738597  0.75529   0.693856   0.0107293\n",
            "#  0.621861   0.56881   0.66972    0.163911 \n",
            "#  0.9471     0.453841  0.466836   0.10744  \n",
            "rank_deficient.jl\n",
            "X = [1 1 2 3;\n",
            "     1 2 3 5;\n",
            "     1 3 5 8;\n",
            "     1 5 8 13;\n",
            "     1 8 13 21;\n",
            "     1 13 21 34]\n",
            "solve_X_y.py\n",
            "import numpy as np\n",
            "X = np.random.rand(10, 4)\n",
            "# array([[ 0.61009055,  0.71722947,  0.48465025,  0.15660522],\n",
            "#        [ 0.02424431,  0.49947237,  0.60493258,  0.8988653 ],\n",
            "#        [ 0.65048106,  0.69667863,  0.52860957,  0.65003537],\n",
            "#        [ 0.56541266,  0.25463788,  0.74047536,  0.64691215],\n",
            "#        [ 0.03052439,  0.47651739,  0.01667898,  0.7613639 ],\n",
            "#        [ 0.87725831,  0.47684888,  0.44039111,  0.39706053],\n",
            "#        [ 0.58302851,  0.20919564,  0.97598994,  0.19268083],\n",
            "#        [ 0.35987338,  0.98331404,  0.06299533,  0.76193058],\n",
            "#        [ 0.625453  ,  0.70985323,  0.62948802,  0.627458  ],\n",
            "#        [ 0.64201569,  0.22264827,  0.71333221,  0.53305839]])\n",
            "rank_deficient.jl\n",
            "import numpy as np\n",
            "X = np.array([\n",
            "        [1, 1, 2, 3],\n",
            "        [1, 2, 3, 5],\n",
            "        [1, 3, 5, 8],\n",
            "        [1, 5, 8, 13],\n",
            "        [1, 8, 13, 21],\n",
            "        [1, 13, 21, 34]])f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left(-\\frac{(x - \\mu)^2}\n",
            "{2\\sigma^2} \\right) \\hspace{20px} (-\\infty < x < \\infty)\n",
            "F_1(x)=y\n",
            "h_1(x)=y-F_1(x)\n",
            "F_2(x)=F_1(x)+h_1(x)\n",
            "F(x)=F_1(x)\\longmapsto F_2(x)=F_1(x)+h_1(x)\\ldots \\longmapsto F_M(x)=F_{M-1}(x)+h_{M-1}(x)\\\\\n",
            "F_0(x)=\\underset{\\gamma}{\\arg\\min} \\sum^n_{i=1} L(y_i,\\gamma )= \\underset{\\gamma}{\\arg\\min} \\sum^n_{i=1}(\\gamma - y_i)^2 = \\frac{1}{n}\\sum^n_{i=1}y_i\n",
            "F_{m+1}(x) = F_m(x) + h_m(x) = y, \\mbox{for } m \\ge 0\n",
            "L(x_1, x_2) = \\frac{1}{2}(x_1-15)^2 + \\frac{1}{2}(x_2-25)^2\n",
            "r_{im} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x)=F_{m-1}(x)} \\quad \\mbox{for } i=1,\\ldots,n.\n",
            "p(x|y) = \\left\\{\n",
            "\\begin{array}{ll}\n",
            "l(x) & (y \\lt y^*) \\\\\n",
            "g(x) & (y \\geq y^*)\n",
            "\\end{array}\n",
            "\\right.\n",
            "EI_{y^*}(x) = (\\gamma + \\frac{g(x)}{l(x)}(1 - \\gamma))^{-1}\n",
            "from hyperopt import hp, tpe, Trials, fmin\n",
            "hyperopt_parameters = {\n",
            "    'C': hp.uniform('C', 0, 2),\n",
            "    'gamma': hp.loguniform('gamma', -8, 2),\n",
            "    'kernel': hp.choice('kernel', ['rbf', 'poly', 'sigmoid'])\n",
            "}\n",
            "def objective(args):\n",
            "    # モデルのインスタンス化\n",
            "    classifier = SVC(**args)\n",
            "    # trainデータを使ってモデルの学習\n",
            "    classifier.fit(x_train, y_train)\n",
            "    # validationデータを使用して、ラベルの予測\n",
            "    predicts = classifier.predict(x_test)\n",
            "    # 予測ラベルと正解ラベルを使用してmicro f1を計算\n",
            "    f1 = f1_score(y_test, predicts, average='micro')\n",
            "    # 今回はmicro f1を最大化したいので、-1をかけて最小化に合わせる\n",
            "    return -1*f1\n",
            "# iterationする回数\n",
            "max_evals = 200\n",
            "# 試行の過程を記録するインスタンス\n",
            "trials = Trials()$ best\n",
            ">> {'C': 1.61749553623185, 'gamma': 0.23056607283675354, 'kernel': 0}\n",
            "$ trials.best_trial['result']\n",
            ">> {'loss': -0.9698492462311558, 'status': 'ok'}hyperopt_parameters_loguniform = {\n",
            "    'C': hp.loguniform('C', -8, 2),\n",
            "    'gamma': hp.loguniform('gamma', -8, 2),\n",
            "    'kernel': hp.choice('kernel', ['rbf', 'poly', 'sigmoid'])\n",
            "    }# HyperOptの計算時間\n",
            "%timeit hyperopt_search(200)\n",
            ">> 1 loop, best of 3: 32.1 s per loopy \\sim Xw \\tag{1.1}\n",
            "L(w) := (y-Xw)^2 \\tag{1.2}\n",
            "\\frac{\\partial L }{\\partial w} = -2(X^Ty-X^TXw)=0 \\tag{1.3}\n",
            "w=\\big(X^TX\\big)^{-1}X^Ty \\tag{1.4}\n",
            "X=\\begin{pmatrix}\n",
            "1 & 1 & 1 \\\\\n",
            "1 & 2 & 3\n",
            "\\end{pmatrix},\\;\n",
            "y=\\begin{pmatrix}\n",
            "0\\\\1\n",
            "\\end{pmatrix},\\;\n",
            "w=\\begin{pmatrix}\n",
            "a\\\\b\\\\c\n",
            "\\end{pmatrix} \\tag{1.5}\n",
            "X^TX=\n",
            "\\begin{pmatrix}\n",
            "1 & 1 \\\\\n",
            "1 & 2 \\\\\n",
            "1 & 3 \\\\\n",
            "\\end{pmatrix}\n",
            "\\begin{pmatrix}\n",
            "1 & 1 & 1 \\\\\n",
            "1 & 2 & 3\n",
            "\\end{pmatrix}\n",
            "=\n",
            "\\begin{pmatrix}\n",
            "2 & 3 & 4 \\\\\n",
            "3 & 5 & 7 \\\\\n",
            "4 & 7 & 10 \n",
            "\\end{pmatrix} \\tag{1.6}\n",
            "\\text{det}\\big(X^TX\\big) = 0 \\tag{1.7}\n",
            "a+b+c=0, \\;\\; a+2b+3c=1 \\tag{1.8}\n",
            "a=c-1, \\;\\; b=1-2c \\tag{1.9}\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "from mpl_toolkits.mplot3d import Axes3D\n",
            "sns.set_context('poster')\n",
            "X = np.array([[1, 1, 1],[1, 2, 3]])\n",
            "y = np.array([[0],[1]])\n",
            "from sklearn.linear_model import LinearRegression\n",
            "np.random.seed(0)\n",
            "model = LinearRegression(fit_intercept=False)\n",
            "model.fit(X,y)\n",
            "model.coef_\n",
            "f(x_0+\\Delta x)\\simeq f(x_0)+\\Delta x \\cdot \\frac{\\partial f}{\\partial x} (x_0) \\tag{1.10}\n",
            "\\Delta x \\propto \\mp \\frac{\\partial f}{\\partial x} (x_0) \\tag{1.11}\n",
            "x_{t+1} = x_{t} -\\eta \\frac{\\partial f}{\\partial x}(x_t) \\tag{1.12}\n",
            "\\begin{align}\n",
            "w_{t+1} &= w_t - \\eta \\frac{\\partial L}{\\partial w}(w_t) \\\\\n",
            "        &= w_t - \\eta \\big[-2(X^Ty-X^TXw_t) \\big] \\tag{1.13}\n",
            "\\end{align}\n",
            "class LR_GradientDecent(object):model = LR_GradientDecent(n_iter=1000)\n",
            "model.fit(X,y)\n",
            "plt.plot(model.loss_arr)\n",
            "plt.xlabel('Number of Iterations')\n",
            "plt.ylabel('Error')\n",
            "model.w.flatten()\n",
            "coefs = []\n",
            "err = []\n",
            "for seed in range(200):\n",
            "    model = LR_GradientDecent(n_iter=1000, seed=seed)\n",
            "    model.fit(X, y)\n",
            "    coefs.append(model.w.flatten())\n",
            "    err.append(model.loss_arr[-1])\n",
            "df_sol = pd.DataFrame(coefs)\n",
            "df_sol.columns = ['a', 'b', 'c']\n",
            "df_sol['error'] = errfig = plt.figure()\n",
            "ax = Axes3D(fig)\n",
            "# solutions found by gradient decent\n",
            "ax.plot(df_sol['a'], df_sol['b'], df_sol['c'], 'o', ms=4, mew=0.5)\n",
            "# exact solutions\n",
            "c = np.linspace(0, 1, 100)\n",
            "a = np.array([i - 1 for i in c])\n",
            "b = np.array([1-2*i for i in c])\n",
            "ax.plot(a, b, c, color='yellow', ms=4, mew=0.5, alpha=0.4)\n",
            "#ax.set_xlim(-1,0)\n",
            "#ax.set_ylim(-1,1)\n",
            "#ax.set_zlim(0,1)\n",
            "#ax.set_xlabel('a')\n",
            "#ax.set_ylabel('b')\n",
            "#ax.set_zlabel('c')\n",
            "ax.view_init(elev=30, azim=45)\n",
            "plt.show()\n",
            "L=\\sum_{n=1}^NL_n \\tag{2.1}\n",
            "\\begin{align}\n",
            "& m_t = \\gamma m_{t-1} + \\eta \\frac{\\partial L}{\\partial w}(w_t)　\\tag{3.1}\\\\\n",
            "& w_{t+1} = w_t - m_t \\tag{3.2}\n",
            "\\end{align}\n",
            "m_t=g_t^{\\eta} + \\gamma g_{t-1}^{\\eta} + \\gamma^2 g_{t-2}^{\\eta} + \\gamma^3 g_{t-3}^{\\eta} + \\cdots \\tag{3.3}\n",
            "\\begin{align}\n",
            "& m_t = \\gamma m_{t-1} + \\eta \\frac{\\partial L}{\\partial w}(w_t - m_{t-1})　\\tag{3.4}\\\\\n",
            "& w_{t+1} = w_t - m_t \\tag{3.5}\n",
            "\\end{align}\n",
            "g^t_i:=\\frac{\\partial L}{\\partial w_i}(w^t) \\tag{3.6}\n",
            "\\begin{align}\n",
            "& v^t_i = v^{t-1}_i + (g^t_i)^2 \\tag{3.7} \\\\\n",
            "& w^{t+1}_i = w^t_i - \\frac{\\eta}{\\sqrt{v^t_i} + \\epsilon} g^t_i　\\tag{3.8}\n",
            "\\end{align}\n",
            "v^t_i = (g^t_i)^2 + (g^{t-2}_i)^2 + (g^{t-3}_i)^2 + \\cdots \\tag{3.9}\n",
            "\\begin{align}\n",
            "& v^t_i = \\gamma v^{t-1}_i + (1-\\gamma)(g^t_i)^2 \\tag{3.10} \\\\\n",
            "& w^{t+1}_i = w^t_i - \\frac{\\eta}{\\sqrt{v^t_i}+\\epsilon} g^t_i　\\tag{3.11}\n",
            "\\end{align}\n",
            "v^t_i=\\gamma^t v^0_i + (1-\\gamma)\\sum_{l=1}^t \\gamma^{t-l}(g^l_i)^2 \\tag{3.12}\n",
            "\\begin{align}\n",
            "& v^t_i = \\gamma v^{t-1}_i + (1-\\gamma)(g^t_i)^2 \\tag{3.13} \\\\\n",
            "& s^t_i = \\gamma s^{t-1}_i + (1-\\gamma)(\\Delta w^{t-1}_i)^2 \\tag{3.14} \\\\\n",
            "& \\Delta w^t_i = -\\frac{\\sqrt{s^t_i + \\epsilon}}{\\sqrt{v^t_i + \\epsilon}} g^t_i \\tag{3.15} \\\\\n",
            "& w^{t+1}_i = w^t_i + \\Delta w^t_i　\\tag{3.16}\n",
            "\\end{align}\n",
            "\\begin{align}\n",
            "& m^t_i = \\beta_1 m^{t-1}_i + (1-\\beta_1)g^t_i \\tag{3.17} \\\\\n",
            "& v^t_i = \\beta_2 v^{t-1}_i + (1-\\beta_2)(g^t_i)^2 \\tag{3.18} \\\\\n",
            "& \\hat{m}^t_i = \\frac{m^t_i}{1-\\beta_1^t} \\tag{3.19} \\\\\n",
            "& \\hat{v}^t_i = \\frac{v^t_i}{1-\\beta_2^t} \\tag{3.20} \\\\\n",
            "& w^{t+1}_i = w^t_i - \\frac{\\eta}{\\sqrt{\\hat{v}^t_i}+\\epsilon} \\hat{m}^t_i　\\tag{3.21}\n",
            "\\end{align}\n",
            "m^t_i=\\beta_1^tm_0 + (1-\\beta_1)\\sum_{k=1}^t \\beta_1^{t-k}g^k_i \\tag{3.22}\n",
            "O[m] \\sim O[g] (1-\\beta_1) \\sum_{k=1}^t \\beta_1^{t-k} = O[g](1-\\beta_1^t) \\tag{3.23}\n",
            "\\begin{align}\n",
            "& m^t_i = \\beta_1 m^{t-1}_i + (1-\\beta_1)g^t_i \\tag{3.24} \\\\\n",
            "& v^t_i = \\beta_2 v^{t-1}_i + (1-\\beta_2)(g^t_i)^2 \\tag{3.25} \\\\\n",
            "& \\hat{m}^t_i = \\frac{m^t_i}{1-\\beta_1^t} \\tag{3.26} \\\\\n",
            "& \\hat{v}^t_i = \\frac{v^t_i}{1-\\beta_2^t} \\tag{3.27} \\\\\n",
            "& r^t = (\\text{feed-back from the loss function}) \\tag{3.28} \\\\\n",
            "& d^t = \\beta_3 d_{t-1} + (1-\\beta_3) r^t \\tag{3.29} \\\\\n",
            "& w^{t+1}_i = w^t_i - \\frac{\\eta}{d^t \\sqrt{\\hat{v}^t_i}+\\epsilon} \\hat{m}^t_i　\\tag{3.30}\n",
            "\\end{align}\n",
            "\\frac{\\partial f}{\\partial x}(x_0+\\Delta x)\\simeq \\frac{\\partial f}{\\partial x}(x_0)+ \\frac{\\partial f}{\\partial x \\partial x}(x_0) \\Delta x \\tag{4.1}\n",
            "\\Delta x = - \\Big[\\frac{\\partial f}{\\partial x \\partial x}(x_0)\\Big]^{-1}\\frac{\\partial f}{\\partial x}(x_0) \\tag{4.2}\n",
            "x_{t+1} = x_t -\\Big[\\frac{\\partial f}{\\partial x \\partial x}(x_t)\\Big]^{-1}\\frac{\\partial f}{\\partial x}(x_t) \\tag{4.3}\n",
            "import numpy as np\n",
            "from sklearn.datasets import load_iris\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import accuracy_score\n",
            "from sklearn.ensemble import RandomForestClassifierFeature Importances:\n",
            "    sepal length (cm)    : 0.074236\n",
            "    sepal width (cm)     : 0.015321\n",
            "    petal length (cm)    : 0.475880\n",
            "    petal width (cm)     : 0.434563\n",
            "rf_reg = RandomForestRegressor(n_estimators=10)\n",
            "rf_reg = rf_reg.fit(X_train, y_train)Feature Importances:\n",
            "    CRIM       : 0.040931\n",
            "    ZN         : 0.001622\n",
            "    INDUS      : 0.005131\n",
            "    CHAS       : 0.000817\n",
            "    NOX        : 0.042200\n",
            "    RM         : 0.536127\n",
            "    AGE        : 0.018797\n",
            "    DIS        : 0.054397\n",
            "    RAD        : 0.001860\n",
            "    TAX        : 0.010357\n",
            "    PTRATIO    : 0.011388\n",
            "    B          : 0.007795\n",
            "    LSTAT      : 0.268579\n",
            "clf_xgb = xgb.XGBClassifier(objective='multi:softmax',\n",
            "                        max_depth = 5,\n",
            "                        learning_rate=0.1,\n",
            "                        n_estimators=100)\n",
            "clf_xgb.fit(X_train, y_train,\n",
            "        eval_set=[(X_test, y_test)],\n",
            "        eval_metric='mlogloss',\n",
            "        early_stopping_rounds=10)\n",
            "y_pred_proba = clf_xgb.predict_proba(X_test)\n",
            "y_pred = np.argmax(y_pred_proba, axis=1)'''\n",
            "# Error\n",
            "reg_xgb = xgb.XGBRegressor(max_depth=3)\n",
            "reg_xgb.fit(X_train, y_train)\n",
            "fti = reg_xgb.feature_importances_\n",
            "'''Feature Importances:\n",
            "    CRIM       :     565.0000\n",
            "    ZN         :      55.0000\n",
            "    INDUS      :      99.0000\n",
            "    CHAS       :      10.0000\n",
            "    NOX        :     191.0000\n",
            "    RM         :     377.0000\n",
            "    AGE        :     268.0000\n",
            "    DIS        :     309.0000\n",
            "    RAD        :      50.0000\n",
            "    TAX        :      88.0000\n",
            "    PTRATIO    :      94.0000\n",
            "    B          :     193.0000\n",
            "    LSTAT      :     285.0000\n",
            "gbm = lgb.LGBMClassifier(objective='multiclass',\n",
            "                        num_leaves = 23,\n",
            "                        learning_rate=0.1,\n",
            "                        n_estimators=100)\n",
            "gbm.fit(X_train, y_train,\n",
            "        eval_set=[(X_test, y_test)],\n",
            "        eval_metric='multi_logloss',\n",
            "        early_stopping_rounds=10)\n",
            "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
            "y_pred_proba = gbm.predict_proba(X_test, num_iteration=gbm.best_iteration)gbm_reg = lgb.LGBMRegressor(objective='regression',\n",
            "                        num_leaves = 31,\n",
            "                        n_estimators=100)\n",
            "gbm_reg.fit(X_train, y_train,\n",
            "        eval_set=[(X_test, y_test)],\n",
            "        eval_metric='l2',\n",
            "        verbose=0)import statsmodels.api as sm                             OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      y   R-squared:                       0.758\n",
            "Model:                            OLS   Adj. R-squared:                  0.749\n",
            "Method:                 Least Squares   F-statistic:                     78.38\n",
            "Date:                Fri, 10 Mar 2017   Prob (F-statistic):           8.08e-92\n",
            "Time:                        15:14:41   Log-Likelihood:                -1011.3\n",
            "No. Observations:                 339   AIC:                             2051.\n",
            "Df Residuals:                     325   BIC:                             2104.\n",
            "Df Model:                          13                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const         38.3323      6.455      5.939      0.000      25.634      51.031\n",
            "x1            -0.1027      0.035     -2.900      0.004      -0.172      -0.033\n",
            "x2             0.0375      0.018      2.103      0.036       0.002       0.073\n",
            "x3             0.0161      0.081      0.198      0.843      -0.144       0.176\n",
            "x4             2.8480      1.058      2.692      0.007       0.767       4.929\n",
            "x5           -19.4905      5.103     -3.819      0.000     -29.530      -9.451\n",
            "x6             3.9906      0.501      7.973      0.000       3.006       4.975\n",
            "x7             0.0004      0.016      0.024      0.980      -0.031       0.032\n",
            "x8            -1.5980      0.256     -6.236      0.000      -2.102      -1.094\n",
            "x9             0.3687      0.090      4.099      0.000       0.192       0.546\n",
            "x10           -0.0139      0.005     -2.667      0.008      -0.024      -0.004\n",
            "x11           -0.9445      0.167     -5.640      0.000      -1.274      -0.615\n",
            "x12            0.0086      0.004      2.444      0.015       0.002       0.015\n",
            "x13           -0.6182      0.063     -9.777      0.000      -0.743      -0.494\n",
            "==============================================================================\n",
            "Omnibus:                      115.727   Durbin-Watson:                   2.041\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              485.421\n",
            "Skew:                           1.416   Prob(JB):                    3.91e-106\n",
            "Kurtosis:                       8.133   Cond. No.                     1.53e+04\n",
            "==============================================================================\n",
            "pvalues = fitted.pvalues\n",
            "feats = boston['feature_names']\n",
            "print('P>|t| :')\n",
            "for i, feat in enumerate(feats):\n",
            "    print('\\t{0:10s} : {1:>.6f}'.format(feat, pvalues[(i + 1)]))\n",
            "P>|t| :\n",
            "    CRIM       : 0.003980\n",
            "    ZN         : 0.036198\n",
            "    INDUS      : 0.843357\n",
            "    CHAS       : 0.007475\n",
            "    NOX        : 0.000160\n",
            "    RM         : 0.000000\n",
            "    AGE        : 0.980493\n",
            "    DIS        : 0.000000\n",
            "    RAD        : 0.000053\n",
            "    TAX        : 0.008030\n",
            "    PTRATIO    : 0.000000\n",
            "    B          : 0.015065\n",
            "    LSTAT      : 0.000000\n",
            "import statsmodels.api as sm                            Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:                      y   No. Observations:                  668\n",
            "Model:                          Logit   Df Residuals:                      657\n",
            "Method:                           MLE   Df Model:                           10\n",
            "Date:                Fri, 10 Mar 2017   Pseudo R-squ.:                  0.3219\n",
            "Time:                        15:36:15   Log-Likelihood:                -305.07\n",
            "converged:                       True   LL-Null:                       -449.89\n",
            "                                        LLR p-value:                 2.391e-56\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          1.2615   8.99e+06    1.4e-07      1.000   -1.76e+07    1.76e+07\n",
            "x1             0.0001      0.000      0.309      0.758      -0.001       0.001\n",
            "x2            -0.9141      0.162     -5.644      0.000      -1.232      -0.597\n",
            "x3             2.7590      0.231     11.939      0.000       2.306       3.212\n",
            "x4            -0.0322      0.009     -3.657      0.000      -0.049      -0.015\n",
            "x5            -0.4421      0.132     -3.350      0.001      -0.701      -0.183\n",
            "x6            -0.0709      0.141     -0.503      0.615      -0.347       0.206\n",
            "x7          2.456e-07   1.77e-07      1.386      0.166   -1.02e-07    5.93e-07\n",
            "x8             0.0023      0.003      0.926      0.354      -0.003       0.007\n",
            "x9             0.6809   8.99e+06   7.57e-08      1.000   -1.76e+07    1.76e+07\n",
            "x10            0.3574   8.99e+06   3.97e-08      1.000   -1.76e+07    1.76e+07\n",
            "x11            0.2231   8.99e+06   2.48e-08      1.000   -1.76e+07    1.76e+07\n",
            "==============================================================================P(w_t | h) = \\text{softmax}(\\text{score}(w_t, h)) \\\\\n",
            "           = \\frac{\\exp\\{\\text{score}(w_t, h)\\}}\n",
            "             {\\sum_\\text{Word w' in Vocab} \\exp \\{ \\text{score}(w', h) \\} }.\n",
            " J_\\text{ML} = \\log P(w_t | h) \\\\\n",
            "  = \\text{score}(w_t, h) -\n",
            "     \\log \\left( \\sum_\\text{Word w' in Vocab} \\exp \\{ \\text{score}(w', h) \\} \\right)\n",
            "J_\\text{NEG} = \\log Q_\\theta(D=1 |w_t, h) +\n",
            "  k \\mathop{\\mathbb{E}}_{\\tilde w \\sim P_\\text{noise}}\n",
            "     \\left[ \\log Q_\\theta(D = 0 |\\tilde w, h) \\right]\n",
            "J^{(t)}_\\text{NEG} = \\log Q_\\theta(D=1 | \\text{the, quick}) +\n",
            "  \\log(Q_\\theta(D=0 | \\text{sheep, quick}))\n",
            "embeddings = tf.Variable(\n",
            "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
            "nce_weights = tf.Variable(\n",
            "  tf.truncated_normal([vocabulary_size, embedding_size],\n",
            "                      stddev=1.0 / math.sqrt(embedding_size)))\n",
            "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
            "# Placeholders for inputs\n",
            "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
            "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
            "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
            "# Compute the NCE loss, using a sample of the negative labels each time.\n",
            "loss = tf.reduce_mean(\n",
            "  tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n",
            "                 num_sampled, vocabulary_size))\n",
            "# We use the SGD optimizer.\n",
            "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
            "for inputs, labels in generate_batch(...):\n",
            "  feed_dict = {training_inputs: inputs, training_labels: labels}\n",
            "  _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict)\n",
            "y_p(t)=w_{1}x_{1}(t)+w_{2}x_{2}(t)+\\cdots +w_{N}x_{N}(t)+b\n",
            "y_p(t)=w_{0}x_{0}(t)+w_{1}x_{1}(t)+\\cdots +w_{N}x_{N}(t)+b\n",
            "\\mathbf{x}(t)=(x_1(t), x_2(t)\\cdots x_{N}(t))^T\n",
            "\\mathbf{w}=(w_1, w_2,\\cdots w_{N})^T\n",
            "y_p(t)=\\mathbf{w}^T\\mathbf{x}(t)+b\n",
            "E(\\mathbf{w}, b)=\\sum_{t=0}^{M}\\left [ {y(t)-y_p(t)} \\right ]^2=\\sum_{t=0}^{M}\\left [ {y(t)-\\left \\{\\mathbf{w}^T\\mathbf{x}(t)+b  \\right \\}} \\right ]^2\n",
            "E(\\mathbf{w}, b)=\\sum_{t=0}^{M}\\left [ {y(t)-y_p(t)} \\right ]^2=\\sum_{t=0}^{M}\\left [ {y(t)-\\left \\{\\mathbf{w}^T\\mathbf{x}(t)+b  \\right \\}} \\right ]^2\n",
            "E(\\mathbf{w}, b)=\\sum_{t=0}^{M}\\left [ {y(t)-y_p(t)} \\right ]^2=\\sum_{t=0}^{M}\\left [ {y(t)-\\left \\{\\mathbf{w}^T\\mathbf{x}(t)+b  \\right \\}} \\right ]^2\n",
            "\\mathbf{w}_{new} = \\mathbf{w}-\\eta \\frac{\\partial }{\\partial \\mathbf{w}}E(\\mathbf{w}, b)\n",
            "b_{new} = b-\\eta \\frac{\\partial }{\\partial b}E(\\mathbf{w}, b)\n",
            "# import関連\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib\n",
            "import matplotlib.pylab as plt\n",
            "import seaborn as sns# 練習問題: 問題を作成\n",
            "from numpy.random import * #乱数のライブラリをimport\n",
            "X_train = rand(100,2) # 0〜1の乱数で 100行2列の行列を生成\n",
            "X_test = rand(100,2) # 0〜1の乱数で 100行2列の行列を生成# 練習問題: 問題を線形回帰モデルで解く\n",
            "from sklearn import linear_model # scikit-learnライブラリの関数を使用します# グラフで予測具合を見る\n",
            "Y_pred = linear_reg_model.predict(X_test) # テストデータから予測してみるimport numpy as np\n",
            "from numpy.random import randn\n",
            "import pandas as pd\n",
            "from pandas import Series, DataFrame\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "%matplotlib inline\n",
            "survey_df = pd.read_csv('corder_survey.csv')\n",
            "survey_df.shape\n",
            "(15620, 113)\n",
            "survey_df.info()\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15620 entries, 0 to 15619\n",
            "Columns: 113 entries, Age to StudentDebtOwe\n",
            "dtypes: float64(85), object(28)\n",
            "memory usage: 13.5+ MB\n",
            "survey_df.describe()\n",
            "for col in survey_df.columns:\n",
            "    print(col)\n",
            "Gender: 性別\n",
            "HasChildren: 子供の有無\n",
            "EmploymentStatus: 現状の雇用形態\n",
            "Age: 年齢\n",
            "Income: 収入\n",
            "HoursLearning: 学習時間\n",
            "SchoolMajor: 専攻\n",
            "sns.countplot('Gender', data=survey_df)\n",
            "dataset = randn(100)\n",
            "plt.hist(dataset)\n",
            "# normed: 正規化, alpha: 透明度, color: 色, bins: ビン数\n",
            "plt.hist(dataset, normed=True, alpha=0.8, color='indianred', bins=15)\n",
            "sns.countplot('HasChildren', data=survey_df)\n",
            "survey_df['HasChildren'].loc[survey_df['HasChildren'] == 0] = 'No'\n",
            "survey_df['HasChildren'].loc[survey_df['HasChildren'] == 1] = 'Yes'\n",
            "survey_df['HasChildren'] = survey_df['HasChildren'].map({0: 'No', 1: 'Yes'})\n",
            "sns.countplot('HasChildren', data=survey_df)\n",
            "sns.countplot('HasChildren', data=survey_df)\n",
            "sns.countplot('EmploymentStatus', data=survey_df)\n",
            "sns.countplot(y='EmploymentStatus', data=survey_df)\n",
            "sns.countplot('Age', data=survey_df)\n",
            "sns.kdeplot(survey_df['Age'])\n",
            "dataset = randn(30)\n",
            "plt.hist(dataset, alpha=0.5)\n",
            "sns.rugplot(dataset)\n",
            "sns.kdeplot(dataset)\n",
            "kernel_options = [\"gau\", \"biw\", \"cos\", \"epa\", \"tri\", \"triw\"]\n",
            "for kernel in kernel_options:\n",
            "    sns.kdeplot(dataset, kernel=kernel, label=kernel)\n",
            "for bw in np.arange(0.5, 2, 0.25):\n",
            "  sns.kdeplot(dataset, bw=bw, label=bw)\n",
            "sns.kdeplot(survey_df['Income'])\n",
            "survey_df['Income'].describe()\n",
            "RuntimeWarning: Invalid value encountered in median\n",
            "count      7329.000000\n",
            "mean      44930.010506\n",
            "std       35582.783216\n",
            "min        6000.000000\n",
            "25%                NaN\n",
            "50%                NaN\n",
            "75%                NaN\n",
            "max      200000.000000\n",
            "Name: Income, dtype: float64\n",
            "sns.boxplot(survey['Income'])\n",
            "sns.boxplot(survey['Income'], whips=np.inf)\n",
            "sns.violinplot(survey_df['Income'])\n",
            "sns.kdeplot(survey_df['HoursLearning'])\n",
            "sns.violinplot(survey_df['HoursLearning'])\n",
            "hours_learning = survey_df['HoursLearning']\n",
            "hours_learning = hours_learning.dropna()\n",
            "sns.distplot(hours_learning)\n",
            "sns.distplot(hours_learning, rug=True, hist=False, kde_kws={'color':'indianred'})\n",
            "sns.countplot(y='SchoolMajor' , data=survey_df)\n",
            "from collections import Counter\n",
            "major_count = Counter(survey_df['SchoolMajor'])\n",
            "major_count.most_common(10)\n",
            "[(nan, 7170),\n",
            " ('Computer Science', 1387),\n",
            " ('Information Technology', 408),\n",
            " ('Business Administration', 284),\n",
            " ('Economics', 252),\n",
            " ('Electrical Engineering', 220),\n",
            " ('English', 204),\n",
            " ('Psychology', 187),\n",
            " ('Electrical and Electronics Engineering', 164),\n",
            " ('Software Engineering', 159)]\n",
            "X = []\n",
            "Y = []\n",
            "major_count_top10 = major_count.most_common(10)\n",
            "for record in major_count_top10:\n",
            "    X.append(record[0])\n",
            "    Y.append(record[1])X = []\n",
            "Y = []\n",
            "major_count_top10 = major_count.most_common(10)\n",
            "major_count_top10.reverse()\n",
            "for record in major_count_top10:\n",
            "    # record[0] == record[0]に関しては下に補足あり\n",
            "    if record[0] == record[0]:\n",
            "        X.append(record[0])\n",
            "        Y.append(record[1])if record[0] == record[0]:\n",
            "if pd.notnull(record[0]):\n",
            "male_female_df = survey_df.where((survey_df['Gender'] == 'male') + (survey_df['Gender'] == 'female') )\n",
            "sns.countplot('Gender', data=male_female_df, hue='HasChildren')\n",
            "fig = sns.FacetGrid(male_female_df, hue='Gender', aspect=4)\n",
            "fig.map(sns.kdeplot, 'Age', shade=True)\n",
            "oldest = male_female_df['Age'].max()\n",
            "fig.set(xlim=(0, oldest))\n",
            "fig.add_legend()\n",
            "# male_female_dfは survey_dfのGenderを男女に絞ったもの\n",
            "# EmploymentStatusの上位5件を取得\n",
            "from collections import Counter\n",
            "employ_count = Counter(male_female_df['EmploymentStatus'])\n",
            "employ_count_top = employ_count.most_common(5)\n",
            "print(employ_count_top)\n",
            "employ_list =[]sns.countplot(y='EmploymentStatus', data=employ_df, hue='Gender')\n",
            "new_survey_df['HasChildren'] = new_survey_df['HasChildren'].map({'No': 0, 'Yes': 1})\n",
            "sns.factorplot('EmploymentStatus', 'HasChildren', data=new_survey_df, aspect=2)\n",
            "sns.factorplot('EmploymentStatus', 'HasChildren', data=new_survey_df, aspect=2, hue='Gender')\n",
            "sns.lmplot('Age', 'HasChildren', data=new_survey_df)\n",
            "sns.lmplot('Age', 'HasChildren', data=new_survey_df, hue='EmploymentStatus')\n",
            "sns.lmplot('Age', 'HasChildren', data=new_survey_df, hue='Gender')\n",
            "fig, (axis1, axis2) = plt.subplots(1, 2, sharey=True)\n",
            "sns.regplot('HasChildren', 'Age', data=new_survey_df, ax=axis1)\n",
            "sns.violinplot(y='Age', x='HasChildren', data=new_survey_df, ax=axis2)\n",
            "fig = sns.FacetGrid(new_survey_df, col='EmploymentStatus', aspect=1.5)\n",
            "fig.map(sns.distplot, 'Age')\n",
            "oldest = new_survey_df['Age'].max()\n",
            "fig.set(xlim=(0, oldest))\n",
            "fig.add_legend()\n",
            "# JUDGE_DAY = 1, 第二添字の[3]は日経平均の終値が入っています。\n",
            "y_array.append([(array_base[i][3] - array_base[i+JUDGE_DAY][3]) / array_base[i][3] * 100])\n",
            "tmp_array = []\n",
            "for j in xrange(i+1, i + data_num + 1):\n",
            "    for k in range(16):\n",
            "        tmp_array.append((array_base[j][k] - array_base[j+1][k]) / array_base[j][k] * 100)\n",
            "x_array.append(tmp_array)\n",
            "NUM_HIDDEN1 = 50\n",
            "NUM_HIDDEN2 = 25\n",
            "def inference(x_ph, keep_prob):def loss(y, target):def training(sess, train_step, loss, x_train_array, y_flg_train_array):accuracy = tf.reduce_mean(tf.abs(y - y_ph))\n",
            "print \"accuracy\"\n",
            "print(sess.run(accuracy, feed_dict={x_ph: test_batch_xs, y_ph: test_batch_ys, keep_prob: 1.0}))\n",
            "# SGDRegressor\n",
            "clf = linear_model.SGDRegressor()\n",
            "testClf(clf, x_train_array, y_train_array, x_test_array, y_test_array)def testClf(clf, x_train_array, y_flg_train_array, x_test_array, y_flg_test_array):1.00044\n",
            "SGDRegressor: 0.943171296872\n",
            "DecisionTreeRegressor: 1.3551351662\n",
            "SVM: 0.945361479916\n",
            "\n",
            "TensorFlow\n",
            "p = sess.run(y, feed_dict={x_ph: data, keep_prob: 1.0})\n",
            "price = ((p[0][0] / 100.) + 1.) * 16892.33\n",
            "print price\n",
            "\n",
            "\n",
            "scikit-learn\n",
            "p = clf.predict(data)\n",
            "price = ((p[0] / 100.) + 1.) * 16892.33\n",
            "print price\n",
            "\n",
            "\n",
            "TensorFlow\n",
            "16804.3398821\n",
            "\n",
            "\n",
            "scikit-learn\n",
            "16822.6013292\n",
            "\n",
            "とりあえず初期値としてn点とる \n",
            "while(n==max回数）\n",
            "    1. ガウス過程で平均・分散を計算（推定）\n",
            "    2. （目的関数の代わりの）Acquisition Functionを更新・次の探索点を選択\n",
            "    3. 最小となる点をサンプル\n",
            "    4. n += 1\n",
            "\\begin{align}\n",
            "m({\\bf x}_{N+1}) & = {\\bf k}^{\\rm T}{\\bf C}_N^{-1}{\\bf t}_{1:t}, \\\\ \n",
            "\\sigma^2({\\bf x}_{N+1}) & = c – {\\bf k}^{\\rm T}{\\bf C}_N^{-1}{\\bf k}\n",
            "\\end{align} \n",
            "\\begin{align}\n",
            "k({\\bf x}, {\\bf x}^{\\prime}) & = \\theta_0 \\exp \\Bigl( -\\frac{\\theta_1}{2} | {\\bf x} – {\\bf x}^{\\prime} |^2 \\Bigr) + \\theta_2 + \\theta_3\\bf x^T {\\bf x}^{\\prime} \\\\  \n",
            "K_{n,m} & = k(\\bf x_n, \\bf x_m)\n",
            "\\end{align} \n",
            "EI(x) = (f^* - \\mu)\\Phi(\\frac{f^*-\\mu}{\\sigma}) + \\sigma \\phi(\\frac{f^*-\\mu}{\\sigma})\n",
            "x^* = argmax (EI(x))\n",
            "{\\begin{bmatrix}Y_a \\\\ Y_b\\end{bmatrix} \\sim N_n\\begin{pmatrix}\n",
            "\\begin{bmatrix}\\mu_a\\\\\\mu_b\\end{bmatrix},\n",
            "\\begin{bmatrix}\\Sigma_{aa} & \\Sigma_{ab}\\\\\\Sigma_{ba} & \\Sigma_{bb}\\end{bmatrix}\\end{pmatrix}\n",
            "}\n",
            "\\begin{align}\n",
            "\\mu_{{\\bf a}|{\\bf b}} & = \\mu_{{\\bf a}} + \\Sigma_{{\\bf a} {\\bf b}} \\Sigma_{{\\bf b} {\\bf b}}^{-1}({\\bf x}_{{\\bf b}} – \\mu_{{\\bf b}}),\\\\ \n",
            "\\Sigma_{{\\bf a}|{\\bf b}} & = \\Sigma_{{\\bf a} {\\bf a}} – \\Sigma_{{\\bf a} {\\bf b}} \\Sigma_{{\\bf b} {\\bf b}}^{-1} \\Sigma_{{\\bf b} {\\bf a}} \\\\\n",
            "\\end{align} \n",
            "y(\\bf x) = \\bf w^T \\bf \\phi(\\bf x)\n",
            "p(\\bf w) = N(\\bf w | \\bf 0, \\alpha^{-1} \\bf I)\n",
            "\\mathbb{E} [{\\bf y}] =  {\\bf 0} \\\\\n",
            "cov[{\\bf y}] = {\\bf K} \\\\\n",
            "K_{nm} = k({\\bf x_n}, {\\bf x_m}) = \\frac{1}{\\alpha} \\phi({\\bf x_n})^T \\phi({\\bf x_m})\n",
            "t_n = y_n + \\epsilon_n \\\\\n",
            "p(t_n|y_n) = N(t_n|y_n,\\beta^{-1}) \\\\　\n",
            "p({\\bf t}_{N+1}) = N(  {\\bf t}_{N+1} | {\\bf 0}, {\\bf C}_{N+1} ) \n",
            "{\n",
            "{\n",
            "\\begin{bmatrix}\n",
            "{\\bf t}_{1:N} \\\\ t_{N+1}\n",
            "\\end{bmatrix} \n",
            "\\sim N\n",
            "\\begin{pmatrix}\n",
            "\\begin{bmatrix}\n",
            "{\\bf \\mu}_{1:N}\\\\\\mu_{N+1}\n",
            "\\end{bmatrix},\n",
            "\\begin{bmatrix}\n",
            "{\\bf C}_N & {\\bf k} \\\\ {\\bf k} & c\n",
            "\\end{bmatrix}\n",
            "\\end{pmatrix}\n",
            "}\n",
            "}\n",
            "\\begin{align}\n",
            "p(t_{N+1}|\\bf t) & = N(t_{N+1}| m({\\bf x}_{N+1}),  \\sigma^2({\\bf x}_{N+1})) \\\\ \n",
            "m({\\bf x}_{N+1}) & = {\\bf k}^{\\rm T}{\\bf C}_N^{-1}{\\bf t}_{1:t}, \\\\ \n",
            "\\sigma^2({\\bf x}_{N+1}) & = c – {\\bf k}^{\\rm T}{\\bf C}_N^{-1}{\\bf k}\n",
            "\\end{align} \n",
            "{\\bf k} = [k({\\bf x}_{t+1}, {\\bf x}_1), k({\\bf x}_{t+1}, {\\bf x}_2), \\cdots, k({\\bf x}_{t+1}, {\\bf x}_t)] \n",
            "I(x) = \\max(f^*-Y,0) \n",
            "\\begin{align}\n",
            "EI(x) & =  E_{Y\\sim \\mathcal{N}(\\mu, \\sigma^2)}[I(x)] \\\\ \n",
            "& = E_{\\epsilon\\sim \\mathcal{N}(0,1)}[I(x)]  \\\\\n",
            "& = \\int_{-\\infty}^{\\infty} I(x) \\phi(\\epsilon) d\\epsilon \\\\\n",
            "& = \\int_{-\\infty}^{(f^*-\\mu)/\\sigma} (f^* - \\mu - \\sigma \\epsilon) \\phi(\\epsilon) d\\epsilon \\\\\n",
            "& = (f^* - \\mu)\\Phi(\\frac{f^*-\\mu}{\\sigma}) - \\sigma \\int_{-\\infty}^{(f^*-\\mu)/\\sigma} \\epsilon \\phi(\\epsilon) d\\epsilon \\\\\n",
            "\\end{align} \n",
            "\\begin{align}\n",
            "EI(x) & = (f^* - \\mu)\\Phi(\\frac{f^*-\\mu}{\\sigma}) + \\frac{\\sigma}{\n",
            "\\sqrt{2\\pi}} \\int_{-\\infty}^{(f^*-\\mu)/\\sigma} (-\\epsilon) e^{-\\epsilon^2/2} d\\epsilon \\\\\n",
            "& = (f^* - \\mu)\\Phi(\\frac{f^*-\\mu}{\\sigma}) + \\frac{\\sigma}{\n",
            "\\sqrt{2\\pi}}  e^{-\\epsilon^2/2}|_{-\\infty}^{(f^*-\\mu)/\\sigma} \\\\\n",
            "& = (f^* - \\mu)\\Phi(\\frac{f^*-\\mu}{\\sigma}) + \\sigma \\big(\\phi(\\frac{f^*-\\mu}{\\sigma}) - 0\\big)\\\\\n",
            "& = (f^* - \\mu)\\Phi(\\frac{f^*-\\mu}{\\sigma}) + \\sigma \\phi(\\frac{f^*-\\mu}{\\sigma})\\\\\n",
            "\\end{align} \n",
            "EI(x) = (f^* - \\mu)\\Phi(\\frac{f^*-\\mu}{\\sigma}) + \\sigma \\phi(\\frac{f^*-\\mu}{\\sigma})\n",
            "import input_data\n",
            "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
            "import tensorflow as tf\n",
            "x = tf.placeholder(\"float\",[None,784])\n",
            "w = tf.Variable(tf.zeros([784,10]))\n",
            "b = tf.Variable(tf.zeros([10]))\n",
            "y = tf.nn.softmax(tf.matmul(x,W)+b)\n",
            "y_ = tf.placeholder(\"float\",[None,10])\n",
            "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
            "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
            "init = t.initialize_all_bariables()\n",
            "sess = tf.Session()\n",
            "sess.run(init)\n",
            "for i in range(1000):\n",
            "   batch_xs, batch_ys = mnist.train.next_batch(100)\n",
            "   sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
            "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
            "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
            "print sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
            "y = ax + b \n",
            "J = b^2+((a+b)-0.3)^2+((2a+b)-1.9)^2+((3a+b)-2.8)^2+((4a+b)-4.3)^2\\\\\n",
            "\\\\\n",
            "  =30a^2+20ab-59.4a+5b^2-18.6b+30.03\\left\\{\n",
            "\\begin{array}{ll}\n",
            "0 = 60a+20b-59.4 \\\\\n",
            "0 = 20a+10b-18.6 \n",
            "\\end{array}\n",
            "\\right.\n",
            "\\left\\{\n",
            "\\begin{array}{ll}\n",
            "a = 1.11 \\\\\n",
            "b=-0.36\n",
            "\\end{array}\n",
            "\\right.\n",
            "y = 1.11x - 0.36 \n",
            "\n",
            "radish_plot.py\n",
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "# data set\n",
            "plt.plot([0,0.3,1.9,2.8,4.3], \"bo\") h_{\\theta}(x) = \\theta_{0}x_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} +\\cdots + \\theta_{n}x_{n}J(\\theta_{0},\\theta_{1}, \\cdots,\\theta_{n})= \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)})-y^{(i)})^2 \n",
            "\\theta=\\begin{bmatrix}\n",
            "\\theta_{0} \\\\\n",
            "\\theta_{1} \\\\\n",
            "\\vdots\\\\\n",
            "\\theta_{n} \n",
            "\\end{bmatrix}\n",
            ",\n",
            "　　x=\\begin{bmatrix}\n",
            "x_{0} \\\\\n",
            "x_{1} \\\\\n",
            "\\vdots\\\\\n",
            "x_{n} \n",
            "\\end{bmatrix}\n",
            "h_{\\theta}(x)= \\theta^T x\\\\\n",
            "x^{(i)}=\\begin{bmatrix}\n",
            "x_{0}^{(i)} \\\\\n",
            "x_{1}^{(i)} \\\\\n",
            "\\vdots\\\\\n",
            "x_{n}^{(i)}\n",
            "\\end{bmatrix}\n",
            ",　　\n",
            "X=\\begin{bmatrix}\n",
            "(x^{(1)})^T \\\\\n",
            "(x^{(2)})^T  \\\\\n",
            "\\vdots\\\\\n",
            "(x^{(m)})^T \n",
            "\\end{bmatrix}\n",
            "=\n",
            "\\begin{bmatrix}\n",
            "x_{0}^{(1)} & x_{1}^{(1)} & \\cdots & x_{n}^{(1)} \\\\\n",
            "x_{0}^{(2)} & x_{1}^{(2)} & \\cdots & x_{n}^{(2)} \\\\\n",
            "\\vdots & \\vdots & & \\vdots\\\\\n",
            "x_{0}^{(m)} & x_{1}^{(m)} & \\cdots & x_{n}^{(m)} \\\\\n",
            "\\end{bmatrix}\n",
            ",　　\n",
            "y=\\begin{bmatrix}\n",
            "y^{(1)} \\\\\n",
            "y^{(2)} \\\\\n",
            "\\vdots\\\\\n",
            "y^{(m)}\n",
            "\\end{bmatrix}\n",
            "\\theta =\n",
            "(X^tX)^{-1}X^ty\n",
            "linalg_lstsq.py\n",
            "x = arange(-3, 10, 0.1)\n",
            "y = 3 + 2 * np.cos(x) + (1/2) * x + np.random.normal(0.0, 1.0, len(x)) \n",
            "\n",
            "\n",
            "linalg_lstsq.py\n",
            "n = 3\n",
            "X = np.zeros((len(x), n), float)\n",
            "X[:,0] = 1\n",
            "X[:,1] = np.cos(x)\n",
            "X[:,2] = x\n",
            "\n",
            "\n",
            "linalg_lstsq.py\n",
            "(theta, residuals, rank, s) = linalg.lstsq(X, y)\n",
            "\n",
            "\n",
            "linalg_lstsq.py\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt5 + 3\n",
            "\n",
            "add.py\n",
            "import tensorflow as tf\n",
            "a = tf.constant(5)\n",
            "b = tf.constant(3)\n",
            "added = tf.add(a, b)Tensor(\"Add_1:0\", shape=(), dtype=int32)\n",
            "# 定数\n",
            "a = tf.constant(3)a = tf.placeholder(tf.float32)\n",
            "b = tf.placeholder(tf.float32)\n",
            "added = tf.add(a, b)\n",
            "with tf.Session() as sess:\n",
            "    print sess.run(added, feed_dict = {a: 3.0, b: 5.0})\n",
            "\n",
            "get_data.py\n",
            "import pandas as pd\n",
            "import tensorflow as tf\n",
            "from sklearn import cross_validation\n",
            "classify.py\n",
            "# 設計図作成\n",
            "# プレースホルダー設定\n",
            "X = tf.placeholder(tf.float32, shape = [None, 4])\n",
            "Y = tf.placeholder(tf.float32, shape = [None, 3])X = [None, 4]\\\\\n",
            "W = [4, 3]\\\\\n",
            "Y = [None, 3]\\\\\n",
            "[None, 4] \\times [4, 3] = [None, 3]\\\\\n",
            "\n",
            "placeholder.py\n",
            "# プレースホルダー設定\n",
            "X = tf.placeholder(tf.float32, shape = [None, 4])\n",
            "Y = tf.placeholder(tf.float32, shape = [None, 3])\n",
            "\n",
            "\n",
            "parameter.py\n",
            "# パラメーター設定\n",
            "W = tf.Variable(tf.random_normal([4, 3], stddev=0.35))\n",
            "\n",
            "\n",
            "activate.py\n",
            "# 活性化関数\n",
            "y_ = tf.nn.softmax(tf.matmul(X, W))\n",
            "\n",
            "\n",
            "lost.py\n",
            "# 損失関数\n",
            "cross_entropy = -tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_, Y)\n",
            "\n",
            "\n",
            "train.py\n",
            "# 学習\n",
            "optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
            "train = optimizer.minimize(cross_entropy)\n",
            "\n",
            "\n",
            "execute.py\n",
            "# 実行\n",
            "with tf.Session() as sess:\n",
            "    sess.run(tf.global_variables_initializer())\n",
            "    for i in range(1000):\n",
            "        x = train_data\n",
            "        y = pd.get_dummies(train_target)\n",
            "        print(sess.run(W))Date,Open,High,Low,Close\n",
            "2014/12/31,17702.11914,17713.75977,17450.76953,17450.76953\n",
            "2014/12/30,17702.11914,17713.75977,17450.76953,17450.76953\n",
            "2014/12/29,17914.55078,17914.55078,17525.66016,17729.83984\n",
            "2014/12/26,17778.91016,17843.73047,17769.00977,17818.96094\n",
            " . . . \n",
            "（略）\n",
            "df1 = pd.read_csv('./pandas_date_ex/example1.csv', index_col='Date', parse_dates='Date')\n",
            "df1.head()         # for check\n",
            ">>> type(df1.index[0])\n",
            "pandas.tslib.Timestamp\n",
            "df20 = pd.read_csv('./pandas_date_ex/example2.csv')\n",
            "df20.head()df2 = pd.read_csv('./pandas_date_ex/example2.csv', index_col=0, parse_dates=0)\n",
            "df2.head()f2 = '%d-%b-%y'\n",
            "my_parser = lambda date: pd.datetime.strptime(date, f2)\n",
            "df21 = pd.read_csv('example2.csv', index_col=0, parse_dates=0, \n",
            "                    date_parser=my_parser)\n",
            "df31 = pd.read_csv('./pandas_date_ex/example3.csv', index_col='Date', parse_dates='Date')\n",
            "df31.head()>>> type(df31.index[0])\n",
            "str\n",
            "f3 = '%Y年%m月%d日'\n",
            "my_parser = lambda date: pd.datetime.strptime(date, f3)\n",
            "df3 = pd.read_csv('./pandas_date_ex/example3.csv', index_col=0, parse_dates=0, \n",
            "                    date_parser=my_parser)\n",
            "df3.head()\n",
            "df3[['High', 'Low']].plot(figsize=(8,4), grid=True)import pandas as pd\n",
            "import numpy as np\n",
            "import math\n",
            "import random\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as snsdef _load_data(data, n_prev=30):  \n",
            "    docX, docY = [], [](X_train, y_train), (X_test, y_test) = train_test_split(df[[\"sin_t\"]])\n",
            "from keras.models import Sequential  \n",
            "from keras.layers.core import Dense, Activation  \n",
            "from keras.layers.recurrent import LSTM# 予測\n",
            "predicted = model.predict(X_test)L(\\mathbf{w},b) = \\prod_{ (\\mathbf x_i,t_i) \\in X }^N p(C=t_i \\mid \\mathbf{x}_i) \n",
            "\\begin{align}\n",
            "E(\\mathbf{w},b) &= -\\log L(\\mathbf{w},b)\\\\ \n",
            "&= -\\sum p(C=t_i \\mid \\mathbf{x}_i)\\\\\n",
            "\\end{align}\n",
            "\\mathbf{u}^{(k+1)} = \\mathbf{u}^{(k)} - \\eta \\frac{\\partial E}{\\partial \\mathbf u}\\\\\n",
            "\\mathbf u = \\{\\mathbf w, b \\}\n",
            "\n",
            "octave.m\n",
            "z = X * theta;\n",
            "\n",
            "\n",
            "python.py\n",
            "# numpyを使った方法だと\n",
            "np.dot(theta, X)\n",
            "\n",
            "\n",
            "octave.m\n",
            "h = activate_function(z)\n",
            "grad = 1 / m * (X'*(h-y))\n",
            "\n",
            "\n",
            "python.py\n",
            "h = activate_function(z)\n",
            "grad = 1/m * np.dot(X.T, h-y)\n",
            "\n",
            "\n",
            "@mac\n",
            "$ cd <workspace>\n",
            "$ git clone --recursive https://github.com/dmlc/xgboost\n",
            "$ cd xgboost; cp make/minimum.mk ./config.mk; make -j4\n",
            "$ cd python-package; sudo python setup.py install\n",
            "\n",
            "\n",
            "@ubuntu\n",
            "$ cd <workspace>\n",
            "$ git clone --recursive https://github.com/dmlc/xgboost\n",
            "$ cd xgboost; make -j4\n",
            "$ cd python-package; sudo python setup.py install\n",
            "\n",
            "\n",
            "regressor.py\n",
            "import xgboost as xgb\n",
            "from sklearn.model_selection import GridSearchCV\n",
            "from sklearn.datasets import load_boston\n",
            "from sklearn.metrics import mean_squared_error\n",
            "classifier.py\n",
            "import xgboost as xgb\n",
            "from sklearn.model_selection import GridSearchCV\n",
            "from sklearn.datasets import load_digits\n",
            "from sklearn.metrics import confusion_matrix, classification_reportdf_nile['lag1'] = df_nile['volume'].shift(1) \n",
            "df_nile['lag2'] = df_nile['volume'].shift(2)\n",
            "df_nile['lag3'] = df_nile['volume'].shift(3)from sklearn.ensemble import RandomForestRegressor\n",
            "r_forest = RandomForestRegressor(\n",
            "            n_estimators=100,\n",
            "            criterion='mse',\n",
            "            random_state=1,\n",
            "            n_jobs=-1\n",
            ")\n",
            "r_forest.fit(X_train, y_train)\n",
            "y_train_pred = r_forest.predict(X_train)\n",
            "y_test_pred = r_forest.predict(X_test)\n",
            "def step_forward(rf_fitted, X, in_out_sep=67, pred_len=33):\n",
            "    # predict step by step\n",
            "    nlags = 3\n",
            "    idx = in_out_sep - nlags - 1\\mathcal{T} = \\{ (x_1, y_1), (x_2, y_2), ...(x_N, y_N) \\}\n",
            "Err_{\\mathcal{T}} = E_{X^0, Y^0} [L(Y^0,\\ \\hat{f}(X^0)) \\mid \\mathcal{T}]  \\\\\n",
            "Err_{in} = \\frac{1}{N} \\sum_{i=1}^{N} E_{Y^0} [L(Y^0_i ,\\ \\hat{f}(x_i)) \\mid \\mathcal{T} ]\n",
            "data = pd.read_csv(\"data.csv\", names=['X', 'Y', 'Z'])\n",
            "data.describe()\n",
            "# =>\n",
            "#                 X          Y           Z\n",
            "# count    7.000000   7.000000    7.000000\n",
            "# mean    42.571429  -8.571429   98.714286\n",
            "# std     42.968427  14.920424    8.440266\n",
            "# min      0.000000 -40.000000   88.000000\n",
            "# 25%      5.000000 -10.000000   92.000000\n",
            "# 50%     33.000000  -5.000000  100.000000\n",
            "# 75%     77.500000   0.000000  104.500000\n",
            "# max    100.000000   5.000000  110.000000\n",
            "from pandas.tools.plotting import scatter_matrix\n",
            "plt.figure()\n",
            "scatter_matrix(data)\n",
            "plt.savefig(\"image.png\")\n",
            "data.corr()\n",
            "#=>\n",
            "#           X         Y         Z\n",
            "# X  1.000000  0.300076  0.550160\n",
            "# Y  0.300076  1.000000 -0.545455\n",
            "# Z  0.550160 -0.545455  1.000000\n",
            "# 値を取り出す\n",
            "x = data.ix[:,0].values\n",
            "y = data.ix[:,1].values\n",
            "z = data.ix[:,2].values$ conda update scipy\n",
            "$ pip install GPy\n",
            "$ pip install gpyopt\n",
            "import GPy\n",
            "import GPyOpt\n",
            "import numpy as np \n",
            "def f(x):\n",
            "    '''\n",
            "    今回最適化する非線形関数\n",
            "    '''\n",
            "    return np.cos(1.5*x) + 0.1*x\n",
            "bounds = [{'name': 'x', 'type': 'continuous', 'domain': (0,10)}]\n",
            "myBopt = GPyOpt.methods.BayesianOptimization(f=f, domain=bounds,initial_design_numdata=5,acquisition_type='LCB')\n",
            "myBopt.run_optimization(max_iter=15)\n",
            "print(myBopt.x_opt) #[ 2.05769988]\n",
            "print(myBopt.fx_opt) #[-0.79271554]\n",
            "myBopt.model.model #ベイズ最適化で使っているガウス過程のモデル(GPyのオブジェクト）\n",
            "myBopt.model.model.predict #ガウス過程の回帰の関数\n",
            "myBopt.X,myBopt.Y #サンプリングしたxとy\n",
            "import GPy\n",
            "import GPyOpt\n",
            "import numpy as np print(myBopt.x_opt) #[ 2.0539031  1.       ]\n",
            "print(myBopt.fx_opt) #[-0.7927657]\n",
            "#未インストールの方は\n",
            "pip install seaborn\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "sns.set() \n",
            "%matplotlib inline\n",
            "df = pd.read_csv('House Price/train.csv')\n",
            "df.head()\n",
            "sns.countplot(x='YrSold', data = df);\n",
            "sns.distplot(df.YearBuilt);\n",
            "df.plot(kind='scatter', x='LotFrontage', y='SalePrice');\n",
            "sns.regplot(x=df.LotFrontage, y=df.SalePrice);\n",
            "#数値データの列のみに絞る\n",
            "df_n = df.select_dtypes(include=[np.number])sns.boxplot(x=\"SaleCondition\", y=\"SalePrice\", data=df);\n",
            "categorical_features = df.select_dtypes(include=[np.object])\n",
            "for c in categorical_features:\n",
            "    df[c] = df[c].astype('category')\n",
            "    if df[c].isnull().any():\n",
            "        df[c] = df[c].cat.add_categories(['MISSING'])\n",
            "        df[c] = df[c].fillna('MISSING')plt.figure(figsize=(13, 11))\n",
            "sns.heatmap(df.corr())\n",
            "plt.tight_layout();\n",
            "cols = df.corr().nlargest(10,'SalePrice')['SalePrice'].index\n",
            "cm = np.corrcoef(df[cols].values.T)cols = ['SalePrice', '1stFlrSF', 'PoolArea', 'OverallQual'] #任意の列を指定、さらに増やしてもよい\n",
            "sns.pairplot(df[cols], size=3)\n",
            "plt.tight_layout();\n",
            "$ cd project-directory\n",
            "$ cp -a node_modules/kuromoji/dict public/\n",
            "<template>\n",
            "  <div id=\"app\">\n",
            "    <input id=\"inputText\" v-model=\"inputText\">\n",
            "    <button v-on:click=\"conv\">変換</button>\n",
            "    <p>outputToken is: {{ outputToken }}</p>\n",
            "  </div>\n",
            "</template>\n",
            "import.py\n",
            "##フィッティングに使うもの\n",
            "from scipy.optimize import curve_fit\n",
            "import numpy as np\n",
            "linear.py\n",
            "list_linear_x = range(0,20,2)\n",
            "linear.py\n",
            "sns.pointplot(x=array_x, y=array_y, join=False)\n",
            "\n",
            "\n",
            "fitting.py\n",
            "## フィッティングしたい関数式を関数として定義してやる\n",
            "def linear_fit(x, a, b):\n",
            "    return a*x + b\n",
            "fitting.py\n",
            "array_y_fit = param[0] * array_x + param[1]\n",
            "nonlinear.py\n",
            "list_y = []\n",
            "fitting.py\n",
            "def nonlinear_fit(x,a,b):\n",
            "    return  b * np.exp(x / (a+x)  )\n",
            "draw.py\n",
            "list_y = []\n",
            "for num in array_x:\n",
            "    list_y.append( param[1] * np.exp( num /(param[0] + num) ))\n",
            "Ubuntu\n",
            "sudo apt-get install -y openjdk-8-jdk\n",
            "\n",
            "\n",
            "Mac\n",
            "brew cask install java\n",
            "\n",
            "\n",
            "Ubuntu\n",
            "sudo apt install maven\n",
            "\n",
            "\n",
            "mac\n",
            "brew install maven\n",
            "\n",
            "\n",
            "Ubuntu\n",
            "wget http://ftp.riken.jp/net/apache/spark/spark-1.6.2/spark-1.6.2-bin-hadoop2.6.tgz\n",
            "$ tar zxvf spark-1.6.2-bin-hadoop2.6.tgz\n",
            "$ sudo mv spark-1.6.2-bin-hadoop2.6 /usr/local/\n",
            "$ sudo ln -s /usr/local/spark-1.6.2-bin-hadoop2.6 /usr/local/spark\n",
            "\n",
            "\n",
            "Ubuntu\n",
            "export SPARK_HOME=/usr/local/spark\n",
            "export PATH=$PATH:$SPARK_HOME/bin\n",
            "\n",
            "\n",
            "Mac\n",
            "brew install apache-spark\n",
            "\n",
            "$ spark-shell --master local[*]\n",
            "(中略)\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n",
            "      /_/#spark                                                                                                                                                        \n",
            "export SPARK_HOME=/usr/local/spark/spark-1.6.2-bin-hadoop2.6\n",
            "export PATH=$PATH:$SPARK_HOME/bin\n",
            "#jupyter spark\n",
            "export PYSPARK_PYTHON=$PYENV_ROOT/shims/python #環境に合わせてパスを合わせること\n",
            "export PYSPARK_DRIVER_PYTHON=$PYENV_ROOT/shims/jupyter\n",
            "export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\"\n",
            "source .bashrc\n",
            "pyspark\n",
            "\n",
            "Scala\n",
            "val data = Array(1, 2, 3, 4, 5)\n",
            "val distData = sc.parallelize(data)\n",
            "\n",
            "\n",
            "Java\n",
            "List<Integer> data = Arrays.asList(1, 2, 3, 4, 5);\n",
            "JavaRDD<Integer> distData = sc.parallelize(data);\n",
            "Python\n",
            "data = [1, 2, 3, 4, 5]\n",
            "distData = sc.parallelize(data)\n",
            "\n",
            "\n",
            "Scala\n",
            "val distFile = sc.textFile(\"data.txt\")\n",
            "distFile: org.apache.spark.rdd.RDD[String] = data.txt\n",
            "\n",
            "\n",
            "Java\n",
            "JavaRDD<String> distFile = sc.textFile(\"data.txt\");\n",
            "\n",
            "\n",
            "Python\n",
            "distFile = sc.textFile(\"data.txt\")\n",
            "\n",
            "\n",
            "Scala\n",
            "val lines = sc.textFile(\"data.txt\")\n",
            "val lineLengths = lines.map(s => s.length)\n",
            "val totalLength = lineLengths.reduce((a, b) => a + b)\n",
            "\n",
            "\n",
            "Java\n",
            "JavaRDD<String> lines = sc.textFile(\"data.txt\");\n",
            "JavaRDD<Integer> lineLengths = lines.map(s -> s.length());\n",
            "int totalLength = lineLengths.reduce((a, b) -> a + b);\n",
            "\n",
            "\n",
            "Python\n",
            "lines = sc.textFile(\"data.txt\")\n",
            "lineLengths = lines.map(lambda s: len(s))\n",
            "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
            "\n",
            "\n",
            "Scala\n",
            "object MyFunctions {\n",
            "  def func1(s: String): String = { ... }\n",
            "}\n",
            "Java\n",
            "JavaRDD<String> lines = sc.textFile(\"data.txt\");\n",
            "JavaRDD<Integer> lineLengths = lines.map(new Function<String, Integer>() {\n",
            "  public Integer call(String s) { return s.length(); }\n",
            "});\n",
            "int totalLength = lineLengths.reduce(new Function2<Integer, Integer, Integer>() {\n",
            "  public Integer call(Integer a, Integer b) { return a + b; }\n",
            "});\n",
            "\n",
            "\n",
            "Python\n",
            "\"\"\"MyScript.py\"\"\"\n",
            "if __name__ == \"__main__\":\n",
            "    def myFunc(s):\n",
            "        words = s.split(\" \")\n",
            "        return len(words)\n",
            "Scala\n",
            "var counter = 0\n",
            "var rdd = sc.parallelize(data)\n",
            "Java\n",
            "int counter = 0;\n",
            "JavaRDD<Integer> rdd = sc.parallelize(data);\n",
            "Python\n",
            "counter = 0\n",
            "rdd = sc.parallelize(data)\n",
            "Scala\n",
            "val lines = sc.textFile(\"data.txt\")\n",
            "val pairs = lines.map(s => (s, 1))\n",
            "val counts = pairs.reduceByKey((a, b) => a + b)\n",
            "\n",
            "\n",
            "Java\n",
            "JavaRDD<String> lines = sc.textFile(\"data.txt\");\n",
            "JavaPairRDD<String, Integer> pairs = lines.mapToPair(s -> new Tuple2(s, 1));\n",
            "JavaPairRDD<String, Integer> counts = pairs.reduceByKey((a, b) -> a + b);\n",
            "\n",
            "\n",
            "Python\n",
            "lines = sc.textFile(\"data.txt\")\n",
            "pairs = lines.map(lambda s: (s, 1))\n",
            "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
            "\n",
            "data_file = \"./kddcup.data_10_percent.gz\"\n",
            "#一般的な作成\n",
            "raw_data = sc.textFile(data_file) \n",
            "#並列化作成\n",
            "raw_data = sc.parallelize(data_file) \n",
            "#フィルタ変換\n",
            "normal_raw_data = raw_data.filter(lambda x: 'normal.' in x)\n",
            "#マップ変換\n",
            "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
            "raw_data_sample = raw_data.takeSample(False, 400000, 1234)\n",
            "normal_raw_data = raw_data.filter(lambda x: \"normal.\" in x)\n",
            "#減算\n",
            "attack_raw_data = raw_data.subtract(normal_raw_data)\n",
            "#デカルト積（直積集合）\n",
            "product = protocols.cartesian(services).collect()\n",
            "print \"There are {} combinations of protocol X service\".format(len(product))\n",
            "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
            "model = ALS.train(ratings, rank, numIterations)\n",
            "predictions_all = model.predictAll(sc.parallelize(f_XY)).map(lambda r: ((r[0], r[1]), limitter(r[2]) ))\n",
            "https://github.com/sryza/aas.git\n",
            "git checkout 1st-edition\n",
            "wget http://www.iro.umontreal.ca/~lisa/datasets/profiledata_06-May-2005.tar.gz\n",
            "tar xzvf profiledata_06-May-2005.tar.gz \n",
            "結果\n",
            "profiledata_06-May-2005/\n",
            "profiledata_06-May-2005/artist_data.txt\n",
            "profiledata_06-May-2005/README.txt\n",
            "profiledata_06-May-2005/user_artist_data.txt\n",
            "profiledata_06-May-2005/artist_alias.txt\n",
            "\n",
            "ml/Main.py\n",
            "from ml.Sentiment import Sentiments, Util\n",
            "from ml.Model import LogisticRegression\n",
            "import matplotlib.pyplot as plt\n",
            "ml/Sentiments.py\n",
            "import random\n",
            "import re\n",
            "from itertools import chain\n",
            "from nltk.stem.porter import PorterStemmer\n",
            "from stop_words import get_stop_words\n",
            "from ml.Model import LogisticRegression\n",
            "ml/Main.py\n",
            "sentiments.add_features()\n",
            "sentiments.save('../8_ML/72_sentiment.txt')\n",
            "\n",
            "\n",
            "ml/Sentiments.py\n",
            "class Sentiments:\n",
            "ml/Main.py\n",
            "model = LogisticRegression()\n",
            "model.calc_weights(sentiments=sentiments.all)\n",
            "\n",
            "\n",
            "ml/Model.py\n",
            "import math\n",
            "from collections import defaultdict\n",
            "ml/Mian.py\n",
            "sentiments.add_predict(model.predict)\n",
            "score = sentiments.calc_score()\n",
            "Util.print_score(score, '77. 正解率の計測')\n",
            "\n",
            "\n",
            "ml/Sentiments.py\n",
            "class Sentiments:77. 正解率の計測\n",
            "    予測の正解率: 0.8743200150065654\n",
            "    正例に関する適合率: 0.8564029290944811\n",
            "    再現率: 0.8994560120052523\n",
            "    F1スコア: 0.8774016468435498\n",
            "\n",
            "ml/Main.py\n",
            "sentiments.restore('../8_ML/72_sentiment.txt')  # 未学習のsentimentsをリストアします.\n",
            "score = sentiments.cross_validation(model=LogisticRegression())\n",
            "Util.print_score(score, '78. 5分割交差検定')\n",
            "\n",
            "\n",
            "ml/Sentiments.py\n",
            "class Sentiments:78. 5分割交差検定\n",
            "    予測の正解率: 0.848988423671968\n",
            "    正例に関する適合率: 0.8481575029900081\n",
            "    再現率: 0.852642297684391\n",
            "    F1スコア: 0.8503632552717463\n",
            "\n",
            "ml/Main.py\n",
            "precision_rates = []\n",
            "recall_rates = []\n",
            "thresholds = [t / 20 for t in range(10)]\n",
            "for threshold in thresholds:\n",
            "    sentiments.restore('../8_ML/72_sentiment.txt')  # 未学習のsentimentsをリストアします.\n",
            "    score = sentiments.cross_validation(model=LogisticRegression(), threshold=threshold)\n",
            "    precision_rates.append(score['precision_rate'])\n",
            "    recall_rates.append(score['recall_rate'])\n",
            "print(thresholds)\n",
            "print(precision_rates)\n",
            "print(recall_rates)\n",
            "インストール先\n",
            "C:\\Python\n",
            "\n",
            "\n",
            "Pythonのバージョンを確認してみる\n",
            "Microsoft Windows [Version 10.0.17134.523]\n",
            "(c) 2018 Microsoft Corporation. All rights reserved.\n",
            "現時点で入っているライブラリをリストする（何も入ってない）\n",
            "C:\\Users\\yoshi> pip freeze\n",
            "C:\\Users\\yoshi>\n",
            "\n",
            "\n",
            "とりあえずpandasとpyodbcは入れとけ\n",
            "C:\\Users\\yoshi> python -m pip install --upgrade pip\n",
            "C:\\Users\\yoshi> pip install pandas\n",
            "C:\\Users\\yoshi> pip install pyodbc\n",
            "\n",
            "\n",
            "現時点で入っているライブラリをリストする\n",
            "C:\\Users\\yoshi>pip freeze\n",
            "  numpy==1.16.1\n",
            "  pandas==0.24.0\n",
            "  pyodbc==4.0.25\n",
            "  python-dateutil==2.7.5\n",
            "  pytz==2018.9\n",
            "  six==1.12.0\n",
            "\n",
            "\n",
            "せっかくだから俺はこのディレクトリを作業場所に選ぶぜ\n",
            "C:\\Users\\yoshi> cd/d \"D:\\OneDrive\\ドキュメント\\Project\\Python\"\n",
            "D:\\OneDrive\\ドキュメント\\Project\\Python>\n",
            "\n",
            "\n",
            "main.py\n",
            "print(\"Hello world!\")\n",
            "\n",
            "\n",
            "やったね！\n",
            "D:\\OneDrive\\ドキュメント\\Project\\Python> python main.py\n",
            "  Hello world!\n",
            "\n",
            "\n",
            "現在のディレクトリ状況\n",
            "D:\\OneDrive\\ドキュメント\\Project\\Python> tree /F\n",
            "  D:.\n",
            "  │  main.py\n",
            "  └─ lib\n",
            "       PyUtils.py\n",
            "\n",
            "\n",
            "main.py\n",
            "import lib.PyUtils\n",
            "lib/PyUtils.py\n",
            "class HelloClass:\n",
            "    def HelloWorldByPyUtils(self):\n",
            "        print(\"Hello PyUtils\")\n",
            "\n",
            "\n",
            "テスト\n",
            "D:\\OneDrive\\ドキュメント\\Project\\Python> python main.py\n",
            "  Hello world!\n",
            "  Hello PyUtils\n",
            "\n",
            "\n",
            "main.py\n",
            "from lib.PyUtils import *\n",
            "lib/PyUtils.py\n",
            "class HelloClass:\n",
            "    def HelloWorldByPyUtils(self):\n",
            "        print(\"Hello PyUtils\")\n",
            "\n",
            "\n",
            "テスト\n",
            "D:\\OneDrive\\ドキュメント\\Project\\Python> python main.py\n",
            "  Hello world!\n",
            "  Hello PyUtils\n",
            "\n",
            "\n",
            "CREATETABLE.sql\n",
            "CREATE TABLE 顧客 (\n",
            "      [所属] NVARCHAR(20) NULL\n",
            "    , [氏名] NVARCHAR(20) NULL\n",
            "    , [氏名（かな）] NVARCHAR(20) NULL\n",
            "    , [メールアドレス] NVARCHAR(20) NULL\n",
            "    , [住所] TEXT NULL\n",
            "    , [誕生日] NVARCHAR(20) NULL\n",
            ");\n",
            "\n",
            "\n",
            "main.py\n",
            "import pyodbc\n",
            "import pandas as pd\n",
            "import lib.PyUtils\n",
            "lib/PyUtils.py\n",
            "import pyodbc\n",
            "import pandas as pd\n",
            "テスト\n",
            "D:\\OneDrive\\ドキュメント\\Project\\Python> python main.py\n",
            "  リクルート,本多 守,ほんだ まもる,1@gmail.com,北海道,2019/2/1\n",
            "  リクルート,早坂 健吾,はやさか けんご,2@gmail.com,青森県,2019/2/2\n",
            "  リクルート,立石 晋,たていし すすむ,3@gmail.com,岩手県,2019/2/3\n",
            "  リクルート,鷲尾 崇之,わしお たかゆき,4@gmail.com,宮城県,2019/2/4\n",
            "  リクルート,郭 卓司,かく たくじ,5@gmail.com,秋田県,2019/2/5\n",
            "  リクルート,大串 幹男,おおぐし みきお,6@gmail.com,山形県,2019/2/6\n",
            "  リクルート,島貫 典彦,しまぬき のりひこ,7@gmail.com,福島県,2019/2/7\n",
            "  リクルート,古城 英孝,こじょう ひでたか,8@gmail.com,茨城県,2019/2/8\n",
            "  リクルート,鯨井 健史,くじらい けんじ,9@gmail.com,栃木県,2019/2/9\n",
            "  リクルート,来栖 源太,くるす げんた,10@gmail.com,群馬県,2019/2/10\n",
            "  リクルート,塩井 崇浩,しおい たかひろ,11@gmail.com,埼玉県,2019/2/11\n",
            "  リクルート,諸田 清人,もろた きよひと,12@gmail.com,千葉県,2019/2/12\n",
            "  リクルート,神宮寺 善文,じんぐうじ よしふみ,13@gmail.com,東京都,2019/2/13\n",
            "  リクルート,池水 了,いけみず さとる,14@gmail.com,神奈川県,2019/2/14\n",
            "  リクルート,香椎 悠祐,かしい ゆうすけ,15@gmail.com,新潟県,2019/2/15\n",
            "  リクルート,真 修吾,ま しゅうご,16@gmail.com,富山県,2019/2/16\n",
            "  リクルート,西薗 浩彦,にしぞの ひろひこ,17@gmail.com,石川県,2019/2/17\n",
            "  リクルート,鑓田 伸洋,やりた のぶひろ,18@gmail.com,福井県,2019/2/18\n",
            "  リクルート,根塚 政晴,ねづか まさはる,19@gmail.com,山梨県,2019/2/19\n",
            "  リクルート,中室 一起,なかむろ かずき,20@gmail.com,長野県,2019/2/20\n",
            "  リクルート,三俣 健士,みつまた けんじ,21@gmail.com,岐阜県,2019/2/21\n",
            "  リクルート,目崎 千裕,めさき ちひろ,22@gmail.com,静岡県,2019/2/22\n",
            "  リクルート,滝田 祐吾,たきだ ゆうご,23@gmail.com,愛知県,2019/2/23\n",
            "  リクルート,八本 真実,やもと まこと,24@gmail.com,三重県,2019/2/24\n",
            "  リクルート,織井 章広,おりい あきひろ,25@gmail.com,滋賀県,2019/2/25\n",
            "  リクルート,能丸 裕孝,のうまる ひろたか,26@gmail.com,京都府,2019/2/26\n",
            "  リクルート,順 恭,じゅん やすし,27@gmail.com,大阪府,2019/2/27\n",
            "  リクルート,幸島 樹一郎,こうじま きいちろう,28@gmail.com,兵庫県,2019/2/28\n",
            "  リクルート,下津浦 光也,しもつうら みつや,29@gmail.com,奈良県,2019/3/1\n",
            "  リクルート,安土 たかゆき,あんど たかゆき,30@gmail.com,和歌山県,2019/3/2\n",
            "main.py\n",
            "import pyodbc\n",
            "import pandas as pd\n",
            "import lib.PyUtils\n",
            "Python3.7Console\n",
            "Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 23:09:28) [MSC v.1916 64 bit (AMD64)] on win32\n",
            "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
            ">>> import numpy as np\n",
            ">>> import pandas as pd\n",
            "naに対して中央値で埋める（ただしグループ別に異なる値が入る）\n",
            "# LotFrontageのNAを、近隣の家（Neighborhood）単位でLotFrontageのmedianを取って、その値で埋める\n",
            "print(data['LotFrontage'].isnull().sum()) #欠損値259\n",
            "data[\"LotFrontage\"] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].apply(lambda x: x.fillna(x.median()))\n",
            "print(data['LotFrontage'].isnull().sum()) #欠損値0\n",
            "\n",
            "\n",
            "lambdaについて\n",
            "# lambda 仮引数: 返り値\n",
            "# この lambda関数 で使用する引数を x と定義する\n",
            "# x は グループ別に split されたうちの 1つ の明細が全量引き渡される\n",
            "# つまるところ、全量の na に対して中央値で埋める となる\n",
            "lambda x: x.fillna(x.median())\n",
            "\n",
            "\n",
            "チートシート1（寄せ集めなので変数名の統制は取れていない）\n",
            "# ◆インポート\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "%precision 3 # 小数点3桁表記\n",
            "%matplotlib inline # jupyterのみ\n",
            "sns.set()\n",
            "チートシート2（寄せ集めなので変数名の統制は取れていない）\n",
            "# ランダムフォレストを実行し、検証用データでMSEを算出\n",
            "model_random_forest = RandomForestRegressor(criterion='mse').fit(X_train, y_train)\n",
            "mean_squared_error(y_test, model_random_forest.predict(X_test))\n",
            "> 1322565474.5486445\n",
            "# 何だよこの数字！？って思ったら「検証用データと予測値の平均二条の差」で検算できる\n",
            "# ◆mse（平均二乗誤差） 対 手動mse による検算で納得感を得る\n",
            "np.mean((y_test - model_random_forest.predict(X_test))**2)\n",
            "> 1322565474.5486445\n",
            "# modelを作ったら y と y^ を散布図に並べる\n",
            "fig, ax = plt.subplots()\n",
            "ax.scatter(x = y_test, y = model_random_forest.predict(X_test))\n",
            "plt.ylabel('predict', fontsize=13)\n",
            "plt.xlabel('SalePrice', fontsize=13)\n",
            "plt.show()\n",
            "\n",
            "\n",
            "チートシート3（寄せ集めなので変数名の統制は取れていない）\n",
            "# モジュールの読み込み\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "import matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "from sklearn.cluster import KMeans\n",
            "from sklearn.preprocessing import MinMaxScalerimport numpy as np\n",
            "import matplotlib.pyplot as pltimport numpy as np\n",
            "import matplotlib.pyplot as plt    #plt.plot(x_values, log_values , \"-\", color=\"red\") # red\n",
            "    plt.plot(x_values, diff_values , \"-\", color=\"red\") # red\n",
            "import numpy as np\\begin{align}\n",
            "e &= \\lim_{n \\to \\infty} \\left(1 + \\frac{1}{n} \\right )^n\\\\\n",
            "  &= 1 + {}_n C _1 \\cdot 1 \\cdot \\frac{1}{n} + {}_n C _2 \\cdot 1 \\cdot \\frac{1^2}{n^2} \\cdot {}_n C _3 \\cdot 1 \\cdot \\frac{1^3}{n^3} \\cdots \\cdots\\\\\n",
            "  &= 1 + \\frac{n}{1!} \\cdot 1 \\cdot \\frac{1}{n} + \\frac{n(n-1)}{2!} \\cdot 1 \\frac{1}{n^2} \\cdot \\frac{n(n-1)(n-2)}{3!} \\cdot 1 \\cdot \\frac{1}{n^3} \\cdots \\cdots\\\\\n",
            "  &= 1 + \\frac{1}{1!} + \\frac{1}{2!} + \\frac{1}{3!} + \\frac{1}{4!} + \\frac{1}{5!} \\cdots \\cdots \\frac{1}{n!}\\\\\n",
            "  & \\frac{1}{2!}以降は、幾ら足しても1より大きくなることはない。式全体は3より小さくなる\\\\\n",
            "  &= 1 + 1 + 0.7182818284590 \\cdots\\\\\n",
            "  &= 2.7182818284590 \\cdots\\\\\n",
            "\\end{align}\n",
            "\\begin{align}\n",
            "\\displaystyle\n",
            "y&=a^x\\\\\n",
            "y^\\prime &= \\lim_{h \\to 0} \\frac{a^{x+h}-a^x}{h}\\\\\n",
            "&=\\lim_{h \\to 0} \\frac{a^x(a^h-1)}{h}\n",
            "\\end{align}\n",
            "\\begin{align}\n",
            "\\displaystyle\n",
            "y&=\\log_ax\\\\\n",
            "y^\\prime &= \\lim_{h \\to 0} \\frac{\\log_a(x+h)-\\log_ax}{h}\\\\\n",
            "&\\frac{1}{h} を前にだし、対数関数の性質より引き算は割り算に直せる。\\\\\n",
            "&=\\lim_{h \\to 0} \\frac{1}{h}\\log_a \\frac{x+h}{x}\\\\\n",
            "&=\\lim_{h \\to 0} \\frac{1}{h}\\log_a \\left (1+\\frac{h}{x} \\right)\n",
            "\\end{align}\n",
            "[,0]    index\n",
            "[,1]    speed   numeric Speed (mph)\n",
            "[,2]    dist    numeric Stopping distance (ft)\n",
            "\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "data= np.loadtxt('cars.csv',delimiter=',',skiprows=1)\n",
            "data[:,1] = map(lambda x: x * 1.61, data[:,1])    # mph から km/h に変換\n",
            "data[:,2] = map(lambda y: y * 0.3048, data[:,2])  # ft から  m に変換\n",
            "\n",
            "fig = plt.figure(figsize=(10,6))\n",
            "ax = fig.add_subplot(111)\n",
            "ax.set_title(\"Stopping Distances of Cars\")\n",
            "ax.set_xlabel(\"speed(km/h)\")\n",
            "ax.set_ylabel(\"distance(m)\")\n",
            "plt.scatter(data[:,1],data[:,2])\n",
            "\n",
            "distance = \\alpha * speed + \\beta\n",
            "# y = 0.74x -5 の直線 ((0,-5),(50,32)を通る)\n",
            "x = [0,  50]\n",
            "y = [-5, 32]\n",
            "plt.plot(x,y)\n",
            "# line: y = 0.74 -5\n",
            "x = [0,  50]\n",
            "y = [-5, 32]\n",
            "plt.plot(x,y)\n",
            "# line: y = 0.54x -5\n",
            "x = [0,  50]\n",
            "y = [-5, 72]\n",
            "plt.plot(x,y)\n",
            "# line: y = 0.74x +15\n",
            "x = [0,  50]\n",
            "y = [15, 52]\n",
            "plt.plot(x,y)\n",
            "i = 18\n",
            "x_i = data[i,1]\n",
            "y_i = data[i,2]\n",
            "y_hat = x_i*0.74-5\n",
            "ax.set_ylim(0,y_i+3)\n",
            "ax.set_xlim(x_i-5,x_i+5)\n",
            "S = \\sum_i^n\\epsilon_i^2=\\sum_i^n (y_i-\\hat{y}_i )^2\n",
            "\\hat{y}_i = \\alpha  x_i + \\beta\n",
            "S = \\sum_i^n \\epsilon_i^2 = \\sum_i^n  ( y_i-\\alpha x_i - \\beta )^2 \n",
            "S(\\alpha) = \\left( \\sum_i^n x_i^2 \\right) \\alpha^2\n",
            " + 2\\left( \\sum_i^n (x_i\\beta - x_i y_i ) \\right) \\alpha \n",
            " + n\\beta^2 - 2\\beta\\sum_i^n y_i + \\sum_i^n y_i^2\n",
            "\n",
            "S(\\alpha) = \\left( \\sum_i^n x_i^2 \\right) \\alpha^2\n",
            " - 2\\left( \\sum_i^n x_i y_i \\right) \\alpha \n",
            " + \\sum_i^n y_i^2  　...　 (if \\beta = 0 )\n",
            "sum_x2 = np.sum([x ** 2 for x in data[:,1]])   # \\sum x_i^2 \n",
            "sum_y2 = np.sum([x ** 2 for y in data[:,2]])   # \\sum y_i^2 \n",
            "sum_xy = data[:,1].dot(data[:,2])              # \\sum x_i y_i\n",
            "S(\\alpha) ≒ 34288 \\alpha^2 - 37768 \\alpha + 11604 x1 = np.linspace(-1,5,200)\n",
            "x1_2 = np.array([x ** 2 for x in x1])\n",
            "S(\\beta) = n\\beta^2\n",
            "+ 2 \\left( \\sum_i^n (x_i\\alpha - y_i) \\right) \\beta\n",
            "+ \\alpha^2\\sum_i^n x_i^2 - 2\\alpha \\sum_i^n x_iy_i + \\sum_i^n y_i^2\n",
            "S(\\beta) = n\\beta^2\n",
            "- 2 \\left( \\sum_i^n y_i \\right) \\beta + \\sum_i^n y_i^2\n",
            "  　...　 (if \\alpha = 0 )\n",
            "\n",
            "sum_x = np.sum(data[:,1])\n",
            "print sum_x\n",
            ">>> 1239.7\n",
            "S(\\beta) ≒ 50\\beta^2 - 1310 \\beta + 11604\n",
            "x1 = np.arange(-100,100,0.01)\n",
            "x1_2 = np.array([x ** 2 for x in x1])\n",
            "n = len(data[:,2])$ python --version\n",
            "Python 3.6.2 :: Anaconda, Inc.\n",
            "$ pip --version\n",
            "pip 9.0.1 from /root/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages (python 3.6)\n",
            "$ python -m pip list | grep Django\n",
            "Django (1.11.7)\n",
            "$ python -m pip list | grep scikit-learn\n",
            "scikit-learn (0.19.0)\n",
            "<canvas id=\"myChart\" width=\"400\" height=\"400\"></canvas>\n",
            "<script>\n",
            "var ctx = document.getElementById(\"myChart\").getContext('2d');\n",
            "var myChart = new Chart(ctx, {\n",
            "    type: 'bar',\n",
            "    data: {\n",
            "        labels: [\"Red\", \"Blue\", \"Yellow\", \"Green\", \"Purple\", \"Orange\"],\n",
            "        datasets: [{\n",
            "            label: '# of Votes',\n",
            "            data: [12, 19, 3, 5, 2, 3],\n",
            "            backgroundColor: [\n",
            "                'rgba(255, 99, 132, 0.2)',\n",
            "                'rgba(54, 162, 235, 0.2)',\n",
            "                'rgba(255, 206, 86, 0.2)',\n",
            "                'rgba(75, 192, 192, 0.2)',\n",
            "                'rgba(153, 102, 255, 0.2)',\n",
            "                'rgba(255, 159, 64, 0.2)'\n",
            "            ],\n",
            "            borderColor: [\n",
            "                'rgba(255,99,132,1)',\n",
            "                'rgba(54, 162, 235, 1)',\n",
            "                'rgba(255, 206, 86, 1)',\n",
            "                'rgba(75, 192, 192, 1)',\n",
            "                'rgba(153, 102, 255, 1)',\n",
            "                'rgba(255, 159, 64, 1)'\n",
            "            ],\n",
            "            borderWidth: 1\n",
            "        }]\n",
            "    },\n",
            "    options: {\n",
            "        scales: {\n",
            "            yAxes: [{\n",
            "                ticks: {\n",
            "                    beginAtZero:true\n",
            "                }\n",
            "            }]\n",
            "        }\n",
            "    }\n",
            "});\n",
            "</script>\n",
            "\n",
            "myapp/train.py\n",
            "from ...models import User$ python manage.py train # 学習\n",
            "$ python manage.py predict # 予測\n",
            "> x <- 1:10\n",
            "> x\n",
            " [1]  1  2  3  4  5  6  7  8  9 10\n",
            "> nrow(x)\n",
            "NULL\n",
            "> ncol(x)\n",
            "NULL\n",
            "> nrow(cars)\n",
            "[1] 50\n",
            "> ncol(cars)\n",
            "[1] 2\n",
            "> dim(cars)\n",
            "[1] 50  2\n",
            "> str(cars)\n",
            "'data.frame':   50 obs. of  2 variables:\n",
            " $ speed: num  4 4 7 7 8 9 10 10 10 11 ...\n",
            " $ dist : num  2 10 4 22 16 10 18 26 34 17 ...\n",
            "> tail(cars, n = 5)\n",
            "   speed dist\n",
            "46    24   70\n",
            "47    24   92\n",
            "48    24   93\n",
            "49    24  120\n",
            "50    25   85\n",
            "> str(iris)\n",
            "'data.frame':   150 obs. of  5 variables:\n",
            " $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n",
            " $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n",
            " $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n",
            " $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n",
            " $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n",
            "> attributes(iris$Species)\n",
            "$levels\n",
            "[1] \"setosa\"     \"versicolor\" \"virginica\" > plot(x = cars$speed, y = cars$dist, xlab = \"speed\", ylab　=　\"dist\")\n",
            "> plot(cars)\n",
            "> str(trees)\n",
            "'data.frame':   31 obs. of  3 variables:\n",
            " $ Girth : num  8.3 8.6 8.8 10.5 10.7 10.8 11 11 11.1 11.2 ...\n",
            " $ Height: num  70 65 63 72 81 83 66 75 80 75 ...\n",
            " $ Volume: num  10.3 10.3 10.2 16.4 18.8 19.7 15.6 18.2 22.6 19.9 ...\n",
            "> plot(trees)\n",
            "> pairs(trees)\n",
            "> str(morley)\n",
            "'data.frame':   100 obs. of  3 variables:\n",
            " $ Expt : int  1 1 1 1 1 1 1 1 1 1 ...\n",
            " $ Run  : int  1 2 3 4 5 6 7 8 9 10 ...\n",
            " $ Speed: int  850 740 900 1070 930 850 950 980 980 880 ...\n",
            "> boxplot(Speed ~ Expt, data = morley, xlab = \"Expt\", ylab = \"Speed\")\n",
            "> levels(iris$Species)\n",
            "[1] \"setosa\"     \"versicolor\" \"virginica\" \n",
            "> tapply(warpbreaks$breaks, warpbreaks[,-1], sum)\n",
            "    tension\n",
            "wool   L   M   H\n",
            "   A 401 216 221\n",
            "   B 254 259 169\n",
            "> xtabs(warpbreaks)\n",
            "    tension\n",
            "wool   L   M   H\n",
            "   A 401 216 221\n",
            "   B 254 259 169\n",
            "> stats::ftable(Titanic)\n",
            "                   Survived  No Yes\n",
            "Class Sex    Age                   \n",
            "1st   Male   Child            0   5\n",
            "             Adult          118  57\n",
            "      Female Child            0   1\n",
            "             Adult            4 140\n",
            "2nd   Male   Child            0  11\n",
            "             Adult          154  14\n",
            "      Female Child            0  13\n",
            "             Adult           13  80\n",
            "3rd   Male   Child           35  13\n",
            "             Adult          387  75\n",
            "      Female Child           17  14\n",
            "             Adult           89  76\n",
            "Crew  Male   Child            0   0\n",
            "             Adult          670 192\n",
            "      Female Child            0   0\n",
            "             Adult            3  20\n",
            "> titanic <- vcdExtra::expand.dft(Titanic)\n",
            "> nrow(titanic)\n",
            "[1] 2201\n",
            "> head(titanic, n = 3)\n",
            "  Class  Sex   Age Survived\n",
            "1   3rd Male Child       No\n",
            "2   3rd Male Child       No\n",
            "3   3rd Male Child       No\n",
            "library(dplyr)\n",
            "library(knitr)\\phi_i(x)=x^i\\hspace{1.0em}(i=0,\\cdots,M-1)\n",
            "\\phi_i(x)=\\exp\\left\\{-\\frac{(x-c_i)^2}{2s^2}\\right\\}\\hspace{1.0em}(i=0,\\cdots,M-1)\n",
            "f(x)= \\sum^{M-1}_{i=0}w_i\\phi_i(x) = w^T\\phi(x) \\hspace{1.0em} （式1）\n",
            "E(w)=\\frac{1}{2}\\sum^N_{n=1}(f(x_n)-t_n)^2 \n",
            "=\\frac{1}{2}\\sum^N_{n=1}\\left(\\sum^{M-1}_{i=0}w_{i}x^{i}_{n}-t_n\\right)^2 \\hspace{1.0em} （式2）\n",
            "%matplotlib inline\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\\phi_i(x)=x^i\\hspace{1.0em}(i=0,\\cdots,M-1)\n",
            "f(x)=w_{1}+w_{2}x+w_{3}x^2+w_{4}x^3\n",
            "\\frac{\\partial E(w)}{\\partial w_m}=\\sum^N_{n=1}\\phi_m(x_n)\\left(\\sum^M_{j=1}w_j\\phi_j(x_n)-t_n\\right)=0 \\hspace{1.0em}(m=1,\\cdots,M) \\hspace{1.0em} （式3）\n",
            "w = (\\phi^T\\phi)^{-1}\\phi^{T}t \\hspace{1.0em} （式4）\n",
            "    \\phi=\\left(\n",
            "    \\begin{array}{ccc} \n",
            "        \\phi_{1}(x_1) & \\phi_{2}(x_1) & \\cdots & \\phi_{M}(x_1)  \\\\ \n",
            "        \\vdots & \\vdots& & \\vdots      \\\\ \n",
            "        \\phi_{1}(x_N) & \\phi_{2}(x_N) & \\cdots & \\phi_{M}(x_N)\\\\ \n",
            "    \\end{array}  \n",
            "    \\right) \\hspace{1.0em}（式5）\n",
            "# 特徴関数φに多項式基底を用いた場合\n",
            "def phi(x):\n",
            "    return [1, x, x**2, x**3]#上記の計算で得られたwを出力する\n",
            "w\n",
            "array([ -0.14051447,  11.51076413, -33.6161329 ,  22.33919877])\n",
            "\\phi_i(x)=\\exp\\left\\{-\\frac{(x-c_i)^2}{2s^2}\\right\\}\\hspace{1.0em}(i=0,\\cdots,M-1)\\hspace{1.0em}（式6） \n",
            "%matplotlib inline\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as pltp(t|w,x) = N(t|\\mu,\\beta^{-1})=\\frac{1}{Z}\\exp\\left\\{-\\frac{1}{2}\\beta(t-\\mu)^2\\right\\} \\hspace{1.0em}（式7）\\\\\n",
            "ただし、\\mu=f(x)=\\sum^M_{i=1}w_i\\phi_i(x), Z=\\sqrt{2\\pi\\beta^{-1}}\n",
            "\\ln{p(T|w,X)}=-\\frac{1}{2}\\beta\\sum^N_{n=1}\\left(t_n-\\sum^{M}_{i=1}w_{i}\\phi_i(x_n)\\right)^2+C \\hspace{1.0em} （式8）\n",
            "p(w|t)=p(t|w)p(w)/p(t) \\\\\n",
            "p(t)=\\int p(t,w)dw=\\int p(t|w)p(w)dw\n",
            "p(w|t)=\\frac{1}{Z'}\\exp\\left\\{-\\frac{1}{2}(w-m)^T\\sum^{-1}(w-m)\\right\\} \\hspace{1.0em} （式9）\\\\\n",
            "ただし \\\\\n",
            "m=\\beta t\\sum\\phi(x)\\\\\n",
            "\\sum^{-1}=\\alpha I+\\beta\\phi(x)\\phi(x)^T\n",
            "N(w|\\mu,\\sum)=\\frac{1}{Z}\\exp\\left\\{(-(w-\\mu)^T\\sum^{-1}(w-\\mu)\\right\\}\n",
            "p(w|t)=N(w|m,\\sum)\n",
            "p(w|t,x)=N(w|m_N,\\sum_N) \\hspace{1.0em} （式10）  \\\\\n",
            "ただし \\\\\n",
            "m_N=\\beta \\sum_N\\phi^{T}t\\\\\n",
            "\\sum^{-1}_N=\\alpha I+\\beta\\phi^T\\phi\n",
            "%matplotlib inline\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt#共分散行列Sigma_Nを見やすく表示\n",
            "print \"\\n\".join(' '.join(\"% .2f\" % x for x in y) for y in Sigma_N)\n",
            " 2.94 -2.03 -0.92 -1.13 -1.28 -1.10 -1.21 -1.14 -1.23 -1.06 -0.96 -2.08\n",
            "-2.03  2.33 -0.70  1.95  0.13  1.02  0.85  0.65  0.98  0.70  0.65  1.44\n",
            "-0.92 -0.70  2.52 -1.86  1.97 -0.29  0.42  0.59  0.13  0.40  0.32  0.63\n",
            "-1.13  1.95 -1.86  3.02 -1.66  1.50  0.17  0.29  0.73  0.33  0.36  0.82\n",
            "-1.28  0.13  1.97 -1.66  2.82 -1.11  1.39  0.22  0.55  0.49  0.40  0.92\n",
            "-1.10  1.02 -0.29  1.50 -1.11  2.39 -1.35  1.72 -0.29  0.53  0.46  0.69\n",
            "-1.21  0.85  0.42  0.17  1.39 -1.35  2.94 -2.06  2.39 -0.02  0.25  1.01\n",
            "-1.14  0.65  0.59  0.29  0.22  1.72 -2.06  4.05 -2.72  1.43  0.37  0.67\n",
            "-1.23  0.98  0.13  0.73  0.55 -0.29  2.39 -2.72  3.96 -1.41  1.23  0.59\n",
            "-1.06  0.70  0.40  0.33  0.49  0.53 -0.02  1.43 -1.41  3.30 -2.27  2.05\n",
            "-0.96  0.65  0.32  0.36  0.40  0.46  0.25  0.37  1.23 -2.27  3.14 -0.86\n",
            "-2.08  1.44  0.63  0.82  0.92  0.69  1.01  0.67  0.59  2.05 -0.86  2.45\n",
            "# 正規分布の確率密度関数\n",
            "def normal_dist_pdf(x, mean, var): \n",
            "    return np.exp(-(x-mean) ** 2 / (2 * var)) / np.sqrt(2 * np.pi * var)# coding: utf-8\n",
            "import numpy as np\n",
            "import math\n",
            "from itertools import izip\n",
            "from sklearn.metrics import accuracy_score, recall_score# coding: utf-8\n",
            "import numpy as np\n",
            "import math\n",
            "from itertools import izip\n",
            "from sklearn.metrics import accuracy_score, recall_scoreimport numpy as np\n",
            "from itertools import izip\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.metrics import recall_score\n",
            "ex1.py\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn import linear_modelimport numpy as np\n",
            "import matplotlib.pyplot as pltz = ax + by + c\n",
            "x = [9.83, -9.97, -3.91, -3.94, -13.67, -14.04, 4.81, 7.65, 5.50, -3.34]\n",
            "y = [-5.50, -13.53, -1.23, 6.07, 1.94, 2.79, -5.43, 15.57, 7.26, 1.34]\n",
            "z = [635.99, 163.78, 86.94, 245.35, 1132.88, 1239.55, 214.01, 67.94, -1.48, 104.18]z = -30.0x + 3.55y + 322\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.feature_selection import VarianceThresholdnull_cols = [col for col in data.columns if data[col].isnull().sum() > 0]\n",
            "print(null_cols)\n",
            "-> []\n",
            "X_train, X_test, y_train, y_test = train_test_split(\n",
            "    data.drop(['ID', 'target'], axis=1),\n",
            "    data['target'],\n",
            "    test_size=0.2,\n",
            "    random_state=2018\n",
            ")\n",
            "# 分散が0（すべて同じ値）のデータは削除します\n",
            "sel = VarianceThreshold(threshold=0)\n",
            "sel.fit(X_train)# numpy に変換したい場合はこちら\n",
            "X_train = sel.transform(X_train)\n",
            "X_test = sel.transform(X_test)sel = VarianceThreshold(threshold=0.1) # 99%が同じデータのもの\n",
            "sel.fit(X_train)# numpy に変換したい場合はこちら\n",
            "X_train = sel.transform(X_train)\n",
            "X_test = sel.transform(X_test)# indexとcolumnsを入れ替える\n",
            "X_train_T = X_train.Tthreshold = 0.8# 使用するデータは直前までのFilter Methodを適用させたものが対象です# 使用するモジュール\n",
            "from sklearn.feature_selection import mutual_info_regression\n",
            "from sklearn.feature_selection import SelectKBest, SelectPercentile# Santandarのデータには適用できないので、簡単な紹介のみ# Santandarのデータには適用できないので、簡単な紹介のみfrom sklearn.tree import DecesionTreeRegressor\n",
            "from sklearn.metrics import mean_squared_error, roc_auc_scoreimport numpy as np\n",
            "import pandas as pd\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.feature_selection import VarianceThresholdsfs1 = SFS(RandomForestClassifier(n_jobs=4),\n",
            "           k_features=10,   # いくつの特徴量を出力させたいのか\n",
            "           forward=True,\n",
            "           floating=False,\n",
            "           verbose=2,\n",
            "           scoring='roc_auc', # 'r2' なども選択できる。\n",
            "           cv=3)clf = RandomForestClassifier(n_estimators=200, random_state=2018, max_depth=4)\n",
            "clf.fit(X_train[selected_feat], y_train)\n",
            "print(roc_auc_score(y_train, clf.predict_proba(X_train[selected_feat])[:, 1]))\n",
            "print(roc_auc_score(y_test, clf.predict_proba(X_test[selected_feat])[:, 1]))\n",
            "sfs1 = SFS(RandomForestClassifier(n_jobs=4),\n",
            "           k_features=15,   # いくつの特徴量を出力させたいのか\n",
            "           forward=False,   # False -> Step Backward\n",
            "           floating=False,\n",
            "           verbose=2,\n",
            "           scoring='roc_auc', # 'r2' なども選択できる。\n",
            "           cv=3)clf = RandomForestClassifier(n_estimators=200, random_state=2018, max_depth=4)\n",
            "clf.fit(X_train[selected_feat], y_train)\n",
            "print(roc_auc_score(y_train, clf.predict_proba(X_train[selected_feat])[:, 1]))\n",
            "print(roc_auc_score(y_test, clf.predict_proba(X_test[selected_feat])[:, 1]))\n",
            "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFSfrom sklearn.linear_model import Lasso\n",
            "from sklearn.feature_selection import SelectFromModel\n",
            "from sklearn.preprocessing import StandardScalerfrom sklearn.ensemble import RandomForestRegressor\n",
            "from sklearn.feature_selection import SelectFromModelimport pandas as pd\n",
            "stock = pd.read_csv('stock_px.csv', parse_dates=True, index_col=0)\n",
            "stock.head(10) # 先頭の 10 件のみ表示\n",
            "# =>\n",
            "#             AAPL   MSFT    XOM     SPX\n",
            "# 2003-01-02  7.40  21.11  29.22  909.03\n",
            "# 2003-01-03  7.45  21.14  29.24  908.59\n",
            "# 2003-01-06  7.45  21.52  29.96  929.01\n",
            "# 2003-01-07  7.43  21.93  28.95  922.93\n",
            "# 2003-01-08  7.28  21.31  28.83  909.93\n",
            "# 2003-01-09  7.34  21.93  29.44  927.57\n",
            "# 2003-01-10  7.36  21.97  29.03  927.57\n",
            "# 2003-01-13  7.32  22.16  28.91  926.26\n",
            "# 2003-01-14  7.30  22.39  29.17  931.66\n",
            "# 2003-01-15  7.22  22.11  28.77  918.22rets = stock.pct_change().dropna()\n",
            "spx_corr = lambda x: x.corrwith(x['SPX'])\n",
            "stock_by_year = rets.groupby(lambda x: x.year)result_2 = stock_by_year.apply(lambda g: g['AAPL'].corr(g['MSFT'])) # アップルとマイクロソフトの相関\n",
            "print( result_2 )\n",
            "# =>\n",
            "# 2003    0.480868\n",
            "# 2004    0.259024\n",
            "# 2005    0.300093\n",
            "# 2006    0.161735\n",
            "# 2007    0.417738\n",
            "# 2008    0.611901\n",
            "# 2009    0.432738\n",
            "# 2010    0.571946\n",
            "# 2011    0.581987def regression(data, yvar, xvars):\n",
            "    Y = data[yvar]\n",
            "    X = data[xvars]\n",
            "    X['intercept'] = 1.\n",
            "    result = sm.OLS(Y, X).fit()\n",
            "    return result.params\n",
            "事前準備\n",
            "pip install python-xbrl\n",
            "\n",
            "\n",
            "XBRLファイルをCSVファイルへ変換する\n",
            "# coding: utf-8\n",
            "from xbrl import XBRLParser\n",
            "import os, re, csv\n",
            "from collections import defaultdict\n",
            "事前準備でエラーのとき\n",
            "pip install lxml\n",
            "\n",
            "\n",
            "事前準備のエラーが解消しないとき\n",
            "set STATICBUILD=true && pip install lxml\n",
            "\n",
            "python cifar10_train.py\n",
            "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
            "2015-11-04 11:45:45.927302: step 0, loss = 4.68 (2.0 examples/sec; 64.221 sec/batch)\n",
            "2015-11-04 11:45:49.133065: step 10, loss = 4.66 (533.8 examples/sec; 0.240 sec/batch)\n",
            "2015-11-04 11:45:51.397710: step 20, loss = 4.64 (597.4 examples/sec; 0.214 sec/batch)\n",
            "2015-11-04 11:45:54.446850: step 30, loss = 4.62 (391.0 examples/sec; 0.327 sec/batch)\n",
            "2015-11-04 11:45:57.152676: step 40, loss = 4.61 (430.2 examples/sec; 0.298 sec/batch)\n",
            "2015-11-04 11:46:00.437717: step 50, loss = 4.59 (406.4 examples/sec; 0.315 sec/batch)\n",
            "...\n",
            "python cifar10_eval.py\n",
            "2015-11-06 08:30:44.391206: precision @ 1 = 0.860\n",
            "...\n",
            "python cifar10_multi_gpu_train.py --num_gpus=2\n",
            "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
            "2015-11-04 11:45:45.927302: step 0, loss = 4.68 (2.0 examples/sec; 64.221 sec/batch)\n",
            "2015-11-04 11:45:49.133065: step 10, loss = 4.66 (533.8 examples/sec; 0.240 sec/batch)\n",
            "2015-11-04 11:45:51.397710: step 20, loss = 4.64 (597.4 examples/sec; 0.214 sec/batch)\n",
            "2015-11-04 11:45:54.446850: step 30, loss = 4.62 (391.0 examples/sec; 0.327 sec/batch)\n",
            "2015-11-04 11:45:57.152676: step 40, loss = 4.61 (430.2 examples/sec; 0.298 sec/batch)\n",
            "2015-11-04 11:46:00.437717: step 50, loss = 4.59 (406.4 examples/sec; 0.315 sec/batch)\n",
            "...\n",
            "# -*- coding: utf-8 -*-\n",
            "%matplotlib inlinefrom sklearn.cross_validation import train_test_split\n",
            "from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC\n",
            "# 線形SVMのインスタンスを生成\n",
            "model = SVC(kernel='linear', random_state=None)from sklearn.linear_model import LogisticRegression\n",
            "model = LogisticRegression(random_state=None)\n",
            "from sklearn.metrics import accuracy_score# テストデータに対する精度\n",
            "pred_test = model.predict(X_test_std)\n",
            "accuracy_test = accuracy_score(y_test, pred_test)\n",
            "print('テストデータに対する正解率： %.2f' % accuracy_test)\n",
            "#分類結果を図示する\n",
            "import matplotlib.pyplot as plt\n",
            "from mlxtend.plotting import plot_decision_regions\n",
            "plt.style.use('ggplot') git clone https://github.com/pjreddie/darknet.git\n",
            "cd darknet\n",
            "#v1まで戻す\n",
            "git checkout c71bff69eaf1e458850ab78a32db8aa25fee17dc\n",
            "#gpuを使う場合はMakefileを編集\n",
            "Makefileのgpuを1へ\n",
            "make\n",
            "wget http://pjreddie.com/media/files/yolov1.weights\n",
            "./darknet yolo test cfg/yolo.cfg yolov1.weights data/dog.jpg\n",
            "tar xf VOCtrainval_11-May-2012.tar\n",
            "tar xf VOCtrainval_06-Nov-2007.tar\n",
            "tar xf VOCtest_06-Nov-2007.tar\n",
            "python voc_label.py   #script配下にあるのでdata以下にコピーしてくる\n",
            "cat 2007_* 2012_train.txt > train.txt\n",
            "<width>353</width><height>500</height><depth>3</depth></size>\n",
            "<bndbox><xmin>48</xmin><ymin>240</ymin><xmax>195</xmax><ymax>371</ymax></bndbox></object>\n",
            "def convert(size, box):\n",
            "    dw = 1./size[0] #1以内で横サイズの比率を出す\n",
            "    dh = 1./size[1] #1以内で縦サイズの比率を出す\n",
            "    x = (box[0] + box[1])/2.0 #左上の座標のxとy座標の加算を2で割る\n",
            "    y = (box[2] + box[3])/2.0 #右下の座標のxとy座標の加算を2で割る\n",
            "    w = box[1] - box[0] #boxの横サイズ\n",
            "    h = box[3] - box[2] #boxの縦サイズ\n",
            "    x = x*dw #x座標に比率をかける\n",
            "    w = w*dw #w座標に比率をかける\n",
            "    y = y*dh #y座標に比率をかける\n",
            "    h = h*dh #h座標に比率をかける\n",
            "    return (x,y,w,h)\n",
            "# -*- coding: utf-8 -*-\n",
            "import os\n",
            "import cv2\n",
            "import glob\n",
            "import numpy as np\n",
            "from PIL import Image, ImageTk\n",
            "import xml.etree.ElementTree as ET\n",
            "import pickle\n",
            "import os\n",
            "from os import listdir, getcwd\n",
            "from os.path import join18     char *train_images = \"/home/pjreddie/data/voc/test/train.txt\";\n",
            "19     char *backup_directory = \"/home/pjreddie/backup/\";\n",
            "make\n",
            "wget http://pjreddie.com/media/files/extraction.conv.weights\n",
            "./darknet yolo train cfg/yolov1/yolo.train.cfg extraction.conv.weights\n",
            "./darknet yolo train cfg/yolov1/yolo.train.cfg extraction.conv.weights -i 1\n",
            "\n",
            "examples/train_detector.c\n",
            "...略\n",
            "void test_detector(char *datacfg, char *cfgfile, char *weightfile, char *filename, float thresh, float hier_thresh, char *outfile, int\\\n",
            " fullscreen)\n",
            "{\n",
            "src/image.c\n",
            "...略\n",
            "void draw_detections(image im, int num, float thresh, box *boxes, float **probs, float **masks, char **names, image **alphabet, int cl\\\n",
            "asses)\n",
            "{\n",
            "...略import os, csv, time\n",
            "import glob[net]\n",
            "batch=64\n",
            "subdivisions=4\n",
            "[net]\n",
            "batch=1\n",
            "subdivisions=1\n",
            "git clone https://github.com/Itseez/opencv.git\n",
            "cd opencv\n",
            "checkout -b 3.4.0\n",
            "sudo apt-get install build-essential\n",
            "sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev\n",
            "sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-devwhile loading shared libraries: libopencv_highgui.so\n",
            "$ ./darknet yolo train cfg/yolo.cfg extraction.conv.weights\n",
            "yolo\n",
            "0: Crop Layer: 448 x 448 -> 448 x 448 x 3 image\n",
            "shift: Using default '0.000000'\n",
            "1: Convolutional Layer: 448 x 448 x 3 image, 64 filters -> 224 x 224 x 64 image\n",
            "2: Maxpool Layer: 224 x 224 x 64 image, 2 size, 2 stride\n",
            "3: Convolutional Layer: 112 x 112 x 64 image, 192 filters -> 112 x 112 x 192 image\n",
            "4: Maxpool Layer: 112 x 112 x 192 image, 2 size, 2 stride\n",
            "5: Convolutional Layer: 56 x 56 x 192 image, 128 filters -> 56 x 56 x 128 image\n",
            "6: Convolutional Layer: 56 x 56 x 128 image, 256 filters -> 56 x 56 x 256 image\n",
            "7: Convolutional Layer: 56 x 56 x 256 image, 256 filters -> 56 x 56 x 256 image\n",
            "8: Convolutional Layer: 56 x 56 x 256 image, 512 filters -> 56 x 56 x 512 image\n",
            "9: Maxpool Layer: 56 x 56 x 512 image, 2 size, 2 stride\n",
            "10: Convolutional Layer: 28 x 28 x 512 image, 256 filters -> 28 x 28 x 256 image\n",
            "11: Convolutional Layer: 28 x 28 x 256 image, 512 filters -> 28 x 28 x 512 image\n",
            "12: Convolutional Layer: 28 x 28 x 512 image, 256 filters -> 28 x 28 x 256 image\n",
            "13: Convolutional Layer: 28 x 28 x 256 image, 512 filters -> 28 x 28 x 512 image\n",
            "14: Convolutional Layer: 28 x 28 x 512 image, 256 filters -> 28 x 28 x 256 image\n",
            "15: Convolutional Layer: 28 x 28 x 256 image, 512 filters -> 28 x 28 x 512 image\n",
            "16: Convolutional Layer: 28 x 28 x 512 image, 256 filters -> 28 x 28 x 256 image\n",
            "17: Convolutional Layer: 28 x 28 x 256 image, 512 filters -> 28 x 28 x 512 image\n",
            "18: Convolutional Layer: 28 x 28 x 512 image, 512 filters -> 28 x 28 x 512 image\n",
            "19: Convolutional Layer: 28 x 28 x 512 image, 1024 filters -> 28 x 28 x 1024 image\n",
            "20: Maxpool Layer: 28 x 28 x 1024 image, 2 size, 2 stride\n",
            "21: Convolutional Layer: 14 x 14 x 1024 image, 512 filters -> 14 x 14 x 512 image\n",
            "22: Convolutional Layer: 14 x 14 x 512 image, 1024 filters -> 14 x 14 x 1024 image\n",
            "23: Convolutional Layer: 14 x 14 x 1024 image, 512 filters -> 14 x 14 x 512 image\n",
            "24: Convolutional Layer: 14 x 14 x 512 image, 1024 filters -> 14 x 14 x 1024 image\n",
            "25: Convolutional Layer: 14 x 14 x 1024 image, 1024 filters -> 14 x 14 x 1024 image\n",
            "26: Convolutional Layer: 14 x 14 x 1024 image, 1024 filters -> 7 x 7 x 1024 image\n",
            "27: Convolutional Layer: 7 x 7 x 1024 image, 1024 filters -> 7 x 7 x 1024 image\n",
            "28: Convolutional Layer: 7 x 7 x 1024 image, 1024 filters -> 7 x 7 x 1024 image\n",
            "29: Connected Layer: 50176 inputs, 4096 outputs\n",
            "30: Dropout Layer: 4096 inputs, 0.500000 probability\n",
            "31: Connected Layer: 4096 inputs, 1470 outputs\n",
            "32: Detection Layer\n",
            "forced: Using default '0'\n",
            "Loading weights from extraction.conv.weights...Done!\n",
            "Learning Rate: 0.001, Momentum: 0.9, Decay: 0.0005\n",
            "/Users/stiaje/Projects/darknet/voc/VOCdevkit/VOC2007/JPEGImages/003434.jpg\n",
            "[1]    43754 abort      ./darknet yolo train cfg/yolo.cfg extraction.conv.weights\n",
            "# import the necessary packages\n",
            "from collections import namedtuple\n",
            "import numpy as np\n",
            "import cv2nkf -w train.txt > train.txt \n",
            "\n",
            "plot.py\n",
            "# coding: utf-8\n",
            "import cv2\n",
            "from matplotlib import pyplot as pltcp ~/.tmux/log/20190128-220050-0-1.0.log test4.log\n",
            "#coding:utf-8\n",
            "import repython plot_log.py --log-file test4_a.log\n",
            "\n",
            "plot_log.py\n",
            "import argparse\n",
            "import logging\n",
            "import os\n",
            "import platform\n",
            "import re\n",
            "import sysif bbox_iou(boxes[index_i], boxes[index_j]) >= nms_threshold:\n",
            "        boxes[index_j].classes[c] = 0\n",
            "data = pandas.read_csv('./csv/bitcoin_log_1month.csv')\n",
            "data = data.sort_values(by='date')\n",
            "data = data.reset_index(drop=True)\n",
            "data['close'] = preprocessing.scale(data['close'])\n",
            "# -*- coding: utf-8 -*-\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "import tflearn\"日付け\",\"終値\",\"始値\",\"高値\",\"安値\",\"前日比率\"\n",
            "\"2017年09月03日\",\"523714.1875\",\"499204.7813\",\"585203.1250\",\"499204.7813\",\"4.91\"\n",
            "\"2017年09月02日\",\"499204.7813\",\"542277.3125\",\"585203.1250\",\"498504.5000\",\"-7.94\"net = tflearn.input_data(shape=[None, self.steps_of_history, 1])\n",
            "net = tflearn.gru(net, n_units=6)\n",
            "net = tflearn.fully_connected(net, 1, activation='linear')net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
            "                loss='mean_square')# 今回は80%を訓練データセット、20%をテストデータセットとして扱う。\n",
            "pos = round(len(X) * (1 - 0.2))\n",
            "trainX, trainY = X[:pos], Y[:pos]\n",
            "testX, testY   = X[pos:], Y[pos:]    model.add(LSTM(self.hidden_neurons, \\\n",
            "              batch_input_shape=(None, self.length_of_sequences, self.in_out_neurons), \\\n",
            "              return_sequences=False))\n",
            "    model.add(Dense(self.in_out_neurons))\n",
            "    model.add(Activation(\"linear\"))\n",
            "    model.compile(loss=\"mape\", optimizer=\"adam\")X, y = make_classification(n_samples=10000, n_features=10, n_informative=10, n_redundant=0, \n",
            "                           n_repeated=0,n_clusters_per_class=8, random_state=123)\n",
            "import operator, math, random, time\n",
            "import numpy as np### Results\n",
            "Baseline AUC train : 0.741195900486\n",
            "Baseline AUC test : 0.731194743417\n",
            "Best AUC train : 0.875623225397\n",
            "Best AUC test : 0.867496316228\n",
            "### Generated feature expression\n",
            "mul(sub(add(mul(ARG9, ARG2), cos(neg(ARG6))), neg(ARG6)), sub(ARG7, cos(cos(neg(ARG6)))))\n",
            "mul(neg(ARG10), mul(ARG9, ARG8))\n",
            "sub(add(ARG1, ARG8), add(mul(add(ARG2, ARG6), ARG8), cos(protectedDiv(ARG10, ARG3))))\n",
            "sub(sub(sub(neg(mul(ARG9, ARG12)), cos(ARG9)), cos(ARG9)), mul(ARG2, ARG12))\n",
            "mul(add(add(ARG6, neg(ARG7)), ARG8), add(sin(ARG6), neg(ARG7)))\n",
            "sin(cos(add(ARG8, ARG2)))\n",
            "cos(cos(add(cos(ARG9), sin(sin(ARG2)))))\n",
            "mul(ARG8, mul(mul(mul(mul(ARG16, ARG16), mul(ARG16, ARG16)), ARG16), ARG16))\n",
            "mul(sub(ARG17, add(add(ARG9, ARG2), sin(ARG2))), add(ARG16, ARG14))\n",
            "add(sub(mul(mul(ARG8, ARG7), ARG7), mul(ARG14, ARG8)), mul(ARG8, ARG7))\n",
            "cos(ARG7)\n",
            "add(mul(ARG10, ARG6), mul(mul(ARG10, ARG6), ARG6))\n",
            "protectedDiv(ARG19, ARG17)\n",
            "add(mul(ARG2, ARG20), mul(ARG10, ARG2))\n",
            "sin(add(add(ARG6, tan(ARG16)), tan(ARG9)))\n",
            "mul(neg(ARG2), sub(sub(sub(sub(ARG18, ARG19), ARG6), ARG6), ARG6))\n",
            "mul(cos(tan(cos(ARG6))), ARG16)\n",
            "sub(cos(ARG6), mul(ARG7, ARG2))\n",
            "mul(ARG16, sin(ARG27))\n",
            "mul(sub(ARG27, ARG15), add(ARG8, add(ARG9, ARG6)))\n",
            "# import libraries\n",
            "import numpy as np\n",
            "import pandas as pds\n",
            "from keras.models import Sequential\n",
            "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
            "from keras.wrappers.scikit_learn import KerasRegressor\n",
            "from sklearn.model_selection import cross_val_score\n",
            "from sklearn.model_selection import KFold\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.pipeline import Pipeline\n",
            "from sklearn.metrics import mean_squared_error\n",
            "from sklearn.datasets import load_diabetes# create regression model\n",
            "def reg_model():\n",
            "    model = Sequential()\n",
            "    model.add(Dense(10, input_dim=10, activation='relu'))\n",
            "    model.add(Dense(16, activation='relu'))\n",
            "    model.add(Dense(1))# use data split and fit to run the model\n",
            "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
            "estimator = KerasRegressor(build_fn=reg_model, epochs=100, batch_size=10, verbose=0)\n",
            "estimator.fit(x_train, y_train)\n",
            "y_pred = estimator.predict(x_test)# use Kfold and cross validation to run the model\n",
            "seed = 7\n",
            "np.random.seed(seed)\n",
            "estimator = KerasRegressor(build_fn=reg_model, epochs=100, batch_size=10, verbose=0)\n",
            "kfold = KFold(n_splits=10, random_state=seed)# create deep learning like regression model\n",
            "def deep_reg_model():\n",
            "    model = Sequential()\n",
            "    model.add(Dense(10, input_dim=10, activation='relu'))\n",
            "    model.add(BatchNormalization())\n",
            "    model.add(Dropout(0.2))\n",
            "    model.add(Dense(256, activation='relu'))\n",
            "    model.add(BatchNormalization())\n",
            "    model.add(Dropout(0.2))\n",
            "    model.add(Dense(128, activation='relu'))\n",
            "    model.add(BatchNormalization())\n",
            "    model.add(Dropout(0.2))\n",
            "    model.add(Dense(64, activation='relu'))\n",
            "    model.add(BatchNormalization())\n",
            "    model.add(Dropout(0.2))\n",
            "    model.add(Dense(1))# use data split and fit to run the model\n",
            "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
            "estimator = KerasRegressor(build_fn=deep_reg_model, epochs=100, batch_size=10, verbose=0)\n",
            "estimator.fit(x_train, y_train)\n",
            "y_pred = estimator.predict(x_test)$ sudo pip install statsmodels\n",
            "$ sudo pip install numpy\n",
            "$ sudo pip install matplotlib\n",
            "y = a + bx + \\varepsilon\n",
            "\n",
            "data.txt\n",
            "#      x        y\n",
            "  -1.000   -1.656\n",
            "  -0.900   -0.734\n",
            "  -0.800   -3.036\n",
            "  -0.700   -1.026\n",
            "  -0.600   -1.104\n",
            "  -0.500    0.023\n",
            "  -0.400    0.246\n",
            "  -0.300    1.817\n",
            "  -0.200    0.651\n",
            "  -0.100    0.082\n",
            "  -0.000    2.524\n",
            "   0.100    2.231\n",
            "   0.200    0.783\n",
            "   0.300    2.489\n",
            "   0.400    1.892\n",
            "   0.500    3.207\n",
            "   0.600    1.868\n",
            "   0.700    3.954\n",
            "   0.800    4.447\n",
            "   0.900    4.024\n",
            "\n",
            "\n",
            "regression.py\n",
            "# coding: utf-8\n",
            "import numpy as np\n",
            "import statsmodels.api as sm\n",
            "import matplotlib.pyplot as plt                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      y   R-squared:                       0.831\n",
            "Model:                            OLS   Adj. R-squared:                  0.822\n",
            "Method:                 Least Squares   F-statistic:                     88.59\n",
            "Date:                Thu, 25 Dec 2014   Prob (F-statistic):           2.25e-08\n",
            "Time:                        14:07:16   Log-Likelihood:                -24.450\n",
            "No. Observations:                  20   AIC:                             52.90\n",
            "Df Residuals:                      18   BIC:                             54.89\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
            "------------------------------------------------------------------------------\n",
            "const          1.2922      0.194      6.647      0.000         0.884     1.701\n",
            "x1             3.1611      0.336      9.412      0.000         2.455     3.867\n",
            "==============================================================================\n",
            "Omnibus:                        0.801   Durbin-Watson:                   2.495\n",
            "Prob(Omnibus):                  0.670   Jarque-Bera (JB):                0.653\n",
            "Skew:                          -0.402   Prob(JB):                        0.721\n",
            "Kurtosis:                       2.628   Cond. No.                         1.74\n",
            "==============================================================================y = \\beta_0 + \\beta_1 \\phi_1(x) + \\beta_2\\phi_2(x) + \\cdots + \\beta_{M-1}\\phi_{M-1}(x) + \\varepsilon\n",
            "y = \\sum_{j=0}^{M-1} \\beta_j\\phi_j(x) + \\varepsilon\n",
            "X = \\begin{Bmatrix}\n",
            "\\phi_0(x_0) & \\phi_1(x_0) & \\phi_2(x_0) \\\\\n",
            "\\phi_0(x_1) & \\phi_1(x_1) & \\phi_2(x_1) \\\\\n",
            "\\phi_0(x_2) & \\phi_1(x_2) & \\phi_2(x_2) \\\\\n",
            "& \\vdots & \\\\\n",
            "\\end{Bmatrix}\n",
            "y = a + bx + cx^2 + \\varepsilon\n",
            "\n",
            "data.txt\n",
            "#      x        y\n",
            "  -2.000   10.260\n",
            "  -1.800    7.403\n",
            "  -1.600    7.779\n",
            "  -1.400    4.310\n",
            "  -1.200    4.051\n",
            "  -1.000    4.577\n",
            "  -0.800    3.416\n",
            "  -0.600    3.628\n",
            "  -0.400    3.968\n",
            "  -0.200    3.780\n",
            "  -0.000    1.873\n",
            "   0.200    2.741\n",
            "   0.400    2.106\n",
            "   0.600    5.286\n",
            "   0.800    8.138\n",
            "   1.000    5.316\n",
            "   1.200    9.159\n",
            "   1.400    9.748\n",
            "   1.600    7.585\n",
            "   1.800   10.726\n",
            "\n",
            "\n",
            "regression_2.py\n",
            "# coding: utf-8\n",
            "import numpy as np\n",
            "import statsmodels.api as sm\n",
            "import matplotlib.pyplot as pltg(z) = \\frac{1}{1 + e^{-z}}\n",
            "Precision = \\frac{True Positive}{True Positive + False Positive}\n",
            "Recall = \\frac{True Positive}{True Positive + False Negative}\n",
            "Accuracy = \\frac{True Positive + True Negative}{True Positive + False Positive + True Negative + False Negative}\n",
            "F = 2 \\frac{Precision * Recall}{Precision + Recall}\n",
            "\n",
            "Program.cs\n",
            "using System;\n",
            "using System.Collections.Generic;\n",
            "using System.Linq;\n",
            "using System.Text;\n",
            "using System.Threading.Tasks;\n",
            "using System.Data;\n",
            "using System.Drawing;\n",
            "using OpenCvSharp;\n",
            "using OpenCvSharp.CPlusPlus;\n",
            "MyMultilayerPerceptron.py\n",
            "#!/usr/bin/env python\n",
            "from mlp import *\n",
            "import os\n",
            "import numpy\n",
            "import gzip, pickle\n",
            "import pylab\n",
            "mlp.py\n",
            "#!/usr/bin/env python\n",
            "from math import exp\n",
            "import numpy\n",
            "1番目の予測\n",
            "No.0 : predict => 0 , answer = > 0\n",
            "No.1 : predict => 1 , answer = > 1\n",
            "No.2 : predict => 4 , answer = > 4\n",
            "No.3 : predict => 6 , answer = > 6\n",
            "No.4 : predict => 7 , answer = > 7\n",
            "No.5 : predict => 9 , answer = > 9\n",
            "00006 / 00006 = 100.00%\n",
            "\n",
            "\n",
            "2番目の予測\n",
            "No.0 : predict => 0 , answer = > 0\n",
            "No.1 : predict => 1 , answer = > 1\n",
            "No.2 : predict => 6 , answer = > 4\n",
            "No.3 : predict => 8 , answer = > 6\n",
            "No.4 : predict => 8 , answer = > 7\n",
            "No.5 : predict => 3 , answer = > 9\n",
            "00002 / 00006 = 33.33%\n",
            "\n",
            "\n",
            "3番目の予測\n",
            "No.0 : predict => 0 , answer = > 0\n",
            "No.1 : predict => 2 , answer = > 2\n",
            "No.2 : predict => 6 , answer = > 4\n",
            "No.3 : predict => 5 , answer = > 5\n",
            "No.4 : predict => 3 , answer = > 7\n",
            "No.5 : predict => 8 , answer = > 9\n",
            "00003 / 00006 = 50.00%\n",
            "\n",
            "tar zxvf spark-1.5.0-bin-hadoop2.6.tar \n",
            "mv spark-1.5.0-bin-hadoop2.6 /usr/local/bin/\n",
            "\n",
            ".bashrc\n",
            "export SPARK_HOME=/usr/local/bin/spark-1.5.0-bin-hadoop2.6\n",
            "\n",
            "ipython notebook\n",
            "import os, sys\n",
            "from datetime import datetime as dt\n",
            "print \"loading PySpark setting...\"\n",
            "spark_home = os.environ.get('SPARK_HOME', None)\n",
            "if not spark_home:\n",
            "    raise ValueError('SPARK_HOME environment variable is not set')\n",
            "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
            "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\n",
            "execfile(os.path.join(spark_home, 'python/pyspark/shell.py'))loading PySpark setting...\n",
            "/usr/local/bin/spark-1.5.0-bin-hadoop2.6\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.0\n",
            "      /_/%matplotlib inline\n",
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "from sklearn import datasets\n",
            "plt.style.use('ggplot')# http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.clusteringdata = sc.parallelize(iris.data[:,[0,2]])\n",
            "model = KMeans.train(data, k, initializationMode=\"random\", seed=None)\n",
            "for i in range(k):\n",
            "    plt.scatter(model.clusterCenters[i][0], model.clusterCenters[i][1], s=200, c=\"purple\", alpha=.7, marker=\"^\")\n",
            "# ------- Create Color Map ------- #\n",
            "xmin = 4.0\n",
            "xmax = 8.5\n",
            "ymin = 0\n",
            "ymax = 8\n",
            "n = 100\n",
            "xx = np.linspace(xmin, xmax, n)\n",
            "yy = np.linspace(ymin, ymax, n)\n",
            "X, Y = np.meshgrid(xx, yy)from pyspark.mllib.classification import SVMWithSGD\n",
            "from pyspark.mllib.regression import LabeledPointfrom pyspark.mllib.classification import LogisticRegressionWithLBFGSfrom pyspark.mllib.tree import RandomForest, RandomForestModel\n",
            "out\n",
            "Test Error = 0.0588235294118\n",
            "Learned classification forest model:\n",
            "TreeEnsembleModel classifier with 5 trees# ------- Predict Data ------- #\n",
            "Z = np.zeros_like(X)\n",
            "for i in range(n):\n",
            "    for j in range(n):\n",
            "        Z[i,j] = model.predict([xx[j],yy[i]])#必要なライブラリを呼びます\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "from pandas import Series,DataFramefrom sklearn.datasets import load_boston\n",
            "boston = load_boston()print(boston.DESCR)\n",
            "\"\"\"\n",
            "Boston House Prices dataset\n",
            "===========================\n",
            "print(boston.data)\n",
            "boston_df.to_csv(\"csvにしたいなまえ.csv\")\n",
            "boston_df.head()boston_df['PRICE'] = DataFrame(boston.target)#インスタンス(モデルの元)を作成します！\n",
            "変数 = LinearRegression()\n",
            "#インスタンス\n",
            "linear_regression = LinearRegression()\n",
            "X.head()\n",
            "\n",
            "Y.head()\n",
            "\"\"\"\n",
            "0      24.0\n",
            "1      21.6\n",
            "2      34.7\n",
            "3      33.4\n",
            "4      36.2\n",
            "5      28.7\n",
            "6      22.9\n",
            "7      27.1\n",
            "8      16.5\n",
            "9      18.9\n",
            "10     15.0\n",
            "11     18.9\n",
            "12     21.7\n",
            "13     20.4\n",
            "14     18.2\n",
            "15     19.9\n",
            "16     23.1\n",
            "17     17.5\n",
            "18     20.2\n",
            "19     18.2\n",
            "20     13.6\n",
            "21     19.6\n",
            "22     15.2\n",
            "23     14.5\n",
            "24     15.6\n",
            "25     13.9\n",
            "26     16.6\n",
            "27     14.8\n",
            "28     18.4\n",
            "29     21.0\n",
            "       ... \n",
            "\"\"\"\n",
            "#通常であれば行列の計算などが必要ですが、簡単にやってくれます。\n",
            "linear_regression.fit(X,Y)\n",
            "print(linear_regression.coef_)\n",
            "\"\"\"\n",
            "[-1.07170557e-01  4.63952195e-02  2.08602395e-02  2.68856140e+00\n",
            " -1.77957587e+01  3.80475246e+00  7.51061703e-04 -1.47575880e+00\n",
            "  3.05655038e-01 -1.23293463e-02 -9.53463555e-01  9.39251272e-03\n",
            " -5.25466633e-01]\n",
            "\"\"\"\n",
            "#先程のfeature_namesをデータフレームに入れます。\n",
            "coefficient['f_names'] = DataFrame(boston.feature_names)\n",
            "linear_regression.intercept_\n",
            "#サンプルをランダムに分けるcross_validation\n",
            "#import sklearn.cross_validation \n",
            "import sklearn.model_selection\n",
            "#線形回帰モデルを作成\n",
            "multi_lreg = LinearRegression()#トレーニングに使用したデータセットでの精度\n",
            "print(multi_lreg.score(X_train,Y_train))% go get github.com/skelterjohn/go.matrix\n",
            "error: error:0D0C50A1:asn1 encoding routines:ASN1_item_verify:unknown message digest algorithm while accessing\n",
            "\n",
            "行列計算例\n",
            "package main\n",
            "結果\n",
            "{1, 2, 3,\n",
            " 4, 5, 6,\n",
            " 7, 8, 9}\n",
            "{1, 2, 3,\n",
            " 4, 5, 6}\n",
            "{ 14,  32,\n",
            "  32,  77,\n",
            "  50, 122}\n",
            "\n",
            "f(x) = w^T\\Phi(x)\n",
            " w = (\\Phi^T\\Phi)^{-1}{\\Phi}t\n",
            "/**\n",
            " * 線形回帰サンプル\n",
            " * 参考 : http://gihyo.jp/dev/serial/01/machine-learning/0011?page=1&ard=1400930362 \n",
            " *//**\n",
            " * 最小二乗法サンプル\n",
            " * 参考 : イラストで学ぶ機械学習 Chapter3.1\n",
            " */最適なパラメーター = {'C': 18.329807108324356, 'epsilon': 1.4384498882876631}\n",
            "交差検証データに対するR2スコア（決定係数） = 0.870461803\n",
            "テストデータに対するR2スコア（〃） = 0.907877172884\n",
            "pip install optuna\n",
            "from sklearn.datasets import load_boston\n",
            "from sklearn.svm import SVR\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import mean_squared_error# 目的関数\n",
            "def objective(trial):\n",
            "    # C\n",
            "    svr_c = trial.suggest_loguniform('svr_c', 1e0, 1e2)\n",
            "    # epsilon\n",
            "    epsilon = trial.suggest_loguniform('epsilon', 1e-1, 1e1)\n",
            "    # SVR\n",
            "    svr = SVR(C=svr_c, epsilon=epsilon)\n",
            "    svr.fit(X_train, y_train)\n",
            "    # 予測\n",
            "    y_pred = svr.predict(X_val)\n",
            "    # CrossvalidationのMSEで比較（最大化がまだサポートされていない）\n",
            "    return mean_squared_error(y_val, y_pred)\n",
            "# optuna\n",
            "study = optuna.create_study()\n",
            "study.optimize(objective, n_trials=100)\n",
            "出力\n",
            "[I 2018-12-05 19:55:25,166] Finished a trial resulted in value: 7.6590342657643\n",
            "05. Current best value is 7.509678888529477 with parameters: {'svr_c': 19.04682\n",
            "554857877, 'epsilon': 1.5323752754042508}.\n",
            "[I 2018-12-05 19:55:25,197] Finished a trial resulted in value: 7.7412732255113\n",
            "2. Current best value is 7.509678888529477 with parameters: {'svr_c': 19.046827\n",
            "54857877, 'epsilon': 1.5323752754042508}.\n",
            "{'svr_c': 19.046827554857877, 'epsilon': 1.5323752754042508}\n",
            "7.509678888529477\n",
            "FrozenTrial(trial_id=62, state=<TrialState.COMPLETE: 1>, value=7.50967888852947\n",
            ", datetime_start=datetime.datetime(2018, 12, 5, 19, 55, 24, 225021), datetime_c\n",
            "mplete=datetime.datetime(2018, 12, 5, 19, 55, 24, 243523), params={'svr_c': 19.\n",
            "46827554857877, 'epsilon': 1.5323752754042508}, user_attrs={}, system_attrs={},\n",
            "intermediate_values={}, params_in_internal_repr={'svr_c': 19.046827554857877, '\n",
            "psilon': 1.5323752754042508})\n",
            "\n",
            "# history\n",
            "hist_df = study.trials_dataframe()\n",
            "hist_df.to_csv(\"boston_svr.csv\")\n",
            "from sklearn.datasets import load_boston\n",
            "from sklearn.svm import SVR\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import r2_score\n",
            "出力\n",
            "交差検証データ　R2 :  0.8685237976554909\n",
            "テストデータ　R2 :  0.9081066406039279\n",
            "\n",
            "{'svr_c': 14.312804360445284, 'epsilon': 7.474784105854122e-09}\n",
            "7.921504496284093\n",
            "import optuna\n",
            "import numpy as np\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.datasets import make_classification\n",
            "print(clf.feature_importances_)\n",
            "[ 0.17287856  0.80608704  0.01884792  0.00218648]\n",
            "print(clf.predict([[0, 0, 0, 0]]))\n",
            "[1]\n",
            "\n",
            "feature_importances_\n",
            "\n",
            "from matplotlib.colors import ListedColormap\n",
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "from sklearn.tree import DecisionTreeClassifier\n",
            "from sklearn.cross_validation import train_test_split\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from xgboost import XGBClassifier\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}