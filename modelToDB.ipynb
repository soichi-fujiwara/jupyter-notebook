{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modelToDB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soichi-fujiwara/jupyter-notebook/blob/master/modelToDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCMfweVZEzPd",
        "colab_type": "text"
      },
      "source": [
        "##モデルをDB化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhMbBx_ECoOl",
        "colab_type": "code",
        "outputId": "30eec1d5-e431-4836-cf37-24582af6e40b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYLgNCFduL-w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36899837-5263-445b-d6c9-4126f7ac38e6"
      },
      "source": [
        "!cat /proc/uptime | awk '{print $1 /60 /60 /24 \"days (\" $1 \"sec)\"}'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.00674838days (583.06sec)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRJCjIdrc3nQ",
        "colab_type": "code",
        "outputId": "f52174a8-978c-455a-ce03-e33d3b54b2a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import IPython\n",
        "#bz2ファイル解凍\n",
        "#!bunzip2 -k -q \"./drive/My Drive/NLP/20170201.tar.bz2\"\n",
        "\n",
        "#tarファイル解凍\n",
        "!tar -xvf \"./drive/My Drive/NLP/20170201.tar\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "entity_vector/\n",
            "entity_vector/entity_vector.model.txt\n",
            "entity_vector/entity_vector.model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2jUrFtNeMMH",
        "colab_type": "code",
        "outputId": "f9c57990-4616-4297-939d-c141fc6cd4b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "#解凍時のみ\n",
        "model_dir = 'entity_vector/entity_vector.model.bin'\n",
        "model = KeyedVectors.load_word2vec_format(model_dir, binary=True,limit=500000)\n",
        "\n",
        "#解凍後確認\n",
        "results = model.most_similar(positive=['[混一色]','逆'])\n",
        "for result in results:\n",
        "  print(result)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "('[清一色]', 0.7387728691101074)\n",
            "('[対々和]', 0.6651833057403564)\n",
            "('[大三元]', 0.6435492634773254)\n",
            "('[断ヤオ九]', 0.6391950845718384)\n",
            "('[七対子]', 0.6391769647598267)\n",
            "('[三色同順]', 0.6375112533569336)\n",
            "('[四喜和]', 0.6348252892494202)\n",
            "('[国士無双_(麻雀)]', 0.6290631294250488)\n",
            "('[四暗刻]', 0.6273880004882812)\n",
            "('[三暗刻]', 0.6180248260498047)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEruRmNNatxY",
        "colab_type": "code",
        "outputId": "7ada92eb-fe49-4017-a1d7-7b9753541c8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install mecab-python3"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/92/e7e7f38df8457fa40c1ca86928be5ddbe2bf341e90a35e6ada30d03ef16d/mecab_python3-0.996.2-cp36-cp36m-manylinux1_x86_64.whl (15.9MB)\n",
            "\u001b[K     |████████████████████████████████| 15.9MB 403kB/s \n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.996.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OzKgJ9slAsf",
        "colab_type": "text"
      },
      "source": [
        "##語彙リスト作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5yIYDAfPHQu",
        "colab_type": "code",
        "outputId": "cba1f01c-0a7c-4f0b-de3f-67b6f3cf17af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "cnt = 1\n",
        "model_vocab_list = []\n",
        "\n",
        "for words, v in model.vocab.items():\n",
        "\n",
        "  #固有名詞用の括弧を削除\n",
        "  #words = words.replace('[', '').replace(']', '')\n",
        "\n",
        "  #-----------------------------------------------------  \n",
        "\n",
        "  # 半角記号+半角数字\n",
        "  #p = re.compile(\"[!-@[-`{-~]\")\n",
        "  \n",
        "  # 半角記号+半角数字+半角英字\n",
        "  #((ほぼ)日本語のみのDB作成用)\n",
        "  p = re.compile(\"[!-~]\")\n",
        "\n",
        "  # 先頭文字\n",
        "  top_char = words[0:1]\n",
        "\n",
        "  #-----------------------------------------------------  \n",
        "\n",
        "#   【M1】dfに格納した後で補足情報を削除する為、ココでは補足情報を残しておく\n",
        "#   #補足情報の_を削除\n",
        "#   words = words.replace(\"_\",\"\")\n",
        "#   #括弧()文字を抽出\n",
        "#   regex = re.compile(\".*?\\((.*?)\\)\")\n",
        "#   #括弧文字をlist型で返却\n",
        "#   ret_list = re.findall(regex, words)\n",
        "\n",
        "  #-----------------------------------------------------  \n",
        "\n",
        "#   #除外文字なし\n",
        "#   if p.match(top_char) is None:\n",
        "#     #括弧文字がある場合は無効化\n",
        "#     if len(ret_list) > 0: \n",
        "#       words = words.replace(\"(\" + ret_list[0] + \")\",'')\n",
        "#       model_vocab_list.append(words)\n",
        "#     else:\n",
        "#       model_vocab_list.append(words)\n",
        "\n",
        "  #【M1】\n",
        "  model_vocab_list.append(words)\n",
        "  #【M1】\n",
        "  \n",
        "  if cnt % 50000 == 0:\n",
        "    print(cnt,\"件 完了\");\n",
        "    \n",
        "  cnt = cnt + 1\n",
        "\n",
        "#重複削除\n",
        "model_vocab_uni_list= list(set(model_vocab_list))\n",
        "  \n",
        "#保存\n",
        "df = pd.DataFrame(model_vocab_uni_list)\n",
        "df.to_csv('./drive/My Drive/NLP/model_vocab_list.csv', header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 件 完了\n",
            "100000 件 完了\n",
            "150000 件 完了\n",
            "200000 件 完了\n",
            "250000 件 完了\n",
            "300000 件 完了\n",
            "350000 件 完了\n",
            "400000 件 完了\n",
            "450000 件 完了\n",
            "500000 件 完了\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "190yBfA8Wayj",
        "colab_type": "text"
      },
      "source": [
        "##語彙リスト読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRvxKV48SLUM",
        "colab_type": "code",
        "outputId": "8d81db1b-9653-496f-b348-a656275a2937",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "csv_dir = './drive/My Drive/NLP/model_vocab_list.csv'\n",
        "df = pd.read_csv(csv_dir,names=['words'])\n",
        "df = df.sort_values('words', ascending=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsgpqqphWIw4",
        "colab_type": "text"
      },
      "source": [
        "##(関数)対義語-対応list作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdwy6HUsTA_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import MeCab\n",
        "#括弧記号無効化\n",
        "import lib_delete_brackets\n",
        "\n",
        "def search_similar_texts(p_words):\n",
        "  pass\n",
        "\n",
        "def wordRevChange(words,gyaku,model,tokenizer):\n",
        "\n",
        "  no_list = ['[混一色]','必然的','意図的','結果的','下向き','場合','横方向','かえって']\n",
        "\n",
        "  #**************************************************************************\n",
        "  #補足情報有無判定\n",
        "  #**************************************************************************\n",
        "  if '_(' in words:\n",
        "    #補足情報あり\n",
        "    info_chk_flg = 0\n",
        "  else:\n",
        "    #補足情報なし and 特定固有名詞\n",
        "    if '[' in words:\n",
        "      info_chk_flg = 1\n",
        "    else:\n",
        "      #ノーマル語彙\n",
        "      info_chk_flg = 2\n",
        "  \n",
        "  #**************************************************************************\n",
        "  #①そのまま対義語化\n",
        "  #**************************************************************************\n",
        "  if info_chk_flg == 0:\n",
        "    asitis_cng_words1 = ''\n",
        "    asitis_cng_words2 = ''\n",
        "    asitis_cng_words3 = ''\n",
        "\n",
        "    rvs_wd_list = model.most_similar(positive=[words,gyaku])\n",
        "\n",
        "    index = 0\n",
        "    rvs_wd_rt_list = []\n",
        "\n",
        "    for index in range(len(rvs_wd_list)):\n",
        "      wd = rvs_wd_list[index][0]\n",
        "      if str(wd) not in no_list:\n",
        "\n",
        "        #対義語候補格納\n",
        "        rvs_wd_rt_list.append(lib_delete_brackets.delete_brackets(wd))\n",
        "\n",
        "    result_list = []\n",
        "\n",
        "    asitis_cng_words1 = lib_delete_brackets.delete_brackets(rvs_wd_rt_list[0])\n",
        "    asitis_cng_words2 = lib_delete_brackets.delete_brackets(rvs_wd_rt_list[1])\n",
        "    asitis_cng_words3 = lib_delete_brackets.delete_brackets(rvs_wd_rt_list[2])\n",
        "\n",
        "    asitis_cng_words1 = asitis_cng_words1.replace('_',\"\").replace('[',\"\").replace(']',\"\")\n",
        "    asitis_cng_words2 = asitis_cng_words2.replace('_',\"\").replace('[',\"\").replace(']',\"\")\n",
        "    asitis_cng_words3 = asitis_cng_words3.replace('_',\"\").replace('[',\"\").replace(']',\"\")\n",
        "\n",
        "    #返却値書き込み\n",
        "    result_list.append(words.replace('[',\"\").replace(']',\"\"))\n",
        "    result_list.append('')\n",
        "    result_list.append(asitis_cng_words1)\n",
        "    result_list.append(asitis_cng_words2)\n",
        "    result_list.append(asitis_cng_words3)\n",
        "\n",
        "    return result_list\n",
        "\n",
        "  #**************************************************************************\n",
        "  #②特定固有名詞\n",
        "  #**************************************************************************\n",
        "  #---------------------------------------------\n",
        "  #②-1 特定固有名詞をそのまま対義語化\n",
        "  #---------------------------------------------\n",
        "  if info_chk_flg == 1:\n",
        "\n",
        "    result_list = []\n",
        "    rvs_wd_rt_list = []\n",
        "\n",
        "    asitis_cng_words1 = ''\n",
        "    asitis_cng_words2 = ''\n",
        "    asitis_cng_words3 = ''\n",
        "\n",
        "    rvs_wd_list = model.most_similar(positive=[words,gyaku])\n",
        "\n",
        "    index = 0\n",
        "\n",
        "    for index in range(len(rvs_wd_list)):\n",
        "      wd = rvs_wd_list[index][0]\n",
        "      if str(wd) not in no_list:\n",
        "\n",
        "        #対義語候補格納\n",
        "        rvs_wd_rt_list.append(lib_delete_brackets.delete_brackets(wd))\n",
        "    \n",
        "    #②-1は「2候補」のみセット\n",
        "    asitis_cng_words1 = lib_delete_brackets.delete_brackets(rvs_wd_rt_list[0])\n",
        "    asitis_cng_words2 = lib_delete_brackets.delete_brackets(rvs_wd_rt_list[1])\n",
        "\n",
        "    asitis_cng_words1 = asitis_cng_words1.replace('_',\"\").replace('[',\"\").replace(']',\"\")\n",
        "    asitis_cng_words2 = asitis_cng_words2.replace('_',\"\").replace('[',\"\").replace(']',\"\")\n",
        "\n",
        "    #返却値書き込み\n",
        "    result_list.append(words.replace('[',\"\").replace(']',\"\"))\n",
        "    result_list.append('')\n",
        "    result_list.append(asitis_cng_words1)\n",
        "    result_list.append(asitis_cng_words2)\n",
        "    \n",
        "    #この時点ではreturnなし\n",
        "\n",
        "    #---------------------------------------------\n",
        "    #②-2 形態素分析後に強制的に対義語化\n",
        "    #---------------------------------------------\n",
        "    word_cng_str = ''\n",
        "    \n",
        "    node = tokenizer.parseToNode(words)\n",
        "\n",
        "    while node:\n",
        "      cut_wd = node.surface\n",
        "      \n",
        "      #特定固有名詞の囲み文字は処理対象外\n",
        "      if(cut_wd != \"[\" and cut_wd != \"]\" and cut_wd != \"(\" and cut_wd != \")\"):\n",
        "        if node.feature.split(\",\")[0] == u\"名詞\":\n",
        "          try:\n",
        "            rvs_wd_list = model.most_similar(positive=[cut_wd,gyaku])\n",
        "            for i in range(4):\n",
        "              rvs_wd = rvs_wd_list[i][0]\n",
        "              if (cut_wd != rvs_wd.replace('[',\"\").replace(']',\"\") and\n",
        "                 str(wd) not in no_list):\n",
        "                #◆結合\n",
        "                word_cng_str = word_cng_str + rvs_wd.replace('[',\"\").replace(']',\"\")\n",
        "                break\n",
        "          except KeyError as error:\n",
        "            #辞書に登録の無い単語の場合\n",
        "            pass\n",
        "        elif (node.feature.split(\",\")[0] == u\"動詞\" or\n",
        "          node.feature.split(\",\")[0] == u\"形容詞\" or\n",
        "          node.feature.split(\",\")[0] == u\"副詞\" or\n",
        "          node.feature.split(\",\")[0] == u\"感動詞\"):\n",
        "          cut_wd = node.feature.split(\",\")[6]\n",
        "\n",
        "          try:\n",
        "            rvs_wd_list = model.most_similar(positive=[cut_wd,gyaku])\n",
        "            for i in range(4):\n",
        "              rvs_wd = rvs_wd_list[i][0]\n",
        "              if (cut_wd != rvs_wd.replace('[',\"\").replace(']',\"\") and\n",
        "                 str(wd) not in no_list):\n",
        "                #◆結合\n",
        "                word_cng_str = word_cng_str + rvs_wd.replace('[',\"\").replace(']',\"\")\n",
        "                break\n",
        "          except KeyError as error:\n",
        "            #辞書に登録の無い単語の場合\n",
        "            pass\n",
        "\n",
        "        else:\n",
        "          #◆結合\n",
        "          word_cng_str = word_cng_str + cut_wd.replace('[',\"\").replace(']',\"\")\n",
        "\n",
        "      node = node.next\n",
        "\n",
        "    #返却値書き込み\n",
        "    #②-2は「1候補」のみセット\n",
        "    asitis_cng_words3 = word_cng_str.replace('_',\"\")\n",
        "    result_list.append(lib_delete_brackets.delete_brackets(asitis_cng_words3))\n",
        "\n",
        "    return result_list\n",
        "\n",
        "  #**************************************************************************\n",
        "  #③ノーマル語彙\n",
        "  #**************************************************************************\n",
        "  #---------------------------------------------\n",
        "  #形態素分析後に強制的に対義語化\n",
        "  #---------------------------------------------\n",
        "  if info_chk_flg == 3:\n",
        "    \n",
        "    node = tokenizer.parseToNode(words)\n",
        "\n",
        "    while node:\n",
        "      cut_wd = node.surface\n",
        "      \n",
        "      #特定固有名詞は処理対象外\n",
        "      if(cut_wd != \"[\" and cut_wd != \"]\" and cut_wd != \"(\" and cut_wd != \")\"):\n",
        "        if node.feature.split(\",\")[0] == u\"名詞\":\n",
        "          try:\n",
        "            rvs_wd_list = model.most_similar(positive=[cut_wd,gyaku])\n",
        "            for i in range(4):\n",
        "              rvs_wd = rvs_wd_list[i][0]\n",
        "              if (cut_wd != rvs_wd.replace('[',\"\").replace(']',\"\") and\n",
        "                 str(wd) not in no_list):\n",
        "                #◆結合\n",
        "                word_cng_str = word_cng_str + rvs_wd.replace('[',\"\").replace(']',\"\")\n",
        "                break\n",
        "          except KeyError as error:\n",
        "            #辞書に登録の無い単語の場合\n",
        "            pass\n",
        "        elif (node.feature.split(\",\")[0] == u\"動詞\" or\n",
        "          node.feature.split(\",\")[0] == u\"形容詞\" or\n",
        "          node.feature.split(\",\")[0] == u\"副詞\" or\n",
        "          node.feature.split(\",\")[0] == u\"感動詞\"):\n",
        "          cut_wd = node.feature.split(\",\")[6]\n",
        "\n",
        "          try:\n",
        "            rvs_wd_list = model.most_similar(positive=[cut_wd,gyaku])\n",
        "            for i in range(4):\n",
        "              rvs_wd = rvs_wd_list[i][0]\n",
        "              if (cut_wd != rvs_wd.replace('[',\"\").replace(']',\"\") and\n",
        "                 str(wd) not in no_list):\n",
        "                #◆結合\n",
        "                word_cng_str = word_cng_str + rvs_wd.replace('[',\"\").replace(']',\"\")\n",
        "                break\n",
        "          except KeyError as error:\n",
        "            #辞書に登録の無い単語の場合\n",
        "            pass\n",
        "\n",
        "        else:\n",
        "          #◆結合\n",
        "          word_cng_str = word_cng_str + cut_wd.replace('[',\"\").replace(']',\"\")\n",
        "\n",
        "      node = node.next\n",
        "\n",
        "    result_list = []\n",
        "\n",
        "    #返却値書き込み\n",
        "    asitis_cng_words1 = word_cng_str.replace('_',\"\")\n",
        "    result_list.append(words.replace('(',\"\").replace(')',\"\"))\n",
        "    result_list.append('')\n",
        "    result_list.append(lib_delete_brackets.delete_brackets(asitis_cng_words1))\n",
        "    result_list.append('')\n",
        "    result_list.append('')\n",
        "\n",
        "    return result_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp-Cbw6x2-Ro",
        "colab_type": "text"
      },
      "source": [
        "##(関数呼出)対義語-対応list作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQGM66ine9J3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import MeCab\n",
        "#対義語生成\n",
        "import lib_delete_brackets\n",
        "#import lib_wordRevChange as lw\n",
        "\n",
        "tokenizer = MeCab.Tagger(\"-Ochasen\")\n",
        "\n",
        "cnt = 1\n",
        "save_list = []\n",
        "\n",
        "jpn_start_flg = 1\n",
        "\n",
        "#列[words]をnumpyに変換\n",
        "val = df.words.values\n",
        "\n",
        "for idx in range(df.shape[0]):\n",
        "\n",
        "  words = str(val[idx])\n",
        "  \n",
        "#   #日本語単語の開始判定\n",
        "#   if words[0:1] == \"あ\":\n",
        "#     jpn_start_flg = 1\n",
        "\n",
        "#   #日本語単語の終了判定\n",
        "#   if words[0:1] == \"龍\":\n",
        "#     break\n",
        "  \n",
        "  wd_len_chk = lib_delete_brackets.delete_brackets(words.replace('[',\"\").replace(']',\"\"))\n",
        "  \n",
        "  #語彙が10文字以下のみ対象\n",
        "  if len(wd_len_chk) <= 10:  \n",
        "    if jpn_start_flg == 1:\n",
        "      if words.isdecimal() is False:\n",
        "        if '・' not in words:\n",
        "          gyaku = u\"逆\"\n",
        "          rev_list = wordRevChange(words,gyaku,model,tokenizer)\n",
        "\n",
        "          #返却値ありのみ格納\n",
        "          if rev_list is not None:\n",
        "            save_list.append(rev_list)\n",
        "\n",
        "  if cnt % 500 == 0:\n",
        "    print(cnt,\"件 終了\")\n",
        "\n",
        "    if cnt % 3000 == 0:\n",
        "      #Gドライブ再マウント\n",
        "      drive.mount('/content/drive')  \n",
        "      #保存\n",
        "      df2 = pd.DataFrame(save_list)\n",
        "      df2.to_csv('./drive/My Drive/NLP/modelToCsv.csv', header=False, index=False)\n",
        "    \n",
        "  cnt = cnt + 1\n",
        "  \n",
        "#Gドライブ再マウント\n",
        "drive.mount('/content/drive')  \n",
        "#保存\n",
        "df2 = pd.DataFrame(save_list)\n",
        "df2.to_csv('./drive/My Drive/NLP/modelToCsv.csv', header=False, index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2q1efaRhvj6",
        "colab_type": "text"
      },
      "source": [
        "##以下、検証用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKoCJxYi8Fl_",
        "colab_type": "code",
        "outputId": "2a3f1500-7e6d-4a36-86f5-9ef9b203c79e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install mecab-python3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/92/e7e7f38df8457fa40c1ca86928be5ddbe2bf341e90a35e6ada30d03ef16d/mecab_python3-0.996.2-cp36-cp36m-manylinux1_x86_64.whl (15.9MB)\n",
            "\u001b[K     |████████████████████████████████| 15.9MB 6.7MB/s \n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.996.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCVilfCaxmGp",
        "colab_type": "code",
        "outputId": "ad65550c-53d5-4b60-e3d2-39395a21d9c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "csv_dir = './drive/My Drive/NLP/modelToCsv.csv'\n",
        "df_ant = pd.read_csv(csv_dir,names=['words','info','ant1','ant2','ant3'])\n",
        "df_ant = df_ant.sort_values('words', ascending=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jitj5RgH2bAt",
        "colab_type": "text"
      },
      "source": [
        "##(取得)対義語関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sqLQwqOxmci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import MeCab\n",
        "import numpy as np\n",
        "\n",
        "def get_ant_word(words):\n",
        "\n",
        "  word_cng_list = []\n",
        "\n",
        "  #-------------------------------------------------\n",
        "  # そのまま対義語化\n",
        "  #-------------------------------------------------\n",
        "  dfTolist = df_ant[df_ant[\"words\"] == words].values.tolist()\n",
        "  ant_list = [flatten for inner in dfTolist for flatten in inner]\n",
        "  \n",
        "  #-------------------------------------------------\n",
        "  # 形態素分析後に対義語化\n",
        "  #-------------------------------------------------\n",
        "  if len(dfTolist) == 0:\n",
        "    tokenizer = MeCab.Tagger(\"-Ochasen\")\n",
        "    node = tokenizer.parseToNode(words)\n",
        "    ant_word = ''\n",
        "    rvs_wd = ''\n",
        "    \n",
        "    while node:\n",
        "      #分かち書きの単語を取得\n",
        "      cut_wd = node.surface\n",
        "      \n",
        "      #数字はそのまま\n",
        "      if cut_wd.isnumeric():\n",
        "        ant_word = ant_word + cut_wd \n",
        "      else:      \n",
        "        if cut_wd != np.nan and cut_wd != '':\n",
        "          if node.feature.split(\",\")[0] == u\"名詞\":\n",
        "            try:\n",
        "              rvs_wd = df_ant[df_ant[\"words\"] == cut_wd].values[0][2]\n",
        "              if rvs_wd is np.nan or rvs_wd == cut_wd:\n",
        "                rvs_wd = df_ant[df_ant[\"words\"] == cut_wd].values[0][3]\n",
        "                if  rvs_wd is np.nan or rvs_wd == cut_wd:\n",
        "                  rvs_wd = df_ant[df_ant[\"words\"] == cut_wd].values[0][4]\n",
        "\n",
        "              ant_word = ant_word + str(rvs_wd)\n",
        "\n",
        "            except IndexError as error:\n",
        "              #辞書に登録の無い単語の場合\n",
        "              ant_word = ant_word + str(cut_wd)\n",
        "\n",
        "          elif (node.feature.split(\",\")[0] == u\"動詞\" or\n",
        "            node.feature.split(\",\")[0] == u\"形容詞\" or\n",
        "            node.feature.split(\",\")[0] == u\"副詞\" or\n",
        "            node.feature.split(\",\")[0] == u\"感動詞\"):\n",
        "\n",
        "            #分かち書きの単語を取得\n",
        "            cut_wd = node.feature.split(\",\")[6]\n",
        "\n",
        "            try:\n",
        "              rvs_wd = df_ant[df_ant[\"words\"] == cut_wd].values[0][2]\n",
        "              if rvs_wd is np.nan or rvs_wd == cut_wd:\n",
        "                rvs_wd = df_ant[df_ant[\"words\"] == cut_wd].values[0][3]\n",
        "                if rvs_wd is np.nan or rvs_wd == cut_wd:\n",
        "                  rvs_wd = df_ant[df_ant[\"words\"] == cut_wd].values[0][4]\n",
        "\n",
        "              ant_word = ant_word + str(rvs_wd)\n",
        "\n",
        "            except IndexError as error:\n",
        "              #辞書に登録の無い単語の場合\n",
        "              ant_word = ant_word + str(cut_wd)\n",
        "\n",
        "          else:\n",
        "            #◆結合\n",
        "            ant_word = ant_word + str(cut_wd)\n",
        "\n",
        "      node = node.next\n",
        "\n",
        "    word_cng_list.append(ant_word)\n",
        "  \n",
        "    return word_cng_list\n",
        "  else:\n",
        "    return ant_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULkjDK6bSyw-",
        "colab_type": "code",
        "outputId": "86ac921c-b5cf-4bfa-bc1e-f39a934bf3d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(get_ant_word(\"ライオンキング\"))\n"
      ],
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ライオンキング', nan, 'ライオン・キング', 'ウィキッド', '尻尾キッド']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJd7lELteNO-",
        "colab_type": "code",
        "outputId": "a9fd5073-4b57-4213-b658-bc8d4c5dc0b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "#df_ant[df_ant['words'].str.contains('風邪',na=False)]\n",
        "df_ant[df_ant['words']==\"ライオンキング\"]"
      ],
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>info</th>\n",
              "      <th>ant1</th>\n",
              "      <th>ant2</th>\n",
              "      <th>ant3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>79766</th>\n",
              "      <td>ライオンキング</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ライオン・キング</td>\n",
              "      <td>ウィキッド</td>\n",
              "      <td>尻尾キッド</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         words  info      ant1   ant2   ant3\n",
              "79766  ライオンキング   NaN  ライオン・キング  ウィキッド  尻尾キッド"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 284
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVubk5ePVpIU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3e997d0f-5152-4a22-d7ea-1a84cc9e6983"
      },
      "source": [
        "tokenizer = MeCab.Tagger(\"-Ochasen\")\n",
        "node = tokenizer.parseToNode(\"テクノブレイク\")\n",
        "\n",
        "while node:\n",
        "  print(node.feature.split(\",\")[0])\n",
        "  node = node.next\n"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOS/EOS\n",
            "名詞\n",
            "BOS/EOS\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}