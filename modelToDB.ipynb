{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modelToDB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soichi-fujiwara/jupyter-notebook/blob/master/modelToDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCMfweVZEzPd",
        "colab_type": "text"
      },
      "source": [
        "##モデルをDB化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhMbBx_ECoOl",
        "colab_type": "code",
        "outputId": "bb369ab2-5547-44fb-e9d8-bf4bf7471e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install mecab-python3\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYLgNCFduL-w",
        "colab_type": "code",
        "outputId": "2b2479c2-d07d-4280-f0ae-f9da29f8603f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cat /proc/uptime | awk '{print $1 /60 /60 /24 \"days (\" $1 \"sec)\"}'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.00161192days (139.27sec)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRJCjIdrc3nQ",
        "colab_type": "code",
        "outputId": "8573fec3-d55c-4006-fd8f-5154e11dd58c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import IPython\n",
        "#bz2ファイル解凍\n",
        "#!bunzip2 -k -q \"./drive/My Drive/NLP/20170201.tar.bz2\"\n",
        "\n",
        "#tarファイル解凍\n",
        "!tar -xvf \"./drive/My Drive/NLP/20170201.tar\"\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "#解凍時のみ\n",
        "model_dir = 'entity_vector/entity_vector.model.bin'\n",
        "model = KeyedVectors.load_word2vec_format(model_dir, binary=True,limit=500000)\n",
        "\n",
        "#解凍後確認\n",
        "results = model.most_similar(positive=['柔らかい'],topn=5)\n",
        "for result in results:\n",
        "  print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "entity_vector/\n",
            "entity_vector/entity_vector.model.txt\n",
            "entity_vector/entity_vector.model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OzKgJ9slAsf",
        "colab_type": "text"
      },
      "source": [
        "##語彙リスト作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5yIYDAfPHQu",
        "colab_type": "code",
        "outputId": "7b33b2b0-a43e-4257-a265-421684ff8f50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "cnt = 1\n",
        "model_vocab_list = []\n",
        "\n",
        "for words, v in model.vocab.items():\n",
        "\n",
        "  #固有名詞用の括弧を削除\n",
        "  #words = words.replace('[', '').replace(']', '')\n",
        "\n",
        "  #-----------------------------------------------------  \n",
        "\n",
        "  # 半角記号+半角数字\n",
        "  #p = re.compile(\"[!-@[-`{-~]\")\n",
        "  \n",
        "  # 半角記号+半角数字+半角英字\n",
        "  #((ほぼ)日本語のみのDB作成用)\n",
        "  p = re.compile(\"[!-~]\")\n",
        "\n",
        "  # 先頭文字\n",
        "  top_char = words[0:1]\n",
        "\n",
        "  #-----------------------------------------------------  \n",
        "\n",
        "#   【M1】dfに格納した後で補足情報を削除する為、ココでは補足情報を残しておく\n",
        "#   #補足情報の_を削除\n",
        "#   words = words.replace(\"_\",\"\")\n",
        "#   #括弧()文字を抽出\n",
        "#   regex = re.compile(\".*?\\((.*?)\\)\")\n",
        "#   #括弧文字をlist型で返却\n",
        "#   ret_list = re.findall(regex, words)\n",
        "\n",
        "  #-----------------------------------------------------  \n",
        "\n",
        "#   #除外文字なし\n",
        "#   if p.match(top_char) is None:\n",
        "#     #括弧文字がある場合は無効化\n",
        "#     if len(ret_list) > 0: \n",
        "#       words = words.replace(\"(\" + ret_list[0] + \")\",'')\n",
        "#       model_vocab_list.append(words)\n",
        "#     else:\n",
        "#       model_vocab_list.append(words)\n",
        "\n",
        "  #【M1】\n",
        "  model_vocab_list.append(words)\n",
        "  #【M1】\n",
        "  \n",
        "  if cnt % 50000 == 0:\n",
        "    print(cnt,\"件 完了\");\n",
        "    \n",
        "  cnt = cnt + 1\n",
        "\n",
        "#重複削除\n",
        "model_vocab_uni_list= list(set(model_vocab_list))\n",
        "  \n",
        "#保存\n",
        "drive.mount('/content/drive')\n",
        "df = pd.DataFrame(model_vocab_uni_list)\n",
        "df.to_csv('./drive/My Drive/NLP/model_vocab_list.csv', header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 件 完了\n",
            "100000 件 完了\n",
            "150000 件 完了\n",
            "200000 件 完了\n",
            "250000 件 完了\n",
            "300000 件 完了\n",
            "350000 件 完了\n",
            "400000 件 完了\n",
            "450000 件 完了\n",
            "500000 件 完了\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "190yBfA8Wayj",
        "colab_type": "text"
      },
      "source": [
        "##語彙リスト読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRvxKV48SLUM",
        "colab_type": "code",
        "outputId": "722fd2ac-8c0d-460f-899d-f921d0b11d9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "csv_dir = './drive/My Drive/NLP/model_vocab_list.csv'\n",
        "df = pd.read_csv(csv_dir,names=['words'])\n",
        "df = df.sort_values('words', ascending=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsgpqqphWIw4",
        "colab_type": "text"
      },
      "source": [
        "##(関数)対義語-対応list作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdwy6HUsTA_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import MeCab\n",
        "#括弧記号無効化\n",
        "import lib_delete_brackets\n",
        "\n",
        "def search_similar_texts(p_words):\n",
        "  pass\n",
        "\n",
        "def wordRevChange(words,gyaku,model,tokenizer):\n",
        "\n",
        "  no_list = ['混一色','ぎゃく','必然的','意図的','結果的','下向き','場合','横方向','かえって']\n",
        "\n",
        "  #**************************************************************************\n",
        "  #補足情報有無判定\n",
        "  #**************************************************************************\n",
        "  if '_(' in words:\n",
        "    #補足情報あり･･･①\n",
        "    info_chk_flg = 0\n",
        "  else:\n",
        "    #補足情報なし and 記事名･･･②\n",
        "    if '[' in words:\n",
        "      info_chk_flg = 1\n",
        "    else:\n",
        "      #ノーマル語彙･･･③\n",
        "      info_chk_flg = 2\n",
        "  \n",
        "  #**************************************************************************\n",
        "  #①そのまま対義語化\n",
        "  #**************************************************************************\n",
        "  if info_chk_flg == 0:\n",
        "    asitis_cng_words1 = ''\n",
        "    asitis_cng_words2 = ''\n",
        "    asitis_cng_words3 = ''\n",
        "\n",
        "    #--------------------------------------------------------\n",
        "    #★対義語リスト取得\n",
        "    #--------------------------------------------------------\n",
        "    rvs_wd_list = model.most_similar(positive=[words,gyaku])\n",
        "\n",
        "    index = 0\n",
        "    rvs_wd_rt_list = []\n",
        "\n",
        "    for index in range(len(rvs_wd_list)):\n",
        "      wd = rvs_wd_list[index][0]\n",
        "      if str(wd) not in no_list:\n",
        "\n",
        "        #対義語候補格納\n",
        "        rvs_wd_rt_list.append(lib_delete_brackets.delete_brackets(wd))\n",
        "\n",
        "    result_list = []\n",
        "\n",
        "    asitis_cng_words1 = lib_delete_brackets.delete_brackets(rvs_wd_rt_list[0])\n",
        "    asitis_cng_words2 = lib_delete_brackets.delete_brackets(rvs_wd_rt_list[1])\n",
        "    asitis_cng_words3 = lib_delete_brackets.delete_brackets(rvs_wd_rt_list[2])\n",
        "\n",
        "    asitis_cng_words1 = asitis_cng_words1.replace('_',\"\").replace('[',\"\").replace(']',\"\")\n",
        "    asitis_cng_words2 = asitis_cng_words2.replace('_',\"\").replace('[',\"\").replace(']',\"\")\n",
        "    asitis_cng_words3 = asitis_cng_words3.replace('_',\"\").replace('[',\"\").replace(']',\"\")\n",
        "\n",
        "    #返却値書き込み\n",
        "    result_list.append(words.replace('[',\"\").replace(']',\"\"))\n",
        "    result_list.append(asitis_cng_words1)\n",
        "    result_list.append(asitis_cng_words2)\n",
        "    result_list.append(asitis_cng_words3)\n",
        "    result_list.append(info_chk_flg)\n",
        "\n",
        "    return result_list\n",
        "\n",
        "  #**************************************************************************\n",
        "  #②記事名\n",
        "  #**************************************************************************\n",
        "  #---------------------------------------------\n",
        "  #②-1 記事名をそのまま対義語化\n",
        "  #---------------------------------------------\n",
        "  if info_chk_flg == 1:\n",
        "\n",
        "    result_list = []\n",
        "    rvs_wd_rt_list = []\n",
        "\n",
        "    asitis_cng_words1 = ''\n",
        "    asitis_cng_words2 = ''\n",
        "    asitis_cng_words3 = ''\n",
        "\n",
        "    #--------------------------------------------------------\n",
        "    #★対義語リスト取得\n",
        "    #--------------------------------------------------------\n",
        "    rvs_wd_list = model.most_similar(positive=[words,gyaku])\n",
        "\n",
        "    index = 0\n",
        "\n",
        "    for index in range(len(rvs_wd_list)):\n",
        "      wd = rvs_wd_list[index][0]\n",
        "      if str(wd) not in no_list:\n",
        "\n",
        "        #対義語候補格納\n",
        "        rvs_wd_rt_list.append(lib_delete_brackets.delete_brackets(wd))\n",
        "    \n",
        "    #②-1は「1候補」のみセット\n",
        "    asitis_cng_words1 = lib_delete_brackets.delete_brackets(rvs_wd_rt_list[0])\n",
        "    #asitis_cng_words2 = lib_delete_brackets.delete_brackets(rvs_wd_rt_list[1])\n",
        "\n",
        "    asitis_cng_words1 = asitis_cng_words1.replace('_',\"\").replace('[',\"\").replace(']',\"\")\n",
        "    #asitis_cng_words2 = asitis_cng_words2.replace('_',\"\").replace('[',\"\").replace(']',\"\")\n",
        "\n",
        "    #返却値書き込み\n",
        "    result_list.append(words.replace('[',\"\").replace(']',\"\"))\n",
        "    result_list.append(asitis_cng_words1)\n",
        "    #result_list.append(asitis_cng_words2)\n",
        "    \n",
        "    #この時点ではreturnなし\n",
        "\n",
        "    #---------------------------------------------\n",
        "    #②-2 形態素分析後に強制的に対義語化\n",
        "    #---------------------------------------------\n",
        "    node = tokenizer.parseToNode(words)\n",
        "\n",
        "    word_cng_str1 = ''\n",
        "    word_cng_str2 = ''\n",
        "    \n",
        "    while node:\n",
        "      cut_wd = node.surface\n",
        "      \n",
        "      #特定固有名詞の囲み文字は処理対象外\n",
        "      if(cut_wd != \"[\" and cut_wd != \"]\" and cut_wd != \"(\" and cut_wd != \")\" and cut_wd != \"_\"):\n",
        "        if node.feature.split(\",\")[0] == u\"名詞\":\n",
        "          try:\n",
        "            #--------------------------------------------------------\n",
        "            #★対義語リスト取得\n",
        "            #--------------------------------------------------------\n",
        "            rvs_wd_list = model.most_similar(positive=[cut_wd,gyaku])\n",
        "            for i in range(4):\n",
        "              wd1 = rvs_wd_list[i][0].replace('[',\"\").replace(']',\"\")\n",
        "              if (cut_wd != wd1 and\n",
        "                 str(wd1) not in no_list):\n",
        "                #◆結合1\n",
        "                word_cng_str1 = word_cng_str1 + wd1\n",
        "\n",
        "                #◆結合2\n",
        "                for ii in range(4):\n",
        "                  wd2 = rvs_wd_list[i+ii+1][0].replace('[',\"\").replace(']',\"\")\n",
        "                  if str(wd2) not in no_list:\n",
        "                    word_cng_str2 = word_cng_str2 + wd2\n",
        "                    break\n",
        "\n",
        "                break\n",
        "          except KeyError as error:\n",
        "            #辞書に登録の無い単語の場合\n",
        "            word_cng_str1 = word_cng_str1 + cut_wd\n",
        "            word_cng_str2 = word_cng_str2 + cut_wd\n",
        "\n",
        "        elif (node.feature.split(\",\")[0] == u\"動詞\" or\n",
        "          node.feature.split(\",\")[0] == u\"形容詞\" or\n",
        "          node.feature.split(\",\")[0] == u\"副詞\" or\n",
        "          node.feature.split(\",\")[0] == u\"感動詞\"):\n",
        "          cut_wd = node.feature.split(\",\")[6]\n",
        "\n",
        "          try:\n",
        "            #--------------------------------------------------------\n",
        "            #★対義語リスト取得\n",
        "            #--------------------------------------------------------\n",
        "            rvs_wd_list = model.most_similar(positive=[cut_wd,gyaku])\n",
        "            for i in range(4):\n",
        "              wd1 = rvs_wd_list[i][0].replace('[',\"\").replace(']',\"\")\n",
        "              if (cut_wd != wd1 and\n",
        "                 str(wd1) not in no_list):\n",
        "                #◆結合1\n",
        "                word_cng_str1 = word_cng_str1 + wd1\n",
        "\n",
        "                #◆結合2\n",
        "                for ii in range(4):\n",
        "                  wd2 = rvs_wd_list[i+ii+1][0].replace('[',\"\").replace(']',\"\")\n",
        "                  if str(wd2) not in no_list:\n",
        "                    word_cng_str2 = word_cng_str2 + wd2\n",
        "                    break\n",
        "                    \n",
        "                break\n",
        "          except KeyError as error:\n",
        "            #辞書に登録の無い単語の場合\n",
        "            word_cng_str1 = word_cng_str1 + cut_wd\n",
        "            word_cng_str2 = word_cng_str2 + cut_wd\n",
        "\n",
        "        else:\n",
        "          #◆結合\n",
        "          word_cng_str1 = word_cng_str1 + cut_wd.replace('[',\"\").replace(']',\"\")\n",
        "          word_cng_str2 = word_cng_str2 + cut_wd.replace('[',\"\").replace(']',\"\")\n",
        "\n",
        "      node = node.next\n",
        "\n",
        "    #返却値書き込み\n",
        "    #②-2は「2候補」セット\n",
        "    asitis_cng_words2 = word_cng_str1.replace('_',\"\")\n",
        "    asitis_cng_words3 = word_cng_str2.replace('_',\"\")\n",
        "\n",
        "    result_list.append(lib_delete_brackets.delete_brackets(asitis_cng_words2))\n",
        "    result_list.append(lib_delete_brackets.delete_brackets(asitis_cng_words3))\n",
        "    result_list.append(info_chk_flg)\n",
        "\n",
        "    return result_list\n",
        "\n",
        "  #**************************************************************************\n",
        "  #③ノーマル語彙\n",
        "  #**************************************************************************\n",
        "  #---------------------------------------------\n",
        "  #形態素分析後に強制的に対義語化\n",
        "  #---------------------------------------------\n",
        "  if info_chk_flg == 2:\n",
        "    \n",
        "    word_cng_str1 = ''\n",
        "    word_cng_str2 = ''\n",
        "    word_cng_str3 = ''\n",
        "\n",
        "    node = tokenizer.parseToNode(words)\n",
        "\n",
        "    while node:\n",
        "      cut_wd = node.surface\n",
        "      \n",
        "      #特定固有名詞は処理対象外\n",
        "      if(cut_wd != \"[\" and cut_wd != \"]\" and cut_wd != \"(\" and cut_wd != \")\" and cut_wd != \"_\"):\n",
        "        if node.feature.split(\",\")[0] == u\"名詞\":\n",
        "          try:\n",
        "            #--------------------------------------------------------\n",
        "            #★対義語リスト取得\n",
        "            #--------------------------------------------------------\n",
        "            rvs_wd_list = model.most_similar(positive=[cut_wd,gyaku])\n",
        "            for i in range(4):\n",
        "              wd1 = rvs_wd_list[i][0].replace('[',\"\").replace(']',\"\")\n",
        "              if (cut_wd != wd1 and\n",
        "                 str(wd1) not in no_list):\n",
        "                #◆結合1\n",
        "                word_cng_str1 = word_cng_str1 + wd1\n",
        "                \n",
        "                #◆結合2\n",
        "                for ii in range(4):\n",
        "                  wd2 = rvs_wd_list[i+ii+1][0].replace('[',\"\").replace(']',\"\")\n",
        "                  if str(wd2) not in no_list:\n",
        "                    word_cng_str2 = word_cng_str2 + wd2\n",
        "                    ii_save = i+ii+1\n",
        "                    \n",
        "                    #◆結合3\n",
        "                    for iii in range(4):\n",
        "                      wd3 = rvs_wd_list[ii_save+iii+1][0].replace('[',\"\").replace(']',\"\")\n",
        "                      if str(wd3) not in no_list:\n",
        "                        word_cng_str3 = word_cng_str3 + wd3\n",
        "                        break\n",
        "                    break\n",
        "                break\n",
        "          except KeyError as error:\n",
        "            #辞書に登録の無い単語の場合\n",
        "            word_cng_str1 = word_cng_str1 + cut_wd\n",
        "            word_cng_str2 = word_cng_str2 + cut_wd\n",
        "            word_cng_str3 = word_cng_str3 + cut_wd\n",
        "          \n",
        "        elif (node.feature.split(\",\")[0] == u\"動詞\" or\n",
        "          node.feature.split(\",\")[0] == u\"形容詞\" or\n",
        "          node.feature.split(\",\")[0] == u\"副詞\" or\n",
        "          node.feature.split(\",\")[0] == u\"感動詞\"):\n",
        "          cut_wd = node.feature.split(\",\")[6]\n",
        "\n",
        "          try:\n",
        "            #--------------------------------------------------------\n",
        "            #★対義語リスト取得\n",
        "            #--------------------------------------------------------\n",
        "            rvs_wd_list = model.most_similar(positive=[cut_wd,gyaku])\n",
        "            for i in range(4):\n",
        "              wd1 = rvs_wd_list[i][0].replace('[',\"\").replace(']',\"\")\n",
        "              if (cut_wd != wd1 and\n",
        "                 str(wd1) not in no_list):\n",
        "                #◆結合1\n",
        "                word_cng_str1 = word_cng_str1 + wd1\n",
        "                \n",
        "                #◆結合2\n",
        "                for ii in range(4):\n",
        "                  wd2 = rvs_wd_list[i+ii+1][0].replace('[',\"\").replace(']',\"\")\n",
        "                  if str(wd2) not in no_list:\n",
        "                    word_cng_str2 = word_cng_str2 + wd2\n",
        "                    ii_save = i+ii+1\n",
        "\n",
        "                    #◆結合3\n",
        "                    for iii in range(4):\n",
        "                      wd3 = rvs_wd_list[ii_save+iii+1][0].replace('[',\"\").replace(']',\"\")\n",
        "                      if str(wd3) not in no_list:\n",
        "                        word_cng_str3 = word_cng_str3 + wd3\n",
        "                        break\n",
        "                    break\n",
        "                break\n",
        "          except KeyError as error:\n",
        "            #辞書に登録の無い単語の場合\n",
        "            word_cng_str1 = word_cng_str1 + cut_wd\n",
        "            word_cng_str2 = word_cng_str2 + cut_wd\n",
        "            word_cng_str3 = word_cng_str3 + cut_wd\n",
        "\n",
        "        else:\n",
        "          #◆結合\n",
        "          word_cng_str1 = word_cng_str1 + cut_wd.replace('[',\"\").replace(']',\"\")\n",
        "          word_cng_str2 = word_cng_str2 + cut_wd.replace('[',\"\").replace(']',\"\")\n",
        "          word_cng_str3 = word_cng_str3 + cut_wd.replace('[',\"\").replace(']',\"\")\n",
        "\n",
        "      node = node.next\n",
        "\n",
        "    result_list = []\n",
        "\n",
        "    #返却値書き込み\n",
        "    asitis_cng_words1 = word_cng_str1.replace('_',\"\")\n",
        "    asitis_cng_words2 = word_cng_str2.replace('_',\"\")\n",
        "    asitis_cng_words3 = word_cng_str3.replace('_',\"\")\n",
        "\n",
        "    result_list.append(words.replace('(',\"\").replace(')',\"\"))\n",
        "    result_list.append(lib_delete_brackets.delete_brackets(asitis_cng_words1))\n",
        "    result_list.append(lib_delete_brackets.delete_brackets(asitis_cng_words2))\n",
        "    result_list.append(lib_delete_brackets.delete_brackets(asitis_cng_words3))\n",
        "    result_list.append(info_chk_flg)\n",
        "\n",
        "    return result_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp-Cbw6x2-Ro",
        "colab_type": "text"
      },
      "source": [
        "##(関数呼出)対義語-対応list作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQGM66ine9J3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import MeCab\n",
        "#対義語生成\n",
        "import lib_delete_brackets\n",
        "#import lib_wordRevChange as lw\n",
        "\n",
        "tokenizer = MeCab.Tagger(\"-Ochasen\")\n",
        "owkt = MeCab.Tagger(\"-Owakati\")        \n",
        "\n",
        "cnt = 1\n",
        "save_list = []\n",
        "del_char_list = [\"も\",\"が\",\"は\",\"の\",\"で\",\"に\",\"を\",\"と\",\"や\",\"か\"]\n",
        "\n",
        "jpn_start_flg = 0\n",
        "\n",
        "#列[words]をnumpyに変換\n",
        "val = df.words.values\n",
        "\n",
        "for idx in range(df.shape[0]):\n",
        "\n",
        "  words = str(val[idx])\n",
        "  \n",
        "  #日本語単語の開始判定\n",
        "  if words[0:2] == \"嘶き\":\n",
        "    jpn_start_flg = 1\n",
        "\n",
        "#   #日本語単語の終了判定\n",
        "#   if words[0:1] == \"龍\":\n",
        "#     break\n",
        "  \n",
        "  if jpn_start_flg == 1:\n",
        "\n",
        "    #語彙数チェック\n",
        "    cnt_words_str = lib_delete_brackets.delete_brackets(words.replace(\"[\",\"\").replace(\"]\",\"\")).replace(\"_\",\"\")\n",
        "\n",
        "    #10文字以下が対象\n",
        "    if len(cnt_words_str) <= 10:\n",
        "      result_owkt = owkt.parse(cnt_words_str)\n",
        "      owkt_list = result_owkt.split(' ')\n",
        "      goi_cnt = len(owkt_list)-1\n",
        "    else:\n",
        "      goi_cnt = 99\n",
        "      \n",
        "#     for del_char in del_char_list:\n",
        "#       try:\n",
        "#         owkt_list.remove(del_char)\n",
        "#       except ValueError as error:\n",
        "#         pass\n",
        "\n",
        "    #語彙に含まれる形態素数が5以下のみ処理対象\n",
        "    if goi_cnt <= 5:\n",
        "      if '#' not in words:\n",
        "        if ':' not in words:\n",
        "          if '・' not in words:\n",
        "            if words.isdecimal() is False:\n",
        "              gyaku = u\"逆\"\n",
        "              rev_list = wordRevChange(words,gyaku,model,tokenizer)\n",
        "\n",
        "              #返却値ありのみ格納\n",
        "              if rev_list is not None:\n",
        "                save_list.append(rev_list)\n",
        "\n",
        "  if cnt % 100 == 0:\n",
        "    print(cnt,\"件 終了\")\n",
        "\n",
        "    if cnt % 1500 == 0:\n",
        "      #Gドライブ再マウント\n",
        "      drive.mount('/content/drive')  \n",
        "      #保存\n",
        "      df2 = pd.DataFrame(save_list)\n",
        "      df2.to_csv('./drive/My Drive/NLP/modelToCsv_4.csv', header=False, index=False)\n",
        "    \n",
        "  cnt = cnt + 1\n",
        "  \n",
        "#Gドライブ再マウント\n",
        "drive.mount('/content/drive')  \n",
        "#保存\n",
        "df2 = pd.DataFrame(save_list)\n",
        "df2.to_csv('./drive/My Drive/NLP/modelToCsv_4.csv', header=False, index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2q1efaRhvj6",
        "colab_type": "text"
      },
      "source": [
        "##以下、検証用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jitj5RgH2bAt",
        "colab_type": "text"
      },
      "source": [
        "##(取得)対義語関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKoCJxYi8Fl_",
        "colab_type": "code",
        "outputId": "8ec45815-e9b7-4060-c3a7-2695d531d9d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install mecab-python3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/92/e7e7f38df8457fa40c1ca86928be5ddbe2bf341e90a35e6ada30d03ef16d/mecab_python3-0.996.2-cp36-cp36m-manylinux1_x86_64.whl (15.9MB)\n",
            "\u001b[K     |████████████████████████████████| 15.9MB 5.9MB/s \n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.996.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCVilfCaxmGp",
        "colab_type": "code",
        "outputId": "d245ff68-edbc-4bcc-f149-ac23aed145f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "csv_dir = './drive/My Drive/NLP/modelToCsv.csv'\n",
        "df_ant = pd.read_csv(csv_dir,names=['words','ant1','ant2','ant3','flg'])\n",
        "df_ant = df_ant.sort_values('words', ascending=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sqLQwqOxmci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import MeCab\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_ant_word(words):\n",
        "\n",
        "  word_cng_list = []\n",
        "\n",
        "  #-------------------------------------------------\n",
        "  # そのまま対義語化\n",
        "  #-------------------------------------------------\n",
        "  dfTolist = df_ant[df_ant[\"words\"] == words].values.tolist()\n",
        "  ant_list = [flatten for inner in dfTolist for flatten in inner]\n",
        "  \n",
        "  #-------------------------------------------------\n",
        "  # 形態素分析後に対義語化\n",
        "  #-------------------------------------------------\n",
        "  if len(dfTolist) == 0:\n",
        "    tokenizer = MeCab.Tagger(\"-Ochasen\")\n",
        "    node = tokenizer.parseToNode(words)\n",
        "    ant_word = ''\n",
        "    rvs_wd = ''\n",
        "    \n",
        "    while node:\n",
        "      #分かち書きの単語を取得\n",
        "      cut_wd = node.surface\n",
        "      \n",
        "      #数字はそのまま\n",
        "      if cut_wd.isnumeric():\n",
        "        ant_word = ant_word + cut_wd \n",
        "      else:      \n",
        "        if cut_wd != np.nan and cut_wd != '':\n",
        "          if node.feature.split(\",\")[0] == u\"名詞\":\n",
        "            try:\n",
        "              rvs_wd = df_ant[df_ant[\"words\"] == cut_wd].values[0][1]\n",
        "              if rvs_wd is np.nan or rvs_wd == cut_wd:\n",
        "                rvs_wd = df_ant[df_ant[\"words\"] == cut_wd].values[0][2]\n",
        "                if  rvs_wd is np.nan or rvs_wd == cut_wd:\n",
        "                  rvs_wd = df_ant[df_ant[\"words\"] == cut_wd].values[0][3]\n",
        "\n",
        "              ant_word = ant_word + str(rvs_wd)\n",
        "\n",
        "            except IndexError as error:\n",
        "              #辞書に登録の無い単語の場合\n",
        "              ant_word = ant_word + str(cut_wd)\n",
        "\n",
        "          elif (node.feature.split(\",\")[0] == u\"動詞\" or\n",
        "            node.feature.split(\",\")[0] == u\"形容詞\" or\n",
        "            node.feature.split(\",\")[0] == u\"副詞\" or\n",
        "            node.feature.split(\",\")[0] == u\"感動詞\"):\n",
        "\n",
        "            #分かち書きの単語を取得\n",
        "            cut_wd = node.feature.split(\",\")[6]\n",
        "\n",
        "            try:\n",
        "              rvs_wd = df_ant[df_ant[\"words\"] == cut_wd].values[0][1]\n",
        "              if rvs_wd is np.nan or rvs_wd == cut_wd:\n",
        "                rvs_wd = df_ant[df_ant[\"words\"] == cut_wd].values[0][2]\n",
        "                if rvs_wd is np.nan or rvs_wd == cut_wd:\n",
        "                  rvs_wd = df_ant[df_ant[\"words\"] == cut_wd].values[0][3]\n",
        "\n",
        "              ant_word = ant_word + str(rvs_wd)\n",
        "\n",
        "            except IndexError as error:\n",
        "              #辞書に登録の無い単語の場合\n",
        "              ant_word = ant_word + str(cut_wd)\n",
        "\n",
        "          else:\n",
        "            #◆結合\n",
        "            ant_word = ant_word + str(cut_wd)\n",
        "\n",
        "      node = node.next\n",
        "\n",
        "    word_cng_list.append(ant_word)\n",
        "  \n",
        "    return word_cng_list\n",
        "  else:\n",
        "    return ant_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULkjDK6bSyw-",
        "colab_type": "code",
        "outputId": "05fd6de3-46db-42b7-ef30-dafd0f4ad1dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(get_ant_word(\"全国高等学校バスケットボール選抜優勝大会\"))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['全国高等学校野球選抜優勝大会']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJd7lELteNO-",
        "colab_type": "code",
        "outputId": "a7616127-c2ba-4952-f3d0-7f1e48bb8e4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(df_ant[df_ant['words'].str.contains('叫ぶ',na=False)])\n",
        "print(df_ant[df_ant['words']==\"叫ぶ\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         words  info ant1            ant2     ant3\n",
            "111040  叫ぶ詩人の会   NaN  夜光虫  ハイテクノロジー・スーサイド  怒る哲学者の気\n",
            "Empty DataFrame\n",
            "Columns: [words, info, ant1, ant2, ant3]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb4jpB4mt6Fe",
        "colab_type": "code",
        "outputId": "ab181f9d-c651-4c3c-ca89-e3bf57619bd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "import MeCab\n",
        "no_list = ['ぎゃく','必然的','意図的','結果的','下向き','場合','横方向','かえって']\n",
        "words = \"あいさつ\"\n",
        "gyaku = \"逆\"\n",
        "\n",
        "tokenizer = MeCab.Tagger(\"-Ochasen\")\n",
        "\n",
        "word_cng_str1 = ''\n",
        "word_cng_str2 = ''\n",
        "word_cng_str3 = ''\n",
        "\n",
        "node = tokenizer.parseToNode(words)\n",
        "\n",
        "while node:\n",
        "  cut_wd = node.surface\n",
        "\n",
        "  #特定固有名詞は処理対象外\n",
        "  if(cut_wd != \"[\" and cut_wd != \"]\" and cut_wd != \"(\" and cut_wd != \")\" and cut_wd != \"_\"):\n",
        "    if node.feature.split(\",\")[0] == u\"名詞\":\n",
        "      try:\n",
        "        #--------------------------------------------------------\n",
        "        #★対義語リスト取得\n",
        "        #--------------------------------------------------------\n",
        "        rvs_wd_list = model.most_similar(positive=[cut_wd,gyaku])\n",
        "        for i in range(4):\n",
        "          wd1 = rvs_wd_list[i][0].replace('[',\"\").replace(']',\"\")\n",
        "          if (cut_wd != wd1 and\n",
        "             str(wd1) not in no_list):\n",
        "            #◆結合1\n",
        "            word_cng_str1 = word_cng_str1 + wd1\n",
        "            print(\"wd1:\",wd1)\n",
        "\n",
        "            #◆結合2\n",
        "            for ii in range(4):\n",
        "              wd2 = rvs_wd_list[i+ii+1][0].replace('[',\"\").replace(']',\"\")\n",
        "              print(\"wd2:\",i+ii+1,wd2)\n",
        "              if str(wd2) not in no_list:\n",
        "                word_cng_str2 = word_cng_str2 + wd2\n",
        "                ii_save = i+ii+1\n",
        "\n",
        "                #◆結合3\n",
        "                for iii in range(4):\n",
        "                  wd3 = rvs_wd_list[ii_save+iii+1][0].replace('[',\"\").replace(']',\"\")\n",
        "                  print(\"wd3:\",ii_save+iii+1,wd3)\n",
        "                  if str(wd3) not in no_list:\n",
        "                    word_cng_str3 = word_cng_str3 + wd3\n",
        "                    break\n",
        "                break\n",
        "            break\n",
        "      except KeyError as error:\n",
        "        #辞書に登録の無い単語の場合\n",
        "        word_cng_str1 = word_cng_str1 + cut_wd\n",
        "        word_cng_str2 = word_cng_str2 + cut_wd\n",
        "        word_cng_str3 = word_cng_str3 + cut_wd\n",
        "\n",
        "    elif (node.feature.split(\",\")[0] == u\"動詞\" or\n",
        "      node.feature.split(\",\")[0] == u\"形容詞\" or\n",
        "      node.feature.split(\",\")[0] == u\"副詞\" or\n",
        "      node.feature.split(\",\")[0] == u\"感動詞\"):\n",
        "      cut_wd = node.feature.split(\",\")[6]\n",
        "\n",
        "      try:\n",
        "        #--------------------------------------------------------\n",
        "        #★対義語リスト取得\n",
        "        #--------------------------------------------------------\n",
        "        rvs_wd_list = model.most_similar(positive=[cut_wd,gyaku])\n",
        "        for i in range(4):\n",
        "          wd1 = rvs_wd_list[i][0].replace('[',\"\").replace(']',\"\")\n",
        "          if (cut_wd != wd1 and\n",
        "             str(wd1) not in no_list):\n",
        "            #◆結合1\n",
        "            word_cng_str1 = word_cng_str1 + wd1\n",
        "            print(\"wd1:\",wd1)\n",
        "\n",
        "            #◆結合2\n",
        "            for ii in range(4):\n",
        "              wd2 = rvs_wd_list[i+ii+1][0].replace('[',\"\").replace(']',\"\")\n",
        "              print(\"wd2:\",i+ii+1,wd2)\n",
        "              if str(wd2) not in no_list:\n",
        "                word_cng_str2 = word_cng_str2 + wd2\n",
        "                ii_save = i+ii+1\n",
        "\n",
        "                #◆結合3\n",
        "                for iii in range(4):\n",
        "                  wd3 = rvs_wd_list[ii_save+iii+1][0].replace('[',\"\").replace(']',\"\")\n",
        "                  print(\"wd3:\",ii_save+iii+1,wd3)\n",
        "                  if str(wd3) not in no_list:\n",
        "                    word_cng_str3 = word_cng_str3 + wd3\n",
        "                    break\n",
        "                break\n",
        "            break\n",
        "      except KeyError as error:\n",
        "        #辞書に登録の無い単語の場合\n",
        "        word_cng_str1 = word_cng_str1 + cut_wd\n",
        "        word_cng_str2 = word_cng_str2 + cut_wd\n",
        "        word_cng_str3 = word_cng_str3 + cut_wd\n",
        "\n",
        "    else:\n",
        "      #◆結合\n",
        "      word_cng_str1 = word_cng_str1 + cut_wd.replace('[',\"\").replace(']',\"\")\n",
        "      word_cng_str2 = word_cng_str2 + cut_wd.replace('[',\"\").replace(']',\"\")\n",
        "      word_cng_str3 = word_cng_str3 + cut_wd.replace('[',\"\").replace(']',\"\")\n",
        "\n",
        "  node = node.next\n",
        "\n",
        "print(word_cng_str1)\n",
        "print(word_cng_str2)\n",
        "print(word_cng_str3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wd1: 挨拶\n",
            "wd2: 1 必ず\n",
            "wd3: 2 乾杯\n",
            "挨拶\n",
            "必ず\n",
            "乾杯\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibG8Zic5j374",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = model[\"日本\"]\n",
        "b = model[\"高齢者\"]\n",
        "\n",
        "vector = a - b\n",
        "\n",
        "word = model.most_similar( [ vector ], [], 10)\n",
        "word"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}