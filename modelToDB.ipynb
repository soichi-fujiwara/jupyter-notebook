{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modelToDB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soichi-fujiwara/jupyter-notebook/blob/master/modelToDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCMfweVZEzPd",
        "colab_type": "text"
      },
      "source": [
        "##モデルをDB化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhMbBx_ECoOl",
        "colab_type": "code",
        "outputId": "7dbf35a2-05f6-44fd-cea7-6353c6db6258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install mecab-python3"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.6/dist-packages (0.996.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDVIuIbd2WYi",
        "colab_type": "code",
        "outputId": "2205b8d6-f7cc-4b27-fc2c-047777bfc8c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYLgNCFduL-w",
        "colab_type": "code",
        "outputId": "2b2479c2-d07d-4280-f0ae-f9da29f8603f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cat /proc/uptime | awk '{print $1 /60 /60 /24 \"days (\" $1 \"sec)\"}'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.00161192days (139.27sec)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRJCjIdrc3nQ",
        "colab_type": "code",
        "outputId": "9975a296-4be6-4172-f6ef-bf7b91b2c1c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "import IPython\n",
        "#bz2ファイル解凍\n",
        "#!bunzip2 -k -q \"./drive/My Drive/NLP/20170201.tar.bz2\"\n",
        "\n",
        "#tarファイル解凍\n",
        "!tar -xvf \"./drive/My Drive/NLP/20170201.tar\"\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "#解凍時のみ\n",
        "model_dir = 'entity_vector/entity_vector.model.bin'\n",
        "model = KeyedVectors.load_word2vec_format(model_dir, binary=True,limit=500000)\n",
        "\n",
        "#解凍後確認\n",
        "results = model.most_similar(positive=['柔らかい'],topn=5)\n",
        "for result in results:\n",
        "  print(result)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "entity_vector/\n",
            "entity_vector/entity_vector.model.txt\n",
            "entity_vector/entity_vector.model.bin\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "('硬い', 0.8799270391464233)\n",
            "('やわらかい', 0.8635358810424805)\n",
            "('軟らかい', 0.8242496252059937)\n",
            "('やわらかく', 0.7888022661209106)\n",
            "('柔らかく', 0.7741994857788086)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OzKgJ9slAsf",
        "colab_type": "text"
      },
      "source": [
        "##語彙リスト作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5yIYDAfPHQu",
        "colab_type": "code",
        "outputId": "793dcd5f-3484-405d-d0a9-f229a87acb5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "cnt = 1\n",
        "model_vocab_list = []\n",
        "\n",
        "for words, v in model.vocab.items():\n",
        "\n",
        "  #固有名詞用の括弧を削除\n",
        "  #words = words.replace('[', '').replace(']', '')\n",
        "\n",
        "  #-----------------------------------------------------  \n",
        "\n",
        "  # 半角記号+半角数字\n",
        "  #p = re.compile(\"[!-@[-`{-~]\")\n",
        "  \n",
        "  # 半角記号+半角数字+半角英字\n",
        "  #((ほぼ)日本語のみのDB作成用)\n",
        "  p = re.compile(\"[!-~]\")\n",
        "\n",
        "  # 先頭文字\n",
        "  top_char = words[0:1]\n",
        "\n",
        "  #-----------------------------------------------------  \n",
        "\n",
        "#   【M1】dfに格納した後で補足情報を削除する為、ココでは補足情報を残しておく\n",
        "#   #補足情報の_を削除\n",
        "#   words = words.replace(\"_\",\"\")\n",
        "#   #括弧()文字を抽出\n",
        "#   regex = re.compile(\".*?\\((.*?)\\)\")\n",
        "#   #括弧文字をlist型で返却\n",
        "#   ret_list = re.findall(regex, words)\n",
        "\n",
        "  #-----------------------------------------------------  \n",
        "\n",
        "#   #除外文字なし\n",
        "#   if p.match(top_char) is None:\n",
        "#     #括弧文字がある場合は無効化\n",
        "#     if len(ret_list) > 0: \n",
        "#       words = words.replace(\"(\" + ret_list[0] + \")\",'')\n",
        "#       model_vocab_list.append(words)\n",
        "#     else:\n",
        "#       model_vocab_list.append(words)\n",
        "\n",
        "  #【M1】\n",
        "  model_vocab_list.append(words)\n",
        "  #【M1】\n",
        "  \n",
        "  if cnt % 50000 == 0:\n",
        "    print(cnt,\"件 完了\");\n",
        "    \n",
        "  cnt = cnt + 1\n",
        "\n",
        "#重複削除\n",
        "model_vocab_uni_list= list(set(model_vocab_list))\n",
        "  \n",
        "#保存\n",
        "drive.mount('/content/drive')\n",
        "df = pd.DataFrame(model_vocab_uni_list)\n",
        "df.to_csv('./drive/My Drive/NLP/model_vocab_list.csv', header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 件 完了\n",
            "100000 件 完了\n",
            "150000 件 完了\n",
            "200000 件 完了\n",
            "250000 件 完了\n",
            "300000 件 完了\n",
            "350000 件 完了\n",
            "400000 件 完了\n",
            "450000 件 完了\n",
            "500000 件 完了\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "190yBfA8Wayj",
        "colab_type": "text"
      },
      "source": [
        "##語彙リスト読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRvxKV48SLUM",
        "colab_type": "code",
        "outputId": "9202ab9e-acff-48ac-b3a1-70e46d3c5f78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "csv_dir = './drive/My Drive/NLP/model_vocab_list.csv'\n",
        "df = pd.read_csv(csv_dir,names=['words'])\n",
        "df = df.sort_values('words', ascending=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsgpqqphWIw4",
        "colab_type": "text"
      },
      "source": [
        "##(関数)対義語-対応list作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdwy6HUsTA_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import MeCab\n",
        "#括弧記号無効化\n",
        "import lib_delete_brackets\n",
        "\n",
        "def search_similar_texts(p_words):\n",
        "  pass\n",
        "\n",
        "def wordRevChange(words,gyaku,model,tokenizer):\n",
        "\n",
        "  no_list = ['混一色','デュラララ!!','ぎゃく','必然的','意図的','結果的','下向き','場合','横方向','かえって']\n",
        "\n",
        "  #**************************************************************************\n",
        "  #補足情報有無判定\n",
        "  #**************************************************************************\n",
        "  if '_(' in words:\n",
        "    #補足情報あり･･･①\n",
        "    info_chk_flg = 0\n",
        "  else:\n",
        "    #補足情報なし and 記事名･･･②\n",
        "    if '[' in words:\n",
        "      info_chk_flg = 1\n",
        "    else:\n",
        "      #ノーマル語彙･･･③\n",
        "      info_chk_flg = 2\n",
        "  \n",
        "  #**************************************************************************\n",
        "  #①そのまま対義語化\n",
        "  #**************************************************************************\n",
        "  if info_chk_flg == 0:\n",
        "    asitis_cng_words1 = ''\n",
        "    asitis_cng_words2 = ''\n",
        "    asitis_cng_words3 = ''\n",
        "\n",
        "    #--------------------------------------------------------\n",
        "    #★対義語リスト取得\n",
        "    #--------------------------------------------------------\n",
        "    rvs_wd_list = model.most_similar(positive=[words,gyaku])\n",
        "\n",
        "    index = 0\n",
        "    rvs_wd_rt_list = []\n",
        "\n",
        "    for index in range(len(rvs_wd_list)):\n",
        "      wd = rvs_wd_list[index][0].replace('[',\"\").replace(']',\"\")\n",
        "      if str(wd) not in no_list:\n",
        "\n",
        "        #対義語候補格納\n",
        "        rvs_wd_rt_list.append(lib_delete_brackets.delete_brackets(wd))\n",
        "\n",
        "    result_list = []\n",
        "\n",
        "    asitis_cng_words1 = lib_delete_brackets.delete_brackets(rvs_wd_rt_list[0])\n",
        "    asitis_cng_words2 = lib_delete_brackets.delete_brackets(rvs_wd_rt_list[1])\n",
        "    asitis_cng_words3 = lib_delete_brackets.delete_brackets(rvs_wd_rt_list[2])\n",
        "\n",
        "    asitis_cng_words1 = asitis_cng_words1.replace('_',\"\").replace('[',\"\").replace(']',\"\")\n",
        "    asitis_cng_words2 = asitis_cng_words2.replace('_',\"\").replace('[',\"\").replace(']',\"\")\n",
        "    asitis_cng_words3 = asitis_cng_words3.replace('_',\"\").replace('[',\"\").replace(']',\"\")\n",
        "\n",
        "    #返却値書き込み\n",
        "    result_list.append(words.replace('[',\"\").replace(']',\"\"))\n",
        "    result_list.append(asitis_cng_words1)\n",
        "    result_list.append(asitis_cng_words2)\n",
        "    result_list.append(asitis_cng_words3)\n",
        "    result_list.append(info_chk_flg)\n",
        "\n",
        "    return result_list\n",
        "\n",
        "  #**************************************************************************\n",
        "  #②記事名\n",
        "  #**************************************************************************\n",
        "  #---------------------------------------------\n",
        "  #②-1 記事名をそのまま対義語化\n",
        "  #---------------------------------------------\n",
        "  if info_chk_flg == 1:\n",
        "\n",
        "    result_list = []\n",
        "    rvs_wd_rt_list = []\n",
        "\n",
        "    asitis_cng_words1 = ''\n",
        "    asitis_cng_words2 = ''\n",
        "    asitis_cng_words3 = ''\n",
        "\n",
        "    #--------------------------------------------------------\n",
        "    #★対義語リスト取得\n",
        "    #--------------------------------------------------------\n",
        "    rvs_wd_list = model.most_similar(positive=[words,gyaku])\n",
        "\n",
        "    index = 0\n",
        "\n",
        "    for index in range(len(rvs_wd_list)):\n",
        "      wd = rvs_wd_list[index][0].replace('[',\"\").replace(']',\"\")\n",
        "      if str(wd) not in no_list:\n",
        "\n",
        "        #対義語候補格納\n",
        "        rvs_wd_rt_list.append(lib_delete_brackets.delete_brackets(wd))\n",
        "    \n",
        "    #②-1は「1候補」のみセット\n",
        "    asitis_cng_words1 = lib_delete_brackets.delete_brackets(rvs_wd_rt_list[0])\n",
        "    #asitis_cng_words2 = lib_delete_brackets.delete_brackets(rvs_wd_rt_list[1])\n",
        "\n",
        "    asitis_cng_words1 = asitis_cng_words1.replace('_',\"\").replace('[',\"\").replace(']',\"\")\n",
        "    #asitis_cng_words2 = asitis_cng_words2.replace('_',\"\").replace('[',\"\").replace(']',\"\")\n",
        "\n",
        "    #返却値書き込み\n",
        "    result_list.append(words.replace('[',\"\").replace(']',\"\"))\n",
        "    result_list.append(asitis_cng_words1)\n",
        "    #result_list.append(asitis_cng_words2)\n",
        "    \n",
        "    #この時点ではreturnなし\n",
        "\n",
        "    #---------------------------------------------\n",
        "    #②-2 形態素分析後に強制的に対義語化\n",
        "    #---------------------------------------------\n",
        "    node = tokenizer.parseToNode(words)\n",
        "\n",
        "    word_cng_str1 = ''\n",
        "    word_cng_str2 = ''\n",
        "    \n",
        "    while node:\n",
        "      cut_wd = node.surface\n",
        "      \n",
        "      #特定固有名詞の囲み文字は処理対象外\n",
        "      if(cut_wd != \"[\" and cut_wd != \"]\" and cut_wd != \"(\" and cut_wd != \")\" and cut_wd != \"_\"):\n",
        "        if node.feature.split(\",\")[0] == u\"名詞\":\n",
        "          try:\n",
        "            #--------------------------------------------------------\n",
        "            #★対義語リスト取得\n",
        "            #--------------------------------------------------------\n",
        "            rvs_wd_list = model.most_similar(positive=[cut_wd,gyaku])\n",
        "            for i in range(4):\n",
        "              wd1 = rvs_wd_list[i][0].replace('[',\"\").replace(']',\"\")\n",
        "              if (cut_wd != wd1 and\n",
        "                 str(wd1) not in no_list):\n",
        "                #◆結合1\n",
        "                word_cng_str1 = word_cng_str1 + wd1\n",
        "\n",
        "                #◆結合2\n",
        "                for ii in range(4):\n",
        "                  wd2 = rvs_wd_list[i+ii+1][0].replace('[',\"\").replace(']',\"\")\n",
        "                  if str(wd2) not in no_list:\n",
        "                    word_cng_str2 = word_cng_str2 + wd2\n",
        "                    break\n",
        "\n",
        "                break\n",
        "          except KeyError as error:\n",
        "            #辞書に登録の無い単語の場合\n",
        "            word_cng_str1 = word_cng_str1 + cut_wd\n",
        "            word_cng_str2 = word_cng_str2 + cut_wd\n",
        "\n",
        "        elif (node.feature.split(\",\")[0] == u\"動詞\" or\n",
        "          node.feature.split(\",\")[0] == u\"形容詞\" or\n",
        "          node.feature.split(\",\")[0] == u\"副詞\" or\n",
        "          node.feature.split(\",\")[0] == u\"感動詞\"):\n",
        "          cut_wd = node.feature.split(\",\")[6]\n",
        "\n",
        "          try:\n",
        "            #--------------------------------------------------------\n",
        "            #★対義語リスト取得\n",
        "            #--------------------------------------------------------\n",
        "            rvs_wd_list = model.most_similar(positive=[cut_wd,gyaku])\n",
        "            for i in range(4):\n",
        "              wd1 = rvs_wd_list[i][0].replace('[',\"\").replace(']',\"\")\n",
        "              if (cut_wd != wd1 and\n",
        "                 str(wd1) not in no_list):\n",
        "                #◆結合1\n",
        "                word_cng_str1 = word_cng_str1 + wd1\n",
        "\n",
        "                #◆結合2\n",
        "                for ii in range(4):\n",
        "                  wd2 = rvs_wd_list[i+ii+1][0].replace('[',\"\").replace(']',\"\")\n",
        "                  if str(wd2) not in no_list:\n",
        "                    word_cng_str2 = word_cng_str2 + wd2\n",
        "                    break\n",
        "                    \n",
        "                break\n",
        "          except KeyError as error:\n",
        "            #辞書に登録の無い単語の場合\n",
        "            word_cng_str1 = word_cng_str1 + cut_wd\n",
        "            word_cng_str2 = word_cng_str2 + cut_wd\n",
        "\n",
        "        else:\n",
        "          #◆結合\n",
        "          word_cng_str1 = word_cng_str1 + cut_wd.replace('[',\"\").replace(']',\"\")\n",
        "          word_cng_str2 = word_cng_str2 + cut_wd.replace('[',\"\").replace(']',\"\")\n",
        "\n",
        "      node = node.next\n",
        "\n",
        "    #返却値書き込み\n",
        "    #②-2は「2候補」セット\n",
        "    asitis_cng_words2 = word_cng_str1.replace('_',\"\")\n",
        "    asitis_cng_words3 = word_cng_str2.replace('_',\"\")\n",
        "\n",
        "    result_list.append(lib_delete_brackets.delete_brackets(asitis_cng_words2))\n",
        "    result_list.append(lib_delete_brackets.delete_brackets(asitis_cng_words3))\n",
        "    result_list.append(info_chk_flg)\n",
        "\n",
        "    return result_list\n",
        "\n",
        "  #**************************************************************************\n",
        "  #③ノーマル語彙\n",
        "  #**************************************************************************\n",
        "  #---------------------------------------------\n",
        "  #形態素分析後に強制的に対義語化\n",
        "  #---------------------------------------------\n",
        "  if info_chk_flg == 2:\n",
        "    \n",
        "    word_cng_str1 = ''\n",
        "    word_cng_str2 = ''\n",
        "    word_cng_str3 = ''\n",
        "\n",
        "    node = tokenizer.parseToNode(words)\n",
        "\n",
        "    while node:\n",
        "      cut_wd = node.surface\n",
        "      \n",
        "      #特定固有名詞は処理対象外\n",
        "      if(cut_wd != \"[\" and cut_wd != \"]\" and cut_wd != \"(\" and cut_wd != \")\" and cut_wd != \"_\"):\n",
        "        if node.feature.split(\",\")[0] == u\"名詞\":\n",
        "          try:\n",
        "            #--------------------------------------------------------\n",
        "            #★対義語リスト取得\n",
        "            #--------------------------------------------------------\n",
        "            rvs_wd_list = model.most_similar(positive=[cut_wd,gyaku])\n",
        "            for i in range(4):\n",
        "              wd1 = rvs_wd_list[i][0].replace('[',\"\").replace(']',\"\")\n",
        "              if (cut_wd != wd1 and\n",
        "                 str(wd1) not in no_list):\n",
        "                #◆結合1\n",
        "                word_cng_str1 = word_cng_str1 + wd1\n",
        "                \n",
        "                #◆結合2\n",
        "                for ii in range(4):\n",
        "                  wd2 = rvs_wd_list[i+ii+1][0].replace('[',\"\").replace(']',\"\")\n",
        "                  if str(wd2) not in no_list:\n",
        "                    word_cng_str2 = word_cng_str2 + wd2\n",
        "                    ii_save = i+ii+1\n",
        "                    \n",
        "                    #◆結合3\n",
        "                    for iii in range(4):\n",
        "                      wd3 = rvs_wd_list[ii_save+iii+1][0].replace('[',\"\").replace(']',\"\")\n",
        "                      if str(wd3) not in no_list:\n",
        "                        word_cng_str3 = word_cng_str3 + wd3\n",
        "                        break\n",
        "                    break\n",
        "                break\n",
        "          except KeyError as error:\n",
        "            #辞書に登録の無い単語の場合\n",
        "            word_cng_str1 = word_cng_str1 + cut_wd\n",
        "            word_cng_str2 = word_cng_str2 + cut_wd\n",
        "            word_cng_str3 = word_cng_str3 + cut_wd\n",
        "          \n",
        "        elif (node.feature.split(\",\")[0] == u\"動詞\" or\n",
        "          node.feature.split(\",\")[0] == u\"形容詞\" or\n",
        "          node.feature.split(\",\")[0] == u\"副詞\" or\n",
        "          node.feature.split(\",\")[0] == u\"感動詞\"):\n",
        "          cut_wd = node.feature.split(\",\")[6]\n",
        "\n",
        "          try:\n",
        "            #--------------------------------------------------------\n",
        "            #★対義語リスト取得\n",
        "            #--------------------------------------------------------\n",
        "            rvs_wd_list = model.most_similar(positive=[cut_wd,gyaku])\n",
        "            for i in range(4):\n",
        "              wd1 = rvs_wd_list[i][0].replace('[',\"\").replace(']',\"\")\n",
        "              if (cut_wd != wd1 and\n",
        "                 str(wd1) not in no_list):\n",
        "                #◆結合1\n",
        "                word_cng_str1 = word_cng_str1 + wd1\n",
        "                \n",
        "                #◆結合2\n",
        "                for ii in range(4):\n",
        "                  wd2 = rvs_wd_list[i+ii+1][0].replace('[',\"\").replace(']',\"\")\n",
        "                  if str(wd2) not in no_list:\n",
        "                    word_cng_str2 = word_cng_str2 + wd2\n",
        "                    ii_save = i+ii+1\n",
        "\n",
        "                    #◆結合3\n",
        "                    for iii in range(4):\n",
        "                      wd3 = rvs_wd_list[ii_save+iii+1][0].replace('[',\"\").replace(']',\"\")\n",
        "                      if str(wd3) not in no_list:\n",
        "                        word_cng_str3 = word_cng_str3 + wd3\n",
        "                        break\n",
        "                    break\n",
        "                break\n",
        "          except KeyError as error:\n",
        "            #辞書に登録の無い単語の場合\n",
        "            word_cng_str1 = word_cng_str1 + cut_wd\n",
        "            word_cng_str2 = word_cng_str2 + cut_wd\n",
        "            word_cng_str3 = word_cng_str3 + cut_wd\n",
        "\n",
        "        else:\n",
        "          #◆結合\n",
        "          word_cng_str1 = word_cng_str1 + cut_wd.replace('[',\"\").replace(']',\"\")\n",
        "          word_cng_str2 = word_cng_str2 + cut_wd.replace('[',\"\").replace(']',\"\")\n",
        "          word_cng_str3 = word_cng_str3 + cut_wd.replace('[',\"\").replace(']',\"\")\n",
        "\n",
        "      node = node.next\n",
        "\n",
        "    result_list = []\n",
        "\n",
        "    #返却値書き込み\n",
        "    asitis_cng_words1 = word_cng_str1.replace('_',\"\")\n",
        "    asitis_cng_words2 = word_cng_str2.replace('_',\"\")\n",
        "    asitis_cng_words3 = word_cng_str3.replace('_',\"\")\n",
        "\n",
        "    result_list.append(words.replace('(',\"\").replace(')',\"\"))\n",
        "    result_list.append(lib_delete_brackets.delete_brackets(asitis_cng_words1))\n",
        "    result_list.append(lib_delete_brackets.delete_brackets(asitis_cng_words2))\n",
        "    result_list.append(lib_delete_brackets.delete_brackets(asitis_cng_words3))\n",
        "    result_list.append(info_chk_flg)\n",
        "\n",
        "    return result_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp-Cbw6x2-Ro",
        "colab_type": "text"
      },
      "source": [
        "##(関数呼出)対義語-対応list作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQGM66ine9J3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import MeCab\n",
        "#対義語生成\n",
        "import lib_delete_brackets\n",
        "#import lib_wordRevChange as lw\n",
        "\n",
        "tokenizer = MeCab.Tagger(\"-Ochasen\")\n",
        "owkt = MeCab.Tagger(\"-Owakati\")        \n",
        "\n",
        "cnt = 1\n",
        "save_list = []\n",
        "del_char_list = [\"も\",\"が\",\"は\",\"の\",\"で\",\"に\",\"を\",\"と\",\"や\",\"か\"]\n",
        "\n",
        "jpn_start_flg = 0\n",
        "\n",
        "#列[words]をnumpyに変換\n",
        "val = df.words.values\n",
        "\n",
        "for idx in range(df.shape[0]):\n",
        "\n",
        "  words = str(val[idx])\n",
        "  \n",
        "  #日本語単語の開始判定\n",
        "  if words[0:2] == \"[あ\":\n",
        "    jpn_start_flg = 1\n",
        "\n",
        "  #日本語単語の終了判定\n",
        "  if words[0:4] == \"[プラム\":\n",
        "    break\n",
        "  \n",
        "  if jpn_start_flg == 1:\n",
        "\n",
        "    #語彙数チェック\n",
        "    cnt_words_str = lib_delete_brackets.delete_brackets(words.replace(\"[\",\"\").replace(\"]\",\"\")).replace(\"_\",\"\")\n",
        "\n",
        "    #10文字以下が対象\n",
        "    if len(cnt_words_str) <= 10:\n",
        "      result_owkt = owkt.parse(cnt_words_str)\n",
        "      owkt_list = result_owkt.split(' ')\n",
        "      goi_cnt = len(owkt_list)-1\n",
        "    else:\n",
        "      goi_cnt = 99\n",
        "      \n",
        "#     for del_char in del_char_list:\n",
        "#       try:\n",
        "#         owkt_list.remove(del_char)\n",
        "#       except ValueError as error:\n",
        "#         pass\n",
        "\n",
        "    #語彙に含まれる形態素数が5以下のみ処理対象\n",
        "    if goi_cnt <= 5:\n",
        "      if '#' not in words:\n",
        "        if ':' not in words:\n",
        "          if '・' not in words:\n",
        "            if words.isdecimal() is False:\n",
        "              gyaku = u\"逆\"\n",
        "              rev_list = wordRevChange(words,gyaku,model,tokenizer)\n",
        "\n",
        "              #返却値ありのみ格納\n",
        "              if rev_list is not None:\n",
        "                save_list.append(rev_list)\n",
        "\n",
        "  if cnt % 100 == 0:\n",
        "    print(cnt,\"件 終了\")\n",
        "\n",
        "    if cnt % 1500 == 0:\n",
        "      #Gドライブ再マウント\n",
        "      drive.mount('/content/drive')  \n",
        "      #保存\n",
        "      df2 = pd.DataFrame(save_list)\n",
        "      df2.to_csv('./drive/My Drive/NLP/modelToCsv_1_002.csv', header=False, index=False)\n",
        "    \n",
        "  cnt = cnt + 1\n",
        "  \n",
        "#Gドライブ再マウント\n",
        "drive.mount('/content/drive')  \n",
        "#保存\n",
        "df2 = pd.DataFrame(save_list)\n",
        "df2.to_csv('./drive/My Drive/NLP/modelToCsv_1_002.csv', header=False, index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2q1efaRhvj6",
        "colab_type": "text"
      },
      "source": [
        "##以下、検証用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jitj5RgH2bAt",
        "colab_type": "text"
      },
      "source": [
        "##(取得)対義語関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "96dc9835-165c-466a-d7c7-2634b2ab7c98",
        "id": "xesHToe2yCBc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install mecab-python3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/92/e7e7f38df8457fa40c1ca86928be5ddbe2bf341e90a35e6ada30d03ef16d/mecab_python3-0.996.2-cp36-cp36m-manylinux1_x86_64.whl (15.9MB)\n",
            "\u001b[K     |████████████████████████████████| 15.9MB 2.5MB/s \n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.996.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCVilfCaxmGp",
        "colab_type": "code",
        "outputId": "72debff7-0b7c-481b-b7a8-1319408ca45e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7K_Rh2SmDEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_dir_1 = './drive/My Drive/NLP/modelToCsv_1.csv'\n",
        "csv_dir_2 = './drive/My Drive/NLP/modelToCsv_2.csv'\n",
        "csv_dir_3 = './drive/My Drive/NLP/modelToCsv_3.csv'\n",
        "csv_dir_4 = './drive/My Drive/NLP/modelToCsv_4.csv'\n",
        "\n",
        "df_ant_1 = pd.read_csv(csv_dir_1,names=['words','ant1','ant2','ant3','flg'])\n",
        "df_ant_2 = pd.read_csv(csv_dir_2,names=['words','ant1','ant2','ant3','flg'])\n",
        "df_ant_3 = pd.read_csv(csv_dir_3,names=['words','ant1','ant2','ant3','flg'])\n",
        "df_ant_4 = pd.read_csv(csv_dir_4,names=['words','ant1','ant2','ant3','flg'])\n",
        "\n",
        "df_ant = pd.concat([df_ant_1, df_ant_2,df_ant_3,df_ant_4])\n",
        "df_ant = df_ant.sort_values('words', ascending=True)\n",
        "df_ant.to_csv('./drive/My Drive/NLP/df_ant_words.csv', header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULkjDK6bSyw-",
        "colab_type": "code",
        "outputId": "9869f235-6326-4b0d-a28a-fa348f6083b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#対義語生成\n",
        "import lib_antnym as ant\n",
        "\n",
        "ret_list = ant.get_ant_word(\"なかのひと\")\n",
        "print(ret_list)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['反面のやみ', '一方のつら', '中のおもい']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibG8Zic5j374",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = model[\"日本\"]\n",
        "b = model[\"高齢者\"]\n",
        "\n",
        "vector = a - b\n",
        "\n",
        "word = model.most_similar( [ vector ], [], 10)\n",
        "word"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}