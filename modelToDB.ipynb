{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modelToDB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soichi-fujiwara/jupyter-notebook/blob/master/modelToDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCMfweVZEzPd",
        "colab_type": "text"
      },
      "source": [
        "##モデルをDB化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhMbBx_ECoOl",
        "colab_type": "code",
        "outputId": "8b07a7dd-39cd-4415-b8db-4ab90b8c476e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRJCjIdrc3nQ",
        "colab_type": "code",
        "outputId": "8963acba-2295-40c3-d0b3-f699bcf36531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import IPython\n",
        "#bz2ファイル解凍\n",
        "#!bunzip2 -k -q \"./drive/My Drive/NLP/20170201.tar.bz2\"\n",
        "\n",
        "#tarファイル解凍\n",
        "!tar -xvf \"./drive/My Drive/NLP/20170201.tar\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "entity_vector/\n",
            "entity_vector/entity_vector.model.txt\n",
            "entity_vector/entity_vector.model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2jUrFtNeMMH",
        "colab_type": "code",
        "outputId": "4c1ffa27-d541-47b8-83d2-47cb01a7417a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "#解凍時のみ\n",
        "#model_dir = 'entity_vector/entity_vector.model.bin'\n",
        "#model = KeyedVectors.load_word2vec_format(model_dir, binary=True,limit=500000)\n",
        "\n",
        "#解凍後確認\n",
        "results = model.most_similar(positive=[u'リンゴ','逆'])\n",
        "for result in results:\n",
        "  print(result)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('ナッツ', 0.608566403388977)\n",
            "('バナナ', 0.6054157018661499)\n",
            "('蜜', 0.6049777865409851)\n",
            "('トマト', 0.6029523015022278)\n",
            "('木の実', 0.6004878878593445)\n",
            "('ハエ', 0.5923978686332703)\n",
            "('ミルク', 0.5920907855033875)\n",
            "('ニンジン', 0.5913463830947876)\n",
            "('タネ', 0.5892855525016785)\n",
            "('キャンディー', 0.5888605117797852)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEruRmNNatxY",
        "colab_type": "code",
        "outputId": "d1ec2a21-1c40-4645-aec1-4baa8670109f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install mecab-python3"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/92/e7e7f38df8457fa40c1ca86928be5ddbe2bf341e90a35e6ada30d03ef16d/mecab_python3-0.996.2-cp36-cp36m-manylinux1_x86_64.whl (15.9MB)\n",
            "\u001b[K     |████████████████████████████████| 15.9MB 3.5MB/s \n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.996.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OzKgJ9slAsf",
        "colab_type": "text"
      },
      "source": [
        "##語彙-対義語候補 対応CSV作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5yIYDAfPHQu",
        "colab_type": "code",
        "outputId": "237da95b-51c4-47a8-ad5e-5a23d9fc28ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "cnt = 1\n",
        "model_vocab_list = []\n",
        "\n",
        "for words, v in model.vocab.items():\n",
        "\n",
        "  words = words.replace('_', '')\n",
        "  \n",
        "  #固有名詞用の括弧を削除\n",
        "  words = words.replace('[', '').replace(']', '')\n",
        "\n",
        "  #-----------------------------------------------------  \n",
        "\n",
        "  # 半角記号+半角数字\n",
        "  #p = re.compile(\"[!-@[-`{-~]\")\n",
        "  # 半角記号+半角数字+半角英字\n",
        "  #((ほぼ)日本語のみのDB作成用)\n",
        "  p = re.compile(\"[!-~]\")\n",
        "\n",
        "  # 先頭文字\n",
        "  top_char = words[0:1]\n",
        "\n",
        "  #-----------------------------------------------------  \n",
        "\n",
        "  #補足情報の_を削除\n",
        "  words = words.replace(\"_\",\"\")\n",
        "  #括弧文字を抽出\n",
        "  regex = re.compile(\".*?\\((.*?)\\)\")\n",
        "  #括弧文字をlist型で返却\n",
        "  ret_list = re.findall(regex, words)\n",
        "\n",
        "  #-----------------------------------------------------  \n",
        "\n",
        "  #除外文字なし\n",
        "  if p.match(top_char) is None:\n",
        "    #括弧文字がある場合は無効化\n",
        "    if len(ret_list) > 0: \n",
        "      words = words.replace(\"(\" + ret_list[0] + \")\",'')\n",
        "      model_vocab_list.append(words)\n",
        "    else:\n",
        "      model_vocab_list.append(words)\n",
        "      \n",
        "  if cnt % 50000 == 0:\n",
        "    print(cnt,\"件 完了\");\n",
        "    \n",
        "  cnt = cnt + 1\n",
        "\n",
        "#重複削除\n",
        "model_vocab_uni_list= list(set(model_vocab_list))\n",
        "  \n",
        "#保存\n",
        "df = pd.DataFrame(model_vocab_uni_list)\n",
        "df.to_csv('./drive/My Drive/NLP/model_vocab_list.csv', header=False, index=False)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 件 完了\n",
            "100000 件 完了\n",
            "150000 件 完了\n",
            "200000 件 完了\n",
            "250000 件 完了\n",
            "300000 件 完了\n",
            "350000 件 完了\n",
            "400000 件 完了\n",
            "450000 件 完了\n",
            "500000 件 完了\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRvxKV48SLUM",
        "colab_type": "code",
        "outputId": "ab689db6-97c8-4fa2-ee63-55de9ba4eb25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "csv_dir = './drive/My Drive/NLP/model_vocab_list.csv'\n",
        "df = pd.read_csv(csv_dir,names=['words'])\n",
        "df = df.sort_values('words', ascending=True)\n",
        "\n",
        "print(df.count())\n",
        "print(df[df[\"words\"]==\"ゆず\"])"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "words    358056\n",
            "dtype: int64\n",
            "       words\n",
            "312144    ゆず\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdwy6HUsTA_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import MeCab\n",
        "#括弧記号無効化\n",
        "import lib_delete_brackets\n",
        "\n",
        "def search_similar_texts(p_words):\n",
        "  pass\n",
        "\n",
        "def wordRevChange(words,gyaku,inherent_words,model,tokenizer):\n",
        "\n",
        "  word_cng_list = []\n",
        "\n",
        "  #**************************************************************************\n",
        "  #そのまま対義化\n",
        "  #**************************************************************************\n",
        "  asitis_cng_words1 = ''\n",
        "  asitis_cng_words2 = ''\n",
        "  try:\n",
        "\n",
        "    rvs_wd_list = model.most_similar(positive=[inherent_words,gyaku])\n",
        "    inherent_words_bk = inherent_words.replace('[',\"\").replace(']',\"\")\n",
        "    \n",
        "    index = 0\n",
        "    rvs_wd_rt_list = []\n",
        "\n",
        "    for index in range(len(rvs_wd_list)):\n",
        "      if len(rvs_wd_list[index][0]) < 10 and inherent_words_bk != str(rvs_wd_list[index][0]):\n",
        "\n",
        "        #対義語候補格納\n",
        "        rvs_wd_rt_list.append(rvs_wd_list[index][0])\n",
        "\n",
        "    asitis_cng_words1 = rvs_wd_rt_list[0]\n",
        "    asitis_cng_words2 = rvs_wd_rt_list[1]\n",
        "      \n",
        "  except KeyError as error:\n",
        "    #辞書に登録の無い単語の場合\n",
        "    #search_similar_texts(inherent_words)\n",
        "    pass\n",
        "  except IndexError as error:\n",
        "    #辞書に登録の無い単語の場合\n",
        "    #search_similar_texts(inherent_words)\n",
        "    pass\n",
        "\n",
        "  #**************************************************************************\n",
        "  #形態素分析後に対義化\n",
        "  #**************************************************************************\n",
        "  node = tokenizer.parseToNode(words)\n",
        "\n",
        "  while node:\n",
        "    cut_wd = node.surface\n",
        "\n",
        "    if node.feature.split(\",\")[0] == u\"名詞\":\n",
        "      try:\n",
        "        rvs_wd_list = model.most_similar(positive=[cut_wd,gyaku])\n",
        "        for i in range(2):\n",
        "          rvs_wd = rvs_wd_list[i][0]\n",
        "          if cut_wd != rvs_wd.replace('[',\"\").replace(']',\"\"):\n",
        "            #◆結合\n",
        "            word_cng_list.append(rvs_wd.replace('[',\"\").replace(']',\"\"))\n",
        "            break\n",
        "      except KeyError as error:\n",
        "        #辞書に登録の無い単語の場合\n",
        "        #search_similar_texts(cut_wd)\n",
        "        pass\n",
        "    elif (node.feature.split(\",\")[0] == u\"動詞\" or\n",
        "      node.feature.split(\",\")[0] == u\"形容詞\" or\n",
        "      node.feature.split(\",\")[0] == u\"副詞\" or\n",
        "      node.feature.split(\",\")[0] == u\"感動詞\"):\n",
        "      cut_wd = node.feature.split(\",\")[6]\n",
        "\n",
        "      try:\n",
        "        rvs_wd_list = model.most_similar(positive=[cut_wd,gyaku])\n",
        "        for i in range(2):\n",
        "          rvs_wd = rvs_wd_list[i][0]\n",
        "          if cut_wd != rvs_wd.replace('[',\"\").replace(']',\"\"):\n",
        "            #◆結合\n",
        "            word_cng_list.append(rvs_wd.replace('[',\"\").replace(']',\"\"))\n",
        "            break\n",
        "      except KeyError as error:\n",
        "        #辞書に登録の無い単語の場合\n",
        "        #search_similar_texts(cut_wd)\n",
        "        pass\n",
        "\n",
        "    else:\n",
        "      #◆結合\n",
        "      word_cng_list.append(cut_wd.replace('[',\"\").replace(']',\"\"))\n",
        "\n",
        "    node = node.next\n",
        "\n",
        "  asitis_cng_words1 = asitis_cng_words1.replace('[',\"\").replace(']',\"\").replace('_',\"\")\n",
        "  asitis_cng_words2 = asitis_cng_words2.replace('[',\"\").replace(']',\"\").replace('_',\"\")\n",
        "\n",
        "  result_list = []\n",
        "\n",
        "  result_list.append(words)\n",
        "  result_list.append(lib_delete_brackets.delete_brackets(asitis_cng_words1))\n",
        "  result_list.append(lib_delete_brackets.delete_brackets(asitis_cng_words2))\n",
        "  result_list.append(''.join(word_cng_list))\n",
        "\n",
        "  return result_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQGM66ine9J3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import MeCab\n",
        "#対義語生成\n",
        "#import lib_wordRevChange as lw\n",
        "\n",
        "tokenizer = MeCab.Tagger(\"-Ochasen\")\n",
        "\n",
        "cnt = 1\n",
        "save_list = []\n",
        "jpn_start_flg = 0\n",
        "\n",
        "#列[words]をnumpyに変換\n",
        "val = df.words.values\n",
        "\n",
        "for idx in range(df.shape[0]):\n",
        "\n",
        "  words = str(val[idx])\n",
        "  \n",
        "  #日本語単語の開始判定\n",
        "  if words[0:1] == \"あ\":\n",
        "    jpn_start_flg = 1\n",
        "  \n",
        "  if jpn_start_flg == 1:\n",
        "    if len(words) <= 10:\n",
        "      if words.isdecimal() is False:\n",
        "        if '・' not in words:\n",
        "          gyaku = u\"逆\"\n",
        "          inherent_words = '[' + words + ']'  \n",
        "          rev_list = wordRevChange(words,gyaku,inherent_words,model,tokenizer)\n",
        "          save_list.append(rev_list)\n",
        "\n",
        "  if cnt % 1000 == 0:\n",
        "    print(cnt,\"件 終了\")\n",
        "\n",
        "    if cnt % 3000 == 0:\n",
        "      #Gドライブ再マウント\n",
        "      drive.mount('/content/drive')  \n",
        "      #保存\n",
        "      df2 = pd.DataFrame(save_list)\n",
        "      df2.to_csv('./drive/My Drive/NLP/modelToCsv.csv', header=False, index=False)\n",
        "    \n",
        "  cnt = cnt + 1\n",
        "  \n",
        "#Gドライブ再マウント\n",
        "drive.mount('/content/drive')  \n",
        "#保存\n",
        "df2 = pd.DataFrame(save_list)\n",
        "df2.to_csv('./drive/My Drive/NLP/modelToCsv.csv', header=False, index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2q1efaRhvj6",
        "colab_type": "text"
      },
      "source": [
        "##以下、検証用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93NfdGafVlHH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "2edcbfe8-6cf4-46f1-bdaf-9bf43e25a7bb"
      },
      "source": [
        "\n",
        "rt = model.most_similar(positive=[\"歩く\",\"逆\"])\n",
        "rt"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('曲がる', 0.670616865158081),\n",
              " ('逃げる', 0.6605823040008545),\n",
              " ('寄る', 0.6599925756454468),\n",
              " ('泳ぐ', 0.6526366472244263),\n",
              " ('滑る', 0.6492980718612671),\n",
              " ('行く', 0.6289190053939819),\n",
              " ('転がる', 0.6097988486289978),\n",
              " ('持ち上げる', 0.6081393957138062),\n",
              " ('曲げる', 0.6051719188690186),\n",
              " ('触る', 0.6028169393539429)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOhrO5yHTa_T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "25737087-78c6-4488-a125-b97056c6a4c0"
      },
      "source": [
        "tokenizer = MeCab.Tagger(\"-Ochasen\")\n",
        "node = tokenizer.parseToNode(\"さようなら\")\n",
        "\n",
        "while node:\n",
        "  print(node.feature.split(\",\")[0])\n",
        "  node = node.next"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOS/EOS\n",
            "感動詞\n",
            "BOS/EOS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY60x7YxydNT",
        "colab_type": "text"
      },
      "source": [
        "##以下、検証用2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKoCJxYi8Fl_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "2715eaf7-100b-4583-e205-40ce0b4563f2"
      },
      "source": [
        "!pip install mecab-python3"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/92/e7e7f38df8457fa40c1ca86928be5ddbe2bf341e90a35e6ada30d03ef16d/mecab_python3-0.996.2-cp36-cp36m-manylinux1_x86_64.whl (15.9MB)\n",
            "\u001b[K     |████████████████████████████████| 15.9MB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.996.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCVilfCaxmGp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6afa4f00-412f-4346-814a-cfb03e8a438e"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "csv_dir = './drive/My Drive/NLP/modelToCsv_500000.csv'\n",
        "df_ant = pd.read_csv(csv_dir,names=['words','ant1','ant2','ant3'])\n",
        "df_ant = df_ant.sort_values('words', ascending=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sqLQwqOxmci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import MeCab\n",
        "import numpy as np\n",
        "\n",
        "def get_ant_word(words):\n",
        "\n",
        "  word_cng_list = []\n",
        "\n",
        "  #-------------------------------------------------\n",
        "  # そのまま対義語化\n",
        "  #-------------------------------------------------\n",
        "  try:\n",
        "    rvs_wd = df_ant[df_ant[\"words\"] == words].values[0][1]\n",
        "    if rvs_wd == np.nan:\n",
        "      rvs_wd = df_ant[df_ant[\"words\"] == words].values[0][2]\n",
        "      if  rvs_wd == np.nan:\n",
        "        rvs_wd = df_ant[df_ant[\"words\"] == words].values[0][3]\n",
        "\n",
        "    word_cng_list.append(rvs_wd)\n",
        "\n",
        "  except IndexError as error:\n",
        "    #辞書に登録の無い単語の場合\n",
        "    pass\n",
        "\n",
        "  #-------------------------------------------------\n",
        "  # 形態素分析後に対義語化\n",
        "  #-------------------------------------------------\n",
        "  tokenizer = MeCab.Tagger(\"-Ochasen\")\n",
        "  node = tokenizer.parseToNode(words)\n",
        "  ant_word = ''\n",
        "\n",
        "  while node:\n",
        "    #分かち書きの単語を取得\n",
        "    cut_wd = node.surface\n",
        "\n",
        "    if cut_wd != np.nan and cut_wd != '':\n",
        "      if node.feature.split(\",\")[0] == u\"名詞\":\n",
        "        try:\n",
        "          rvs_wd = df_ant[df_ant[\"words\"] == cut_wd].values[0][1]\n",
        "          if rvs_wd is np.nan:\n",
        "            rvs_wd = df_ant[df_ant[\"words\"] == cut_wd].values[0][2]\n",
        "            if  rvs_wd is np.nan:\n",
        "              rvs_wd = df_ant[df_ant[\"words\"] == cut_wd].values[0][3]\n",
        "\n",
        "          ant_word = ant_word + rvs_wd\n",
        "\n",
        "        except IndexError as error:\n",
        "          #辞書に登録の無い単語の場合\n",
        "          pass\n",
        "\n",
        "      elif (node.feature.split(\",\")[0] == u\"動詞\" or\n",
        "        node.feature.split(\",\")[0] == u\"形容詞\" or\n",
        "        node.feature.split(\",\")[0] == u\"副詞\" or\n",
        "        node.feature.split(\",\")[0] == u\"感動詞\"):\n",
        "\n",
        "        #分かち書きの単語を取得\n",
        "        cut_wd = node.feature.split(\",\")[6]\n",
        "\n",
        "        try:\n",
        "          rvs_wd = df_ant[df_ant[\"words\"] == cut_wd].values[0][1]\n",
        "          if rvs_wd is np.nan:\n",
        "            rvs_wd = df_ant[df_ant[\"words\"] == cut_wd].values[0][2]\n",
        "            if rvs_wd is np.nan:\n",
        "              rvs_wd = df_ant[df_ant[\"words\"] == cut_wd].values[0][3]\n",
        "\n",
        "          ant_word = ant_word + rvs_wd\n",
        "\n",
        "        except IndexError as error:\n",
        "          #辞書に登録の無い単語の場合\n",
        "          pass\n",
        "\n",
        "      else:\n",
        "        #◆結合\n",
        "        ant_word = ant_word + cut_wd\n",
        "\n",
        "    node = node.next\n",
        "\n",
        "  word_cng_list.append(ant_word)\n",
        "\n",
        "  word_cng_list_uni = list(set(word_cng_list))\n",
        "  \n",
        "  return word_cng_list_uni"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULkjDK6bSyw-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "346d3fdb-dc50-4c07-ed0d-a90f67e37ad3"
      },
      "source": [
        "print(get_ant_word(\"波乗りジョニー\"))"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ハイジャンプサム', 'MISS']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLug8mpVzn_5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "5c2bc241-f0be-4035-d9d8-b7ec1a7268da"
      },
      "source": [
        "df_ant[df_ant[\"words\"] == \"波乗りジョニー\"]"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>ant1</th>\n",
              "      <th>ant2</th>\n",
              "      <th>ant3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>233565</th>\n",
              "      <td>波乗りジョニー</td>\n",
              "      <td>MISS</td>\n",
              "      <td>与作</td>\n",
              "      <td>ハイジャンプサム</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          words  ant1 ant2      ant3\n",
              "233565  波乗りジョニー  MISS   与作  ハイジャンプサム"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    }
  ]
}