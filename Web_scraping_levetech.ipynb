{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web_scraping_levetech.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soichi-fujiwara/jupyter-notebook/blob/master/Web_scraping_levetech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmCv19wpeqxa",
        "colab_type": "code",
        "outputId": "e0b55b1f-08e3-4c04-f594-287b506b2440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install beautifulsoup4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNMKawvAeYAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# 親ページURL(指定：機械学習、データ分析、データマイニング)\n",
        "url_pr = 'https://freelance.levtech.jp/project/search/?order=3&limit=3&sala=7&job%5B%5D=11&job%5B%5D=22&job%5B%5D=32&srchbtn=list_search&keyword=&tag_order='\n",
        "# 子ページURL(ベース)\n",
        "url_c_base = 'https://freelance.levtech.jp' \n",
        "\n",
        "html_doc = requests.get(url_pr).text\n",
        " \n",
        "# データフレームを定義\n",
        "columns = [\"案件名\",\"職務\"]\n",
        "df = pd.DataFrame(columns=columns)\n",
        " \n",
        "# BeautifulSoupでスクレイピング\n",
        "soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "# ラッピング単位のクラスを取得\n",
        "contents  = soup.find_all(\"div\", {\"class\": \"prjHead\"})\n",
        " \n",
        "for content in contents:\n",
        "    content_data = content.find(\"h3\", {\"class\": \"prjHead__ttl\"})\n",
        "\n",
        "    #新着(New) or 広告(CHECK)\n",
        "    chk_ad = content_data.find(\"span\")\n",
        "\n",
        "    if(chk_ad is not None):\n",
        "        if(chk_ad.text == \"New\"):\n",
        "            flg = \"OK\"\n",
        "        else:\n",
        "            flg = \"NG\"\n",
        "    else:\n",
        "        flg = \"OK\"\n",
        "        \n",
        "    if(flg == \"OK\"):\n",
        "        #案件名\n",
        "        content_name = content_data.text\n",
        "        content_name = content_name.replace(\"のエンジニア求人・案件\", \"\")\n",
        "\n",
        "        #子ページリンク先取得\n",
        "        content_website = content_data.a.get(\"href\")\n",
        "\n",
        "        html_c_doc = requests.get(url_c_base + content_website).text\n",
        "        soup_c = BeautifulSoup(html_c_doc, 'html.parser')\n",
        "        # 子ページ内のクラスを取得\n",
        "        contents_c  = soup_c.find(\"div\", {\"class\": \"pjtDetail__row\"})\n",
        "\n",
        "        #職務内容\n",
        "        #(pタグ2個目が対象の要素)\n",
        "        job_detail = contents_c.find_all(\"p\")[1]\n",
        "        job_detail = str(job_detail).replace(\"\\r\", \"\")\n",
        "        job_detail = str(job_detail).replace(\"\\n\", \"\")\n",
        "        job_detail = str(job_detail).replace(\"<p>\", \"\")\n",
        "        job_detail = str(job_detail).replace(\"</p>\", \"\")\n",
        "        job_detail = str(job_detail).replace(\"<br/>\", \"\")\n",
        "        job_detail = str(job_detail).strip()\n",
        "\n",
        "        # 各データをデータフレームに格納\n",
        "        se = pd.Series([content_name, job_detail], columns)\n",
        "        df = df.append(se, columns)\n",
        "\n",
        "# 収集したデータをCSV形式で保存\n",
        "filename = \"ai_list.csv\"\n",
        "df.to_csv(filename, encoding = 'utf-8')\n",
        "files.download(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}